title,link,author,published,itunes_duration,image,Audio,intro,timestamps,ai_summary,key_insights,quotes,categories,category1,category2,category3,category4,category5,category6,category7,category8,category9,category10,x_avg,y_avg,z_avg
Truly Serverless Infra for AI Engineers - with Erik Bernhardsson of Modal,https://www.latent.space/p/modal,Alessio + swyx,"Fri, 16 Feb 2024 17:42:47 GMT",3745,https://substackcdn.com/feed/podcast/1084089/post/141683117/ae711cfe69e11d4097303dcff543aa6f.jpg,https://api.substack.com/feed/podcast/141683117/a97925bc56ccef4c06571d1d8e82fc0f.mp3,"We’re writing this one day after the monster release of  OpenAI’s Sora  and  Gemini 1.5 . We covered this on   ‘s  ThursdAI space , so head over there for our takes. IRL: We’re ONE WEEK away from  Latent Space: Final Frontiers , the second edition and anniversary of our  first ever Latent Space event !  Also: join us on  June 25-27  for the  biggest AI Engineer conference of the year ! Online: All three Discord clubs are thriving.  Join us every Wednesday/Friday ! Almost 12 years ago, while working at Spotify, Erik Bernhardsson built one of the first open source vector databases,  Annoy , based on ANN search. He also built  Luigi , one of the predecessors to Airflow, which helps data teams orchestrate and execute data-intensive and long-running jobs. Surprisingly, he didn’t start  yet another  vector database company, but instead in 2021 founded  Modal , the “high-performance cloud for developers”. In 2022 they opened doors to developers  after their seed round , and in 2023 announced their GA with a  $16m Series A . More importantly, they have won fans among both household names like Ramp, Scale AI, Substack, and Cohere, and newer startups like (upcoming guest!) Suno.ai and individual hackers (Modal was the top tool of choice in the Vercel AI Accelerator): We've covered the nuances of GPU workloads, and how we need new developer tooling and runtimes for them (see our episodes with  Chris Lattner of Modular  and  George Hotz of tiny  to start). In this episode, we run through the major limitations of the actual infrastructure behind the clouds that run these models, and how Erik envisions the “postmodern data stack”.  In his 2021 blog post  “Software infrastructure 2.0: a wishlist” , Erik had “Truly serverless” as one of his points: *  The word cluster is an anachronism to an end-user in the cloud! I'm already running things in the cloud where there's elastic resources available at any time. Why do I have to think about the underlying pool of resources? Just maintain it for me. *  I don't ever want to provision anything in advance of load. *  I don't want to pay for idle resources. Just let me pay for whatever resources I'm actually using. *  Serverless doesn't mean it's a burstable VM that saves its instance state to disk during periods of idle. Swyx called this  Self Provisioning Runtimes  back in the day. Modal doesn’t put you in YAML hell, preferring to colocate infra provisioning right next to the code that utilizes it, so you can  just add GPU   (and disk, and retries…) : After 3 years, we finally have a big market push for this:  running inference on generative models is going to be the killer app for serverless , for a few reasons: *   AI models are stateless:  even in conversational interfaces, each message generation is a fully-contained request to the LLM. There’s no knowledge that is stored in the model itself between messages, which means that tear down / spin up of resources doesn’t create any headaches with maintaining state. *  Token-based pricing  is better aligned with serverless infrastructure than fixed monthly costs of traditional software. *  GPU scarcity  makes it really expensive to have reserved instances that are available to you 24/7. It’s much more convenient to build with a serverless-like infrastructure. In the episode we covered a lot more topics like maximizing GPU utilization, why Oracle Cloud rocks, and how Erik has never owned a TV in his life. Enjoy! Show Notes *  Modal *  ErikBot *  Erik’s Blog *  Software Infra 2.0 Wishlist *  Luigi *  Annoy *  Hetzner *  CoreWeave *  Cloudflare FaaS *  Poolside AI *  Modular Inference Engine Chapters *","[00:00:00] Introductions * [00:02:00] Erik's OSS work at Spotify: Annoy and Luigi * [00:06:22] Starting Modal * [00:07:54] Vision for a ""postmodern data stack"" * [00:10:43] Solving container cold start problems * [00:12:57] Designing Modal's Python SDK * [00:15:18] Self-Revisioning Runtime * [00:19:14] Truly Serverless Infrastructure * [00:20:52] Beyond model inference * [00:22:09] Tricks to maximize GPU utilization * [00:26:27] Differences in AI and data science workloads * [00:28:08] Modal vs Replicate vs Modular and lessons from Heroku's ""graduation problem"" * [00:34:12] Creating Erik's clone ""ErikBot"" * [00:37:43] Enabling massive parallelism across thousands of GPUs * [00:39:45] The Modal Sandbox for agents * [00:43:51] Thoughts on the AI Inference War * [00:49:18] Erik's best tweets * [00:51:57] Why buying hardware is a waste of money * [00:54:18] Erik's competitive programming backgrounds * [00:59:02] Why does Sweden have the best Counter Strike players? * [00:59:53] Never owning a car or TV * [01:00:21] Advice for infrastructure startups ","In this episode of the Latent Space podcast, hosts Alessio and Swyx chat with Erik Bernhardsson, founder of Modal. They delve into Erik's journey from Sweden to the forefront of tech innovation in New York, emphasizing his early programming days and eventual contribution to Spotify's music recommendation system. Erik shared his experience building Spotify's music recommendation system during his master’s and creating popular open-source tools, highlighting his long-term belief in vector databases' potential.

Swyx and Erik also discuss Modal's foundation and evolution, focusing on its mission to enhance data teams' productivity by reimagining the data stack from the ground up. Erik elaborates on Modal's design principles, aiming for developer productivity through simplified cloud interactions and the development of a postmodern data stack. They explore how Modal addresses unique challenges for data teams, distinguishing itself by catering to bursty workloads, hardware requirements, and the specific needs of AI and data engineers. The conversation covers Modal's achievements in reducing container cold start times and its effort to maintain developer joy in cloud computation. Erik shares insights on his lessons from working with data, the future of Modal, and his perspectives on industry trends affecting AI and cloud computing, emphasizing Modal's focus on serving AI engineers and its potential to innovate further in cloud services for data-heavy applications.

The podcast concludes with a personal touch, discussing Erik's achievements in competitive programming and how it influenced his approach to solving complex problems in technology. Erik encourages aspiring tech entrepreneurs to deeply understand and be passionate about their chosen field, emphasizing the necessity of a deep dive into infrastructure to build competitive and innovative products.","1. Early focus on making data teams more productive by recognizing the fragmented and inefficient nature of the data stack.
2. Modal operates as a general-purpose compute platform aimed at a variety of applications, not just those tailored to data science.
3. Introduction of Modal's Series A funding round led by Redpoint.
4. Erik Bernhardsson's leadership in developing a scalable music recommender system during his tenure at Spotify.
5. Development of Luigi and Annoy, open-source tooling for workflow management and vector database handling, while at Spotify.
6. Modal's ability to improve GPU utilization for customers by ensuring GPUs are only charged for when actually running.
7. Exploration into programming issues like container cold starts and file system optimizations to enhance cloud-based computing performance.
8. The use of Python function decorators in Modal to simplify the deployment process for developers.
9. The potential impact of AI on increasing developer productivity by enabling developers to build software more efficiently.
10. Erik Bernhardsson's unique personal choices, including never owning a TV or a car.","**Regarding building in the AI space and the advantages of starting a company in this domain:**

Erik  [00:06:53]: ""I started Modal because, you know, like in a way, like I work with data, like throughout most of my career, like every different part of the stack, right? Like everything from business analytics to like deep learning, you know, like building, you know, training neural networks at scale, like everything in between. And so one of the thoughts, like, and one of the observations I had when I started Modal or like why I started was like, I just wanted to build better tools for data teams.""

**On the transformation and specialization of AI/ML platforms for specific needs:**

Swyx  [00:16:59]: ""It's sort of one level of recursion to sort of be the Modal for their customers.""

**Discussing the focus on customer experience and developer productivity:**

Erik  [00:51:24]: ""focusing on developer experience and developer productivity, that's where I think, you know, you can have more of a competitive moat.""

**Reflection on deep expertise required for running cloud services specifically tailored for AI/ML workloads:**

Erik  [01:00:43]: ""one thing I struggle with, like I talked to a lot of people starting companies in the data space or like AI space or whatever. And they sort of come at it at like, you know, from like an application developer point of view. And they're like, I'm going to make this better. But like, guess how you have to make it better. It's like, you have to go very deep on the infrastructure layer.""
","1. Models
2. AI Engineer Career
3. Coding Tools
4. Hardware
5. Open Source
6. Multimodal
7. Finetuning
8. Transformers Math
9. LLM Tooling
10. Datasets 101",Models,AI Engineer Career,Coding Tools,Hardware,Open Source,Multimodal,Finetuning,Transformers Math,LLM Tooling,Datasets 101,0.2449489742,0.0,0.0
Cloud Intelligence at the speed of 5000 tok/s - with Ce Zhang and Vipul Ved Prakash of Together AI,https://www.latent.space/p/together,Alessio + swyx,"Thu, 08 Feb 2024 16:57:15 GMT",3791,https://substackcdn.com/feed/podcast/1084089/post/141447889/54e03cebb4c19a9994df8bc44fa29fde.jpg,https://api.substack.com/feed/podcast/141447889/9dfc03f3219bb432b6c227d2eb8dd7ea.mp3,"Our  first ever demo day  aimed for 15-20 people and ended up  ballooning to >200   and   covered in the news .  We are now running the 2024 edition  in SF on Feb 23 :  Latent Space   Final Frontiers , a startup and research competition in “The Autonomous Workforce”, ​”Beyond Transformers & GPUs”, and “​Embodied AI”.  RSVP here ! You can find all LS online/IRL events on  our new calendar .  Super Early Bird tickets have just gone on sale for   AI Engineer World’s Fair, June 25-27 ! Today we have the honor of hosting two of  Together AI ’s co-founders:  Ce Zhang  (CTO) and  Vipul Ved Prakash  (CEO). This is a rare opportunity to recap the history of the company since  our last check-in with  Tri Dao  (Chief Scientist), some of their big releases, and do a deep dive into the state of the AI inference market.   Together  has emerged as one of the most consequential new startups in the new AI summer, last announcing a  ~$100m Series A  raise in November (at a ~ $360-565m valuation ). But there are at least three Togethers - Together the Research Lab, Together the Fine Tuning & Inference platform, and Together the custom models service. As we clarify on the pod, the overarching philosophy of Together is the ability to improve on all these fronts simultaneously by being “full stack”, from the lowest level kernel and systems programming to the highest level mathematical abstractions driving new model architectures and inference algorithms. Bringing Research and Industry Together In just one year, Together has been behind some of the most exciting research in AI: *  RedPajama , a  fully open source dataset  for model pre-training which mirrored the Llama1 recipe. Then followed by  RedPajama2 ,  a 30T tokens dataset  of filtered and de-duplicated tokens.   *  RedPajama-INCITE-3B and 7B , which were SOTA in a few benchmarks at the  time of release .  *  FlashAttention-2 , developed by Together’s Chief Scientist Tri Dao. We covered FA-2 in  a previous episode with him . *  Mamba-3B ,  the most promising transformer-alternative model that they released in collaboration with Cartesia.  *  StripedHyena , a SOTA graft of Hyena state space models and transformer models together *  Medusa , an alternative to speculative decoding that lets you use multiple decoding heads instead of a draft model.  *  MonarchMixer , which was one of the most popular orals at NeurIPS 2023. It’s an approach to transformers that replaces many of its core parts with Monarch matrices for better computational efficiency.  And I’m sure we missed something! As Vipul reveals, almost 50% of Together staff is researchers, and two of their co-founders (Chris Ré and Percy Liang) are professors at Stanford, so we can expect a lot more here. Bringing “Disaggregated” GPUs Together On their cloud, they offer inference as a service, fine-tuning, pre-training, etc, but unlike other providers they think of themselves as a  disaggregated cloud.  Today,  they have ~8,000 A100 and H100 GPUs on their platform  (an exclusive revealed on the pod!) totaling over 20 exaflops of compute, but instead of just buying more and putting them in a cluster and then exposing a `us-east-1` option for customers, they are  taking heterogenous compute sources and adding a unified layer on top of it  for developers to consume. Building on Ce’s research, Together’s GPU Clusters are taking on comparable AWS and GCP offerings in both cost and speed: Take the  Hessian AI  center in Germany or the  DoE’s INCITE ; they have GPUs that they want to share with researchers, but they lack the cloud layer over it. Similarly, there’s starting to be more and more differentiation amongst types of GPUs: H100s, A100s, MI3000s, etc. Each of them has different availability and performance based on task, and the end user shouldn’t have to be an hardware expert to run inference on a model, so Together abstracts a lot of that away. A big theme of the Together inference stack, a “bag of 50 tricks” that we discuss on the pod, is also “hardware-aware” algorithms like FlashAttention and Mamba, which further emphasize the benefits of co-developing everything  together : Special Focus: Transformer Alternatives As we mentioned above, they are also funding a lot of research in Transformer alternatives. To reiterate a few points on why they matter: *  Longer context is not the motivation for sub-quadratic architectures : Transformers don’t inherently have hard limitations on context size, but they just get extremely expensive. When developing sub-quadratic alternatives, you easily enable very long context, but that’s now how you should compare them. Even at same context size, inference and training is much cheaper on sub-quadratic architectures like Hyena. *  Emergence of hybrid architectures:  a lot of early conversations have been around the  “post-Transformers”  era, but it might be more like  “half-Transformers” . Hybrid architectures could have split layers with some transformer-based and some state-space ones. One of the challenges is that a lot of hardware kernels are optimized for transformer operations, so you’d lose a lot by moving away completely. *  Higher speed = higher GPU throughput:  if we could reach the same benchmark performance on subquadratic architectures, it’d solve a lot of the GPU crunch. Today we peak at ~170 tok/s on inference in some open models;  if we could reach 5,000 tok/s on the same card, you’d be able to serve 30x more customers on the same hardware . As a cloud provider, you’re obviously incentivized to get there. We had a lot of fun chatting with the Together guys and we covered a lot of ground, so enjoy the conversation! Note: This is the first episode of a “cloud providers mini-series”. We have Erik from Modal and Ben from Replicate coming up next! Video Podcast Join us to watching the video version of this pod  on our snazzy YouTube ! Show Notes *  Together AI *  RedPajama Dataset v1 Announcement *  RedPajama Models v1 Announcement *  Together Embeddings *  StripedHyena-7B *  Mamba-3B-SlimPJ *  Vipul's X thread on Anyscale *  Vipul's Razor *  SemiAnalysis' ""Inference Race to the Bottom"" post *  Chris Ré *  Mike Conover's episode *  Slim Pajama by Cerebras *  Dolma by AI2 *  Jina AI *  Tengyu's Voyage AI","* [00:00:00] Introductions * [00:00:43] Origin and current state of Together.ai * [00:02:15] Transition from Apple to Together and the vision for open AI * [00:04:54] How Chris Ré introduced Ce and Vipul * [00:08:43] How RedPajama came to be * [00:13:34] Model training and Transformer alternatives * [00:15:37] DSIR and the importance of data in LLMs * [00:21:19] Inference vs Fine-tuning vs Pre-training usage on Together * [00:23:20] Together's GPU stash * [00:27:02] Why standardization of inference metrics is important * [00:29:26] Building moats in AI inference * [00:31:49] Federated vs disaggregated cloud computing * [00:34:57] Opportunities for improvement in the inference stack * [00:36:13] Anyscale benchmarking drama * [00:41:27] Not just an inference platform * [00:43:50] Together Embeddings and the future of embedding models * [00:45:53] State space models and hybrid architectures * [00:53:52] The need for 5,000 tokens/s speed in AI inference * [01:00:23] What's the most interesting unsolved question in AI?","In this episode of the Latent Space podcast, hosts Alessio and Swyx are joined by Ce, the CTO, and Vipul Ved Prakash, the CEO of Together, an AI company focusing on open and independent AI systems. Founded in June 2022, Together aims to build a platform for user-owned AI systems, emphasizing decentralization and open-source ethos. They differentiate by combining data centers globally in a decentralized manner rather than relying on hyperscaler locations.

Vipul, with a background in open source and experience from Apple, discusses the transition from closed to open systems and the importance of making technology accessible and simplistic for developers, reflecting on his time at Apple and his involvement in creating a collaborative spam filter, Vipul’s Razor.

Ce talks about his and the company's technical evolution, emphasizing data movement across different stacks and the importance of efficient systems. Together's approach to building AI models involves open collaboration and leveraging a wide variety of data, not limited to the internet.

Together recently announced their ""Red Pajama"" data set aimed at fostering community engagement and development. This open-source effort is intended to improve data quality and encourage the creation of models and data sets that serve a broader range of applications. The conversation also touches on the future of embeddings and the hybrid architecture of AI models that combine elements from both state-based models and transformers to optimize for performance and AI capabilities.

A key part of Together's strategy is their investment in research and the development of systems that push the capabilities of AI forward. They are actively hiring across multiple domains to expand their efforts in making AI more open and accessible.

The interview particularly highlights the philosophical alignment and excitement around Together's mission to decentralize AI technology, open up more data for model training, and continuously explore new architectural possibilities for AI models, including state-space models that promise efficient handling of long context sequences.

Listeners are given a comprehensive overview of Together's philosophy, its technical advancements, the potential for open-source AI, and the importance of community involvement in driving the technology forward.","1. Together focuses on building a platform for open-source, independent, user-owned AI systems.
2. Red Pajama dataset released to facilitate open AI research, with V2 emphasizing data quality and providing pre-computed quality signals.
3. The importance of efficient system design in AI platforms, highlighting the need for data movement optimization across all stacks.
4. Demonstrating the significance of fine-tuning and efficient systems in reducing the cost and enhancing the quality of building AI models.
5. Together's approach includes decentralization and a globally distributed cloud for AI computing, aiming for more accessibility and scalability.
6. The potential of state space models and hybrid architectures blending different model types for improved AI performance and efficiency.
7. Together's serverless infrastructure for AI development aims to simplify usage and encourage experimentation among developers.
8. Emphasis on the evolution and importance of embeddings in AI applications for more accurate and efficient data representation.
9. The systematic exploration of scaling laws, data quality, and architectural optimizations in state-based models for future advancements in AI.
10. Together's commitment to open-source and community collaboration in advancing AI research and development.","[00:02:15] ""We think of Together as a platform for everything else, whether these are open models, whether these are models being built by companies that are owned by them.""

[00:04:54] ""We've never had algorithms that improve in capabilities with scale out. So this is almost a new era of computing.""

[00:06:27] ""AI companies are really kind of driven by research.""

[00:09:27] ""Really get inspired by the impact those like datasets have on the community.""

[00:21:12] ""The demand for AI computing is just kind of ramped up very, very quickly, and it will take a while for supply to catch up.""

[00:36:56] ""A good benchmark should think about how it's going to incentivize the field to actually move forward.""

[00:47:09] ""The key thesis on our side is, if you only push on one direction, you are going to reach diminishing return really, really quickly.""

[00:55:47] ""Once you can get sort of an immediate answer from a model, it starts working in a different way and, you know, new types of applications will be created.""

[01:00:39] ""Very excited about how does technology, like starting, right, like change the communication between different edge devices.""

[01:02:33] ""We sort of need framework of thinking about, you know, what the world looks like with advanced intelligence systems in it.""","1. Open Source
2. Models
3. Coding Tools
4. Hardware
5. AI Engineer Career
6. Finetuning
7. Transformers Math
8. FlashAttention
9. Multimodal
10. Agents",Open Source,Models,Coding Tools,Hardware,AI Engineer Career,Finetuning,Transformers Math,FlashAttention,Multimodal,Agents,0.1224744871,0.2224744871,0.1
Why StackOverflow usage is down 50% — with David Hsu of Retool,https://www.latent.space/p/retool,Alessio + swyx,"Thu, 01 Feb 2024 18:15:05 GMT",3504,https://substackcdn.com/feed/podcast/1084089/post/140985865/fe6ca58c479888b94b717ba3613b46f8.jpg,https://api.substack.com/feed/podcast/140985865/c49c4faf8eb5e688f683381006b4fec1.mp3,"We are announcing the second edition of our Latent Space demo day event in SF on 2/23:  Final Frontiers , a startup and research competition in “The Autonomous Workforce”, ​”Beyond Transformers & GPUs”, and “​Embodied AI”.  RSVP here ! The  first one  was aimed for 15-20 people and ended up  blowing up to >200   and   covered in the Information  - let’s see what a year of growth (and competition) does to the local events space in 2024. You can find all  Latent Space events here , and of course get in touch with us to host your own AI Engineer meetups like  AI Engineering Singapore . In our  December 2023 recap  we covered the Four Wars of the AI stack. But how do we know when it’s time to crown a winner? As we kick off 2024, we wanted to do a recap of the State of AI in 2023 to set a baseline of adoption for different products.  Retool  had a  great report at the end of last year  which covered a lot of it.  David Hsu, CEO and co-founder of Retool, joined us to go over it together. We also talked about the history of Retool, why they were too embarrassed to present at YC demo day, and how they got to $1M ARR with 3 employees. If you’re a founder, there are a lot of nuggets of advice in here! Retool AI In our modeling of the “Software 3.0 Stack”, we have generally left a pretty wide open gap as to the “user interface” equivalent of the AI stack: Retool AI launched  4 months ago  with some nifty features for SQL generation, and its own hosted vector storage service (using pgvector). However, as he explains on the pod, the more interesting potential of Retool is in helping developers build AI infused applications quickly, in combination with its  Workflows  feature.  This moves Retool down the stack from just the UI for internal tooling to the business logic “piping” as well. There are a bunch of dedicated tools in this space like  Respell ,  BuildShip ,  Flowise , and  Ironclad Rivet . ""We think that practically every internal app is going to be AI infused over the next three years.""  - David on the pod RIP StackOverflow? In July 2023 we talked about the impact of ChatGPT and Copilot: This was then  disputed by StackOverflow , who pointed out (very fairly so) that there were privacy-related changes in their analytics instrumentation in 2022. StackOverflow no longer reports traffic, but based on StackOverflow’s continuing transparency we can see that organic declines have continued throughout 2023. Retool’s report comes over a year after those changes and has some self reported samples from users: *  57.6% of people said they have used StackOverflow less ; almost all of them replaced it with ChatGPT and Copilot. *  10.2% said they no longer use StackOverflow . We also saw a lot more tools being released in the dev tools space such as (one of our oldest pod friends)  Codeium  (which just raised a  $65M Series B ),  SourceGraph  (and their newly released Cody),  Codium AI  (just released AlphaCodium which was  picked up by Karpathy ),  Phind  (which beat GPT-4 with OSS models), and  Cursor , one of the most beloved products in the dev community at the moment. Intelligence is getting closer and closer to the IDE, and the trend doesn’t seem to be reverting.  We already said that  “You are not too old (to pivot into AI)“ , and the advice still stands. When asked to rate  “Preference for hiring engineers effective at using ChatGPT/Copilot for coding”  on a scale of 1 to 10, where 10 is “Much more likely”, ~40% of companies voted 8-10.  Having an  AI Engineer  skillset is extremely important.  45% of companies between 1,000-4,999 employees said that they increased the difficulty of technical interviews to compensate for these new tools, so the gap between users and non-users will keep widening. Crossing the AI in Production Chasm Geoffrey Moore’s “ Crossing the Chasm ” is one of the most quoted business frameworks. Every market has an initial group of Innovators and Early Adopters, who are willing to suffer through the rough edges of products initially, and eventually crosses into the Early Majority, which expects a full product. In the AI world, ChatGPT and Midjourney / DALL-E have crossed the chasm in the consumer space. Copilot is probably the only tool that did it in the enterprise, having crossed 1M paid users.  ~$50B were invested in AI in 2023, and we still only have <5 breakout products; expect this number to rise in 2024.  According to the survey, only 25% of companies had real production usage, but 77.1% said their company is making efforts to adopt more.  Closing that gap could triple AI adoption in one year. The report also broke down adoption by use case. 66% of companies use it internally, while only 43% do so in customer-facing use cases. Internal usage of AI is much more varied than customer-facing one as well: One point that David made in the podcast is that this number isn’t a knock on AI as a tool, but rather about the demographics of businesses outside of our Silicon Valley bubble: We all work in Silicon Valley, right? We all work at businesses, basically, that sell software as a business. And that's why all the software engineers that we hire basically work on external facing software, which makes sense with most software companies. But if you look at most companies in the world, most companies in the world are actually not software companies. […] Most of the [work of] software engineers in the world actually goes towards these internal facing applications. Beyond code models, it’s clear that the big winners of the first wave of AI adoption are  vector stores and RAG . Knowledge base Q&A, customer chatbots, recommendation systems, etc are all based on them. Retool even rolled out their own with  Retool Vectors . Expect the battlefield to get even hotter in these areas, with Mongo and Chroma leading the charge on a NPS/popularity basis.  It’s also clear that OpenAI won the first campaign in the AI models war, by far. Hopefully Mistral and LLaMA3 will shake up this chart when we look back at it in 2025: TLDR: We’re really early.  If you want to build in AI, there’s a ton of work to be done, and a lot of problems to be solved. You can find the full report  here  to dive through all the numbers.  Video podcast Watch along on our snazzy  YouTube ! Show Notes Companies and Projects: *  Retool *  State of AI Report *  Retool AI *  Retool Workflows *  Raising less money at lower valuations *  Paul Graham's ""playing house"" essay *  Gödel, Escher, Bach (GEB)",* [00:00:00] Introduction * [00:02:43] Retool's founding story and decision not to present at YC demo day initially * [00:09:08] Philosophy on fundraising - raising less money at lower valuations * [00:12:53] Overview of what Retool is * [00:15:41] Origin story of Retool AI product * [00:19:59] Decision to use open source vector database PG Vector * [00:21:29] Most underrated AI use cases * [00:25:56] Retool's AI UX and workflows * [00:30:38] Zapier vs Retool * [00:32:54] Updates from Retool's 2023 State of AI survey * [00:35:21] Who is adopting AI first? * [00:37:40] Evolving engineering hiring practices in the age of Copilot/ChatGPT * [00:40:02] Retool's views on internal vs external AI adoption * [00:41:50] OSS models vs OpenAI in production * [00:44:46] Additional survey questions to ask in 2024 * [00:47:04] Balancing enterprise sales vs bottom-up adoption * [00:51:54] Philosophical thoughts on AGI and intentionality,"In this episode of the Latent Space podcast, hosts Alessio and Swyx welcome David Hsu, a representative from Retool. The conversation kicks off with David sharing his educational background in philosophy and computer science from Oxford, highlighting his study in the intersection of these fields with AI and logic. David recounts the early days and challenges of launching Retool, including the decision not to present at Y Combinator's Demo Day due to a lack of substantive progress at the time. He discusses the importance of focusing on creating value for customers over seeking social validation or chasing funding. This approach has influenced how Retool picks ideas and evaluates when to pivot or persist with a project.

David also touches upon personal interests, such as reading and biking, and shares insights into his reading habits. The discussion shifts towards the evolution and philosophy behind Retool, emphasizing the transition from initial sales-driven efforts to fostering ubiquity and a developer-first culture. The rationale behind focusing on developers as their primary user base, despite market assumptions that might categorize Retool differently, is explored.

David and the hosts delve into Retool's approach to AI, discussing how the platform aims to empower developers to build AI-infused applications efficiently. They examine the impact of AI on software development, internal tools, and how Retool navigates the tension between offering a broad platform capable of serving various use cases while maintaining a focus on the specific needs and privacy concerns of different industries.

Additionally, the conversation covers David's perspective on the broader AI landscape, including thoughts on AGI (Artificial General Intelligence), the importance of models meeting business needs, and philosophical musings inspired by works like Douglas Hofstadter's ""Gödel, Escher, Bach."" David reflects on how AI and its evolution may not need to mirror human intelligence strictly to achieve significant outcomes and benefits.

Throughout the episode, insights into how Retool balances serving both enterprise customers and the wider developer community are provided, alongside reflections on the future of AI in software development and internal tool creation. The podcast wraps up with anticipation for the next State of AI survey results, which could offer updated insights into how perceptions and uses of AI are evolving.","1. AI thinking at Retool began about a year and a half ago, initially as a joke about ""AI programming"" but has now evolved into actual development.
2. Two main value propositions of AI identified: helping build apps faster and assisting developers in creating AI-enabled applications more efficiently.
3. Major focus on enabling developers to rapidly infuse AI into all apps, not just for speeding up coding but for integrating AI functionalities into applications for broader use cases.
4. Introduction of a vector database and AI actions at Retool to streamline the development of AI-infused applications without extensive model selection or testing.
5. Observation that almost every internal app will likely incorporate some form of AI in the coming years for increased productivity.
6. Discussion on open source versus commercial tooling suggests a belief in the triumph of open source for AI infrastructure due to its fast-paced nature and the benefits of community contribution.
7. An emphasized approach towards employing former founders at Retool, around half of the workforce at various stages, to infuse a proactive, outcome-focused culture with minimal politics.
8. Retool's fundraising strategy includes intentionally raising less money at lower valuations to minimize dilution and avoid valuation-related obstacles in the future, underscoring a focus on building a quality company over maximizing valuation metrics.
9. The use of AI for automating workflows within internal tools, allowing for complex multi-step processes to be optimized or handled by AI, thereby significantly enhancing productivity through automation rather than just assistance.
10. Breaking down how Retool adapts and contributes to AI development, touching topics from philosophical underpinnings at a foundational level to practical implementations affecting workflows and internal tools development.","1. **On AI Adoption in Internal Tools**:
   - [00:40:02] ""Only 27% are in production. Of that 27%, 66% are internal use cases... it's interesting to understand how people are adopting or adapting to different modalities that are now coming online for them.""

2. **On Balancing Enterprise Focus with Ubiquity**:
   - [00:48:04] ""We changed our focus to be Ubiquity, actually, last year... Our gold star metric used to always be sort of revenue-generated. We actually changed it to be the number of developers building on the platform... It has to be like all the developers in the world use it.""
   
3. **Practical Utility of AI in Solving Business Problems**:
   - [00:52:07] ""There's a lot of hype in AI right now... not too many use cases. So for us, at least from a Retool context, it really is how do we bring AI and have it actually meet business problems?... but I think really where the Retool interest comes from, me personally, I think philosophically, yeah, I've been thinking recently myself a bit about sort of intentionality and AGI...""

4. **Philosophical Considerations on AGI**:
   - [00:54:29] ""I've been thinking recently myself a bit about sort of intentionality and AGI and like, you know, what would it take for me to say, yes, you know, GPT-X for, you know, any sort of model actually is AGI.""

5. **On Learning from Evolution and Applying to AGI**:
   - [00:55:57] ""It almost feels like humans have sort of naturally self-selected for these like two aims... whereas like the AI doesn't really have that. But maybe you could program it in... you could create a simulation, if you will, like all these AIs, you know, in the world, for example. And maybe you have AGI in the world.""

These insights not only highlight the challenges and considerations in implementing AI but also prompt reflections on the philosophical aspects of AI and its future trajectory.","1. AI Engineer Career
2. Startups
3. Open Source
4. Models
5. Coding Tools
6. Multimodal
7. Hardware
8. Finetuning
9. UX
10. News",AI Engineer Career,Startups,Open Source,Models,Coding Tools,Multimodal,Hardware,Finetuning,UX,News,0.0,0.4449489742,0.2
The Four Wars of the AI Stack (Dec 2023 Audio Recap),https://www.latent.space/p/dec-2023-audio,swyx & Alessio,"Thu, 25 Jan 2024 22:37:40 GMT",4098,https://substackcdn.com/feed/podcast/1084089/post/141025154/8372a440d0577b3107a49f8884aa8948.jpg,https://api.substack.com/feed/podcast/141025154/d20ba7d17799fd035cc4776c36402cc2.mp3,"Note for Latent Space Community members: we have now soft-launched meetups in   Singapore ,  as well as two new virtual paper club/meetups for   AI in Action and LLM Paper Club .  We’re also running  Latent Space: Final Frontiers , our second annual demo day hackathon  from last year . For the first time, we are doing an audio version of monthly AI Engineering recap that we publish on Latent Space! This month it’s “The Four Wars of the AI Stack”; you can find the full recap with all the show notes here:  https://latent.space/p/dec-2023 * [00:00:00] Intro * [00:01:42] The Four Wars of the AI stack: Data quality, GPU rich vs poor, Multimodality, and Rag/Ops war * [00:03:17] Selection process for the four wars and notable mentions * [00:06:58] The end of low background tokens and the impact on data engineering * [00:08:36] The Quality Data Wars (UGC, licensing, synthetic data, and more) * [00:14:51] Synthetic Data * [00:17:49] The GPU Rich/Poors War * [00:18:21] Anyscale benchmark drama * [00:22:00] The math behind Mixtral inference costs * [00:28:48] Transformer alternatives and why they matter * [00:34:40] The Multimodality Wars * [00:38:10] Multiverse vs Metaverse * [00:45:00] The RAG/Ops Wars * [00:50:00] Will frameworks expand up, or will cloud providers expand down? * [00:54:32] Syntax to Semantics * [00:56:41] Outer Loop vs Inner Loop * [00:59:54] Highlight of the month   Get full access to Latent Space at  www.latent.space/subscrib",,"It seems like you've inputted ""e"" which doesn't provide enough context for me to summarize a podcast transcript. Could you please provide more details or the text of the transcript you're referring to? That way, I can help you with a summary.","I'm sorry, but you didn't provide a transcript to extract AI research insights from. Could you please provide the transcript or specific details from it?",,"1. News
2. Event Recaps
3. Monthy Recaps
4. Startups
5. Open Source
6. Finetuning
7. Hardware
8. LLM Tooling
9. Coding Tools
10. AI Engineer Career",News,Event Recaps,Monthy Recaps,Startups,Open Source,Finetuning,Hardware,LLM Tooling,Coding Tools,AI Engineer Career,0.1,0.2224744871,0.0
How to train your own Large Multimodal Model — with Hugo Laurençon & Leo Tronchon of HuggingFace M4,https://www.latent.space/p/idefics,Alessio + swyx,"Fri, 19 Jan 2024 17:09:45 GMT",4310,https://substackcdn.com/feed/podcast/1084089/post/140820515/dc95249bb20e7554690c24ee6bd00081.jpg,https://api.substack.com/feed/podcast/140820515/391f812d878085f4538a316fbecf3cf0.mp3,"Latent Space is heating up! Our  paper club  ran into >99 person Discord limits, oops.  We are also introducing 2 new  online  meetups:  LLM Paper Club Asia  for Asia timezone (led by Ivan), and  AI in Action: hands-on application of AI  (led by KBall).  To be notified of all upcoming Latent Space events, subscribe to  our new Luma calendar  ( sign up for individual events, or   hit the RSS icon to sync all events to calendar ). In the halcyon open research days of 2022 BC ( Before-ChatGPT ), DeepMind was the first to create a SOTA multimodal model by taking a pre-existing LLM ( Chinchilla 80B  - now  dead ?) and pre-existing vision encoder ( CLIP ) and training a “glue” adapter layer, inspiring a generation of stunningly cheap and effective multimodal models including  LLaVA  (one of the  Best Papers of NeurIPS 2023 ),  BakLLaVA  and  FireLLaVA .  However (for reasons we discuss in today’s conversation), DeepMind’s Flamingo model was never open sourced. Based on  the excellent paper , LAION stepped up to create  OpenFlamingo , but it never scaled beyond 9B. Simultaneously, the M4 (audio + video + image + text multimodality) research team at HuggingFace  announced  an independent effort to reproduce Flamingo up to the full 80B scale: The effort started in March, and was released in August 2023. We happened to visit Paris last year, and visited HuggingFace HQ to learn all about HuggingFace’s research efforts, and cover all the ground knowledge LLM people need to become (what  Chip Huyen has termed ) “LMM” people. In other words: What is IDEFICS? IDEFICS is an Open Access Visual Language Model, available in  9B  and  80B  model sizes. As an attempt to re-create an open-access version of  Flamingo , it seems to track very well on a range of multimodal benchmarks (which we discuss in the pod): You can see the reasoning abilities of the models to take a combination of interleaved images + text in a way that allows users to either describe images, ask questions about the images, or extend/combine the images into different artworks (e.g. poetry). 📷 From IDEFICS’s  model card  and  blog post The above demo screenshots are actually fine-tuned instruct versions of IDEFICS — which are again in  9B  and  80B  versions. IDEFICS was built by connecting two unimodal models together to provide the multi-modality you see showcased above. *  Llama  v1 for language (specifically  huggyllama/llama-65b ) - the best available open model at the time, to be swapped for Mistral in the next version of IDEFICS *  A CLIP  model for vision (specifically  laion/CLIP-ViT-H-14-laion2B-s32B-b79K  - after a brief exploration of  EVA-CLIP , which we discuss on the pod) OBELICS: a new type of Multimodal Dataset IDEFICS’ training data used the usual suspect datasets, but to get to par with Flamingo they needed to create a new data set. Enter  OBELICS : “An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents”: * 115B text tokens * 141M English documents * 353M images These bullets are carefully curated and filtered by going through Common Crawl dumps between FEB 2020 - FEB 2023. We discuss the 2 months of mindnumbing, unglamorous work creating this pipeline: There’s a lot of mentions of ‘multi-modal' web documents’ which deserves some explanation. We’ll show you instead of tell you: You can see from this graph that OBELICS ends up outperforming the other image-text pairs dataset (LAION in this case) when stacked head-to-head. You can view a subset of OBELICS and perform visualizations on them  here : 2024 Update: WebSight et al Most of this interview was recorded on Halloween 2023 at HuggingFace’s headquarters in Paris: In anticipation of an IDEFICS v2 release. However, several roadblocks emerged, including a notable scandal around  CSAM in LAION-5B , which affected all models using that dataset. The M4 team have adopted a strategy of shipping smaller advancements in 2024, and the first ship of the year is  WebSight , a dataset of 823,000 HTML/CSS codes representing synthetically generated English websites, each accompanied by a corresponding screenshot (rendered with  Playwright ). This is intended for tasks like screenshot-to-code workflows like Vercel’s V0 or  TLDraw , and will be part of the dataset for IDEFICS-2. As noted in  our Best Papers recap ,  synthetic data  is emerging as one of the top themes of 2024, and the IDEFICS/OBELICS team have wasted no time enabling themselves with it.","* [0:00:00] Intro * [0:00:00] Hugo, Leo’s path into multimodality * [0:09:16] From CLIP to Flamingo * [0:12:54] Benchmarks and Evals * [0:16:54] OBELICS dataset * [0:34:47] Together Redpajama v2 * [0:37:12] GPT4 Vision * [0:38:44] IDEFICS model * [0:40:57] Query-Key Layernorm for training * [0:46:40] Choosing smaller vision encoders - EVA-CLIP vs SIG-GLIP * [0:49:02] IDEFICS v2 * [0:52:39] Multimodal Hallucination * [0:59:12] Why Open Source Multimodality * [1:05:29] Naming: M4, OBELICS, IDEFICS * [1:08:56] 2024 Update from Leo Show Notes *  Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model *  IDEFICS Knowledge sharing memo: technical lessons and mistakes *  Victor Sanh memo *  OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents * Papers cited: *  BLOOM : A 176B-Parameter Open-Access Multilingual Language Model *  Barlow Twins : Self-Supervised Learning via Redundancy Reduction *  CLIP paper : Learning Transferable Visual Models From Natural Language Supervision *  Vision Transformers paper : An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale *  Flamingo paper : a Visual Language Model for Few-Shot Learning *  April 2022 preprint from DeepMind ,  blogpost *  VQAV2 paper : Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering *  OK-VQA: A Visual Question Answering Benchmark  Requiring External Knowledge   ( https://okvqa.allenai.org/ ) *  MMBench: Is Your Multi-modal Model an All-around Player? *  Qwen-VL : A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond *  Sig-GLIP paper :  Sigmoid Loss for Language Image Pre-Training *  Nougat : Neural Optical Understanding for Academic Documents *  MMC4 (Multimodal C4) : An Open, Billion-scale Corpus of Images Interleaved With Text * Dall-E 3 paper:  Improving Image Generation with Better Captions *  GPT-4V(ision) system card  from OpenAI * Query-Key Layernorm trick:  paper  (Scaling Vision Transformers to 22 Billion Parameters),  tweet *  EVA-CLIP: Improved Training Techniques for CLIP at Scale   * “We intially explored using a significantly bigger vision encoder (the biggest in open-access at that time) with  EVA-CLIP . However, we ran into training instabilities very quickly. To lower the risks associated to the change of vision encoder, we decided to continue with  laion/CLIP-ViT-H-14-laion2B-s32B-b79K  which we have been using until that point. We will leave that swap for future iterations and will also consider using higher resolution images.” * Datasets *  Together’s RedPajama-Data-v2 : An open dataset with 30 trillion tokens for training large language models *  LAION COCO: 600M synthetic captions from Laion2B-en *  Chip Huyen’s writeup on LMMs *  Joseph Nelson of Roboflow on Latent Space *  HuggingFace M4 *  HuggingFace timm : library containing SOTA computer vision models, layers, utilities, optimizers, schedulers, data-loaders, augmentations, and training/evaluation scripts. It comes packaged with >700 pretrained models, and is designed to be flexible and easy to use. *  Logan Kilpatrick declaring 2024 the year of Multimodal AI  at AI Engineer Summit   Get full access to Latent Space at  www.latent.space/subscrib",It seems like you've provided an incomplete snippet for summarizing a podcast transcript. Could you provide more details or the content of the transcript you'd like summarized?,"I'm sorry, but you haven't provided a transcript for analysis. Could you please provide the transcript or details from which you need the key AI research insights extracted?",,"1. News
2. Event Recaps
3. Monthy Recaps
4. Startups
5. Open Source
6. Hardware
7. LLM Tooling
8. Coding Tools
9. AI Engineer Career
10. Benchmarks 101",News,Event Recaps,Monthy Recaps,Startups,Open Source,Hardware,LLM Tooling,Coding Tools,AI Engineer Career,Benchmarks 101,-0.1224744871,0.2224744871,-0.1
RLHF 201 - with Nathan Lambert of AI2 and Interconnects,https://www.latent.space/p/rlhf-201,Nathan Lambert,"Thu, 11 Jan 2024 19:22:50 GMT",5130,https://substackcdn.com/feed/podcast/1084089/post/140498023/61ed68e3d8702755739e0bc51cd556d9.jpg,https://api.substack.com/feed/podcast/140498023/123c87d5ab63626b70f035ec3dbb5480.mp3,"In 2023 we did a few Fundamentals episodes covering  Benchmarks 101 ,  Datasets 101 ,  FlashAttention , and  Transformers Math , and it turns out those were some of your evergreen favorites! So we are experimenting with more educational/survey content in the mix alongside  our regular founder and event coverage . Pls  request more ! We have a  new calendar  for events; join to be notified of upcoming things in 2024! Today we visit the  shoggoth mask factory : how do transformer models go from trawling a deeply learned latent space for next-token prediction to a helpful, honest, harmless chat assistant?      Our guest “lecturer” today is  ; you might know him from his prolific online writing on  and  Twitter , or from his previous work leading  RLHF at HuggingFace  and now at the  Allen Institute for AI  (AI2) which recently released  the open source GPT3.5-class Tulu 2 model  which was trained with DPO. He’s widely considered one of the most knowledgeable people on RLHF and RLAIF.  He recently gave an “RLHF 201” lecture at Stanford, so we invited him on the show to re-record it for everyone to enjoy! You can find the  full slides here , which you can use as reference through this episode.  Full video with synced slides For audio-only listeners, this episode comes with slide presentation along our discussion. You can find it on  our YouTube  (like, subscribe, tell a friend, et al). Theoretical foundations of RLHF The foundation and assumptions that go into RLHF go back all the way to Aristotle  (and you can find guidance for further research in the slide below) but there are two key concepts that will be helpful in thinking through this topic and LLMs in general: *  Von Neumann–Morgenstern utility theorem:  you can dive into the  math here , but the TLDR is that when humans make decision there’s usually a “maximum utility” function that measures what the best decision would be; the fact that this function exists, makes it possible for RLHF to model human preferences and decision making. *  Bradley-Terry model:  given two items A and B from a population, you can model the probability that A will be preferred to B (or vice-versa). In our world, A and B are usually two outputs from an LLM (or at the lowest level, the next token).  It turns out that from this minimal set of assumptions, you can build up the mathematical foundations supporting the modern RLHF paradigm! The RLHF loop One important point Nathan makes is that ""for many tasks we want to solve,  evaluation of outcomes is easier than producing the correct behavior "". For example, it might be difficult for you to write a poem, but it's really easy to say if you like or dislike a poem someone else wrote. Going back to the Bradley-Terry Model we mentioned, the core idea behind RLHF is that when given two outputs from a model, you will be able to say which of the two you prefer, and we'll then re-encode that preference into the model. An important point that Nathan mentions is that when you use these preferences to change model behavior ""it doesn't mean that the model believes these things. It's just trained to prioritize these things"". When you have preference for a model to not return instructions on how to write a computer virus for example, you're not erasing the weights that have that knowledge, but you're simply making it hard for that information to surface by prioritizing answers that don't return it. We'll talk more about this in our future Fine Tuning 101 episode as we break down how information is stored in models and how fine-tuning affects it. At a high level, the loop looks something like this: For many RLHF use cases today, we can assume the model we're training is already instruction-tuned for chat or whatever behavior the model is looking to achieve. In the ""Reward Model & Other Infrastructure"" we have multiple pieces: Reward + Preference Model The reward model  is trying to signal to the model how much it should change its behavior based on the human preference, subject to a  KL constraint .  The preference model  itself scores the pairwise preferences from the same prompt (worked better than scalar rewards). One way to think about it is that the reward model tells the model how big of a change this new preference should make in the behavior in absolute terms, while the preference model calculates how big of a difference there is between the two outputs in relative terms. A lot of this derives from John Schulman’s work on PPO: We recommend watching him talk about it in the video above, and also Nathan’s pseudocode distillation of the process: Feedback Interfaces Unlike the ""thumbs up/down"" buttons in ChatGPT, data annotation from labelers is much more thorough and has many axis of judgement. At a simple level, the LLM generates two outputs, A and B, for a given human conversation. It then asks the labeler to use a Likert scale to score which one it preferred, and by how much: Through the labeling process, there are many other ways to judge a generation: We then use all of this data to train a model from the preference pairs we have. We start from the base instruction-tuned model, and then run training in which the loss of our gradient descent is the difference between the good and the bad prompt. Constitutional AI (RLAIF, model-as-judge) As these models have gotten more sophisticated, people started asking the question of whether or not humans are actually a better judge of harmfulness, bias, etc, especially at the current price of data labeling. Anthropic's work on the ""Constitutional AI"" paper is using models to judge models. This is part of a broader ""RLAIF"" space: Reinforcement Learning from AI Feedback. By using a ""constitution"" that the model has to follow, you are able to generate fine-tuning data for a new model that will be RLHF'd on this constitution principles. The RLHF model will then be able to judge outputs of models to make sure that they follow its principles: Emerging Research RLHF is still a nascent field, and there are a lot of different research directions teams are taking; some of the newest and most promising / hyped ones: *  Rejection sampling / Best of N Sampling : the core idea here is that rather than just scoring pairwise generations, you are generating a lot more outputs (= more inference cost), score them all with your reward model and then pick the top N results. LLaMA2 used this approach, amongst many others. *  Process reward models : in Chain of Thought generation, scoring each step in the chain and treating it like its own state rather than just scoring the full output. This is most effective in fields like math that inherently require step-by-step reasoning. *  Direct Preference Optimization (DPO):  We covered DPO in our  NeurIPS Best Papers recap , and Nathan has a  whole blog post  on this; DPO isn’t technically RLHF as it doesn’t have the RL part, but it’s the “GPU Poor” version of it. Mistral-Instruct was a  DPO model , as do  Intel’s Neural Chat  and  StableLM Zephyr .  Expect to see a lot more variants in 2024 given how “easy” this was. *  Superalignment : OpenAI launched research on  weak-to-strong generalization  which we briefly discuss at the 1hr mark. Note: Nathan also followed up this post with RLHF resources from his and peers’ work: Show Notes *  Full RLHF Slides *  Interconnects *  Retort (podcast) *  von Neumann-Morgenstern utility theorem *  Bradley-Terry model  (pairwise preferences model) *  Constitutional AI *  Tamer (2008 paper by Bradley Knox and Peter Stone) *  Paul Christiano et al. RLHF paper *  InstructGPT *  Eureka by Jim Fan *  ByteDance / OpenAI lawsuit *  AlpacaEval *  MTBench *  TruthfulQA  (evaluation tool) *  Self-Instruct Paper *  Open Assistant *  Louis Castricato *  Nazneen Rajani *  Tulu (DPO model from the Allen Institute)",* [00:00:00] Introductions and background on the lecture origins * [00:05:17] History of RL and its applications * [00:10:09] Intellectual history of RLHF * [00:13:47] RLHF for decision-making and pre-deep RL vs deep RL * [00:20:19] Initial papers and intuitions around RLHF * [00:27:57] The three phases of RLHF * [00:31:09] Overfitting issues * [00:34:47] How preferences get defined * [00:40:35] Ballpark on LLaMA2 costs * [00:42:50] Synthetic data for training * [00:47:25] Technical deep dive in the RLHF process * [00:54:34] Projection / best event sampling * [00:57:49] Constitutional AI * [01:04:13] DPO * [01:08:54] What's the Allen Institute for AI? * [01:13:43] Benchmarks and models comparisons,"This podcast episode from the Latent Space podcast features hosts Alessio and Swyx, along with their guest, Dr. Nathan Lambert. Nathan shares insights from his journey in the field of artificial intelligence, particularly focusing on robotics, reinforcement learning, and his experiences working with organizations like FAIR, DeepMind, and Hugging Face. Additionally, Nathan discusses his recent move to the Allen Institute as a research scientist, his passion for ultra-endurance sports, and his creative outlets through blogging and podcasting with Interconnects and Retort, respectively.

The conversation delves into Nathan's work, highlighting his contributions to model-based reinforcement learning, his exploration of the RLHF (Reinforcement Learning from Human Feedback) team at Hugging Face, and his approach to tackling the challenges and stresses inherent in AI research. Nathan emphasizes the importance of balance, incorporating his interests in long-distance running and biking as ways to stay grounded amidst his intensive work schedule.

The podcast touches on various technical and philosophical aspects of working in AI, including the dynamics of job searching in the field, the evolution of reinforcement learning, and the complexities of developing and fine-tuning AI systems. Nathan's blog, ""Interconnects,"" is mentioned as a platform where he shares his thoughts, findings, and the human aspects of working in AI, reaching out to both seasoned professionals and those new to the field.

In discussing reinforcement learning and language models, Nathan brings attention to the shifting landscape of AI research and the practical applications of these technologies. The episode provides listeners with a comprehensive understanding of Nathan's work and insights into the broader AI research community, showcasing the intersection between technical innovation and personal growth.","1. Reinforcement Learning from Human Feedback (RLHF) is becoming essential for NLP and AI development, with major companies now recognizing the necessity of having dedicated teams for RLHF.
2. Direct Preference Optimization (DPO) presents a simpler alternative to traditional RLHF methods, making it more accessible for open-source and academic initiatives.
3. Constitutional AI and super alignment strategies are emerging focuses, with the goal of scaling human preference data collection through AI models themselves, thereby preparing for future scenarios where AI might surpass human capabilities in terms of decision-making and preference modeling.
4. The effectiveness of RLHF, including techniques such as DPO, hinges significantly on the quality and characteristics of the preference data used during the reinforcement learning process.
5. There is a growing trend toward starting companies and initiatives in the RLHF space, aiming to provide RLHF as a service or to innovate within this area, indicating the field's recognized potential for future development and commercial application.",,"1. Models
2. RLHF 201
3. Finetuning
4. Datasets 101
5. AI Engineer Career
6. Coding Tools
7. Evaluation
8. Open Source
9. Benchmarks 101
10. Monthy Recaps",Models,RLHF 201,Finetuning,Datasets 101,AI Engineer Career,Coding Tools,Evaluation,Open Source,Benchmarks 101,Monthy Recaps,0.1224744871,-0.1224744871,0.3224744871
The Accidental AI Canvas - with Steve Ruiz of tldraw,https://www.latent.space/p/tldraw,Steve Ruiz,"Fri, 05 Jan 2024 20:43:11 GMT",3849,https://substackcdn.com/feed/podcast/1084089/post/140363662/9ed265c4af48ed511ad47a577333b91f.jpg,https://api.substack.com/feed/podcast/140363662/933490664e48df958d799d7c5281abae.mp3,"Happy 2024! We appreciated all the feedback on the listener survey  ( still open, link here ) ! Surprising to see that some people’s favorite episodes were others’ least, but we’ll always work on improving our audio quality and booking great guests. Help us out by leaving reviews on  Twitter ,  YouTube , and  Apple Podcasts ! 🙏   Big thanks to  Chris Anderson  for the latest review - be like Chris! Note to the Audio-only Listener Because of the nature of today’s topic, it makes the most sense to follow along the demo on video rather than audio. There’s also about 30 mins of demos and technical detail that we had to remove from the audio version, because they didn’t make sense without video. Trailer  here . Full 90min chat: (In other words, pls jump over and watch on our YouTube if you can! Did you know we are now  posting every episode to YouTube ? We’ve been multimodal for a  long time !) Trend 1: GPT4-V Coding You might remember Greg Brockman’s hand-scribble-to-working-website demo from the  GPT-4 demo  from March. This was largely inaccessible to the rest of us until the GPT4-V API was released at Dev Day in November. As mentioned in our  November 2023 recap , one of the biggest viral trends was tldraw’s open source  “Make It Real”  demo: starting from a simple wireframe and text annotations, you could create a  real, functioning UI  with the click of a button.  Provoking another crisis of confidence in developer circles: And using  state charts : And provoking  responses from Excalidraw, a competitor . You can see us creating a Replit clone in this silent video here: Since our intervew the new GPT4V Coding metagame has been merging app UI’s and SQL with Supabase (another  AIE Summit speaker ) and other backend tools: *  generating SQL *  converting ERDs to SQL  ( part 2 , for  MariaDB ) *  seeding sample data *  doing migrations Trend 2: Latent Consistency Models As covered in  the Latent Space Paper Club in November , 3 papers drove a roughly ~100x acceleration in the speed of text to image generation over the past year: *  Consistency Models  (with Ilya Sutskever) *  Latent Consistency Models  (from Tsinghua) *  LCM-LoRA  (also Tsinghua, same authors) With the invaluable help of Fal.ai (friends of the show and AI Engineer Summit and progenitors of the viral GPU Rich/Poor hats mentioned on  the Semianalysis episode ), TLDraw has also been at the forefront of putting this research into production, with two projects: *  drawfast : add a prompt, start sketching into the canvas and see each stroke affect the drawing. Overlap multiple of them to extend and merge drawings. *  lens : a collaborative canvas where in real time people can draw and have their sketch turn into AI-generated art. Start drawing at the bottom and see it scroll into the magic canvas.  For nontechnical people in your life, we do recommend showing them  lens.tldraw.com  (and its predecessor that we discuss on the show) on your and their mobile devices. The Rise of Multimodal Prompting At the first AI Engineer Summit in October, Logan (our  first guest !) declared this  the Year of Multimodality . Over the next 2 months we saw an explosion of activity in multimodal:  GPT-4V ’s API release at OpenAI Dev Day ( our coverage here ),  LLaVA  ( our chat with author here  on Visual Instruction Tuning),  BakLLaVA ,  Qwen-VL ,  CogVLM , etc. On today’s episode we have  Steve Ruiz, founder of tldraw . The project originally started as an open source whiteboard that Steve built for himself and then  “accidentally made a really, really good visual multimodal prompting application  environment”.  Turns out that infinite canvas and generative models are a very good match: *  Design is iterative:  DALL-E, Midjourney, etc all work in a linear way: prompt goes in, 1-4 images come back. As you generate more, the previous images scroll away from your view. In a canvas environment, you can see the progression of your generation and visually “branch” by putting new prompts in different spaces. *  UI has “layers”:  when designing interfaces there are different layers to it: the functionality, the style, the state, etc. Some of what they are building in tldraw is bringing images into the canvas to influence different layers:  “One thing that we've done is to bring in screenshots of other apps, like here's Stripe.com, like make it look like Stripe, you know? Or like here's Linear.com, like let's do it this way”.    In the episode we spend a lot more time talking through all of these ideas and how Steve’s background in fine arts came back to being really useful in building a multi-modal AI canvas. Enjoy! Show Notes *  tldraw *  Open Source Repo *  Make Real (Wireframe to UI) *  drawfast.tldraw.com *  lens.tldraw.com *  Perfect Free Hand  and  Perfect Arrows *  “Make Real, the story so far” *  Dog CEO * Other whiteboarding products mentioned *  Excalidraw *  FigJam *  Adobe Whiteboard * See also Steve’s interviews on  the Slow Steady Pod  and  TWiSt , and subscribe to his  substack! *  TLDraw Wireframe kit *  TLDraw LLM starter",* [00:00:00] Introductions * [00:01:02] Steve's Background In Fine Arts and Transition into Tech * [00:08:22] The creation of tldraw and its open source origin * [00:15:44] The Inception and Growth of tldraw * [00:18:40] The Integration of AI with tldraw and Make It Real Feature * [00:21:56] Discussion on Multimodal Prompting and Iterative Design * [00:32:32] The Concept of Parallel Prompting in Design * [00:34:11] Impact of AI on developer jobs * [00:37:28] Additional Layers in Multimodal Prompting * [00:45:18] Introduction of DrawFast and Lens Projects * [00:50:03] tldraw 2.0 and the future of the project * [00:55:41] The Competitive Landscape of Canvas Tools and tldraw's Unique Position * [01:00:22] Advice for Founders: Following Your Interests and Desires,"In this episode of the Latent Space podcast, host Swyx welcomes Steve Ruiz, the creator behind the innovative design tool, tldraw, and another project called Make It Real. They discuss Steve's unconventional entrance into the tech industry, his background in fine art, and how he transitioned into software development and design, leveraging his creativity and analytical skills.

Steve shares his journey from utilizing his master's in visual art to moving into tech, initially through product design and later into more technical roles. This shift led him to work on challenges in interactive design and eventually to specialize in creating intuitive, collaborative experiences. His work with tldraw and Make It Real is highlighted as evolving from his unique blend of artistic and technical expertise.

Steve also delves into technical challenges, particularly around creating perfect arrows and freehand drawing capabilities, showcasing his iterative process of problem-solving in public, which garnered a significant following. His contributions have been adopted by major platforms like Canva and Excalidraw, cementing his impact on the design and development community.

The conversation also touches on the significance of open-source work and the tight-knit collaborations within the tech community that helped refine his projects. Steve emphasizes the importance of making complex functionalities accessible and user-friendly, reflected in tldraw's design and its underlying philosophy.

Swyx navigates through Steve's career trajectory, highlighting his stints at Framer and Play, and his venture into open source during the lockdown which led to the development of significant projects like perfect arrows and perfect freehand. This journey underscores the blend of art, technology, and community in Steve's work, leading to innovative solutions in design and interaction tools.

Towards the end, they touch upon the future potential of integrating AI with tools like tldraw, discussing speculative features and the exciting directions collaborative and creative tools are heading, driven by advancements in technology and Steve's forward-thinking approach.","1. The transition from fine art and studio art to tech and product design allowed Steve to bring a unique perspective to creating collaborative creative digital tools.
2. Tldraw emerged from Steve's journey into open-source projects that focused on visually and technically complex problems, like designing better arrows and implementing perfect freehand drawing in vector form.
3. The significant breakthroughs in perfect freehand drawing caught attention across multiple platforms, including notable tools like Canva and draw.io, due to its unique, efficient vector-based approach.
4. Tldraw's development was fueled by the technical and creative challenges in creating an intuitive, infinite canvas tool, leading to innovations in selection mechanisms, undo/redo operations, and general canvas interactions inspired by existing design tools.
5. Integrating AI capabilities into tldraw has expanded its utility, enabling users to convert sketches into working digital products, such as websites and interactive elements, directly through AI-generated code.
6. Tldraw serves as a platform that combines the infinite canvas concept with AI, offering a broad arena for experimenting with and deploying AI-enhanced features across various applications, not limited to design or collaboration tools.
7. The future of tldraw includes focusing more on being an accessible, open-source canvas for exploring AI capabilities rather than competing as a standalone design tool, positioning it as a universal platform for AI-assisted creative and technical projects.
8. Plans for tldraw 2.0 involve solidifying its position in the market by introducing commercial licenses while maintaining its open-source nature to foster continued innovation and application across different sectors.
9. Discussions with entities like OpenAI aim at embedding tldraw more deeply into AI ecosystems, reflecting its potential as a key tool for enhancing user interactions with AI through visual and interactive inputs.
10. Steve emphasizes following one's interests and curiosities as foundational for innovation, reflecting his path from art to tech and underscoring the importance of passion-driven projects in the tech industry.","1. **[00:07:36] Steve:** ""_It taught me to think about building things that other people are going to use. Taught me about the type of code that you write when you're in an exploratory phase rather than in an execution, like production phase._""
   
2. **[00:08:21] Steve:** ""_And slowly we made a really, really good, tight little library for doing exactly what I wanted. Like here are a bunch of mouse points or just arbitrary points, give me back a polygon that surrounds them._""
   
3. **[00:15:46] Steve:** ""_But yeah, I walked away from that... but I certainly learned how to, how to tweet and learn how to record a good GIF and learn how to talk into a microphone and all that type of stuff._""
   
4. **[00:21:56] Steve:** ""_You could also, you know, write on top. Like you could kind of modify this and maybe even give it the same type of markup that you would give to a designer... and draw some arrows or maybe paste in a screenshot and say, hey, make it look more stylistically close to this other thing._""
   
5. **[00:24:50] Steve:** ""_...but because this is tldrawn, because you can draw on top of this stuff, you could also, you know, write on top... and then what you do is you select the website that they gave you back, the previous result, along with all this markup, and you use that as the new input._""
   
6. **[00:34:11] Steve:** ""_It feels like we had gone off, you know, as collaboration and AI and stuff was going in one direction, we kind of just went off in our own weird, like, 'hey, the world is really going to need a whiteboard at some point' direction. And then it just, they kind of met us where we were at._""
   
7. **[00:41:34] Steve:** ""_...if I really needed just to count 60 seconds or something... Yeah, it's like I wasn’t doing anything different, so it was pretty good. That's a kind of a passive thing. Anyway, I shipped it at the end of November 2021..._""
   
8. **[01:00:29] Steve:** ""_Having a really good feeling for that sort of desire and being able to respect and follow that desire as like... if you really, really feel like working on thing, that might be the tip of a very complex iceberg..._""

Each of these quotes provides insights into the process of creation, exploration, learning from experiences, and the importance of following one's interests and intuition in the field of technology and product design. These perspectives could inspire AI engineers to embrace an exploratory approach, understand the value of user-centric design, and recognize the potential impacts and opportunities presented by AI and open-source collaborations.","1. Startups
2. Open Source
3. Coding Tools
4. UX
5. AI Engineer Career
6. Models
7. Finetuning
8. Multimodal
9. Agents
10. Hardware",Startups,Open Source,Coding Tools,UX,AI Engineer Career,Models,Finetuning,Multimodal,Agents,Hardware,-0.1,0.2224744871,0.20000000000000004
NeurIPS 2023 Recap — Top Startups,https://www.latent.space/p/neurips-2023-startups,Alessio + swyx,"Sat, 30 Dec 2023 16:30:00 GMT",9714,https://substackcdn.com/feed/podcast/1084089/post/140188949/a4cb321c338686f6d72d54120b467535.jpg,https://api.substack.com/feed/podcast/140188949/767b140a547ef05b86ecf161c4e56b98.mp3,"We are running an  end of year listener survey ! Please let us know any feedback you have, what episodes resonated with you, and guest requests for 2024!  Survey link here . We can’t think of a more Latent-Space-y way to end 2023 than with a mega episode featuring many old and new friends recapping their biggest news, achievements, and themes and memes of the year! We previously covered  the Best Papers of NeurIPS 2023 , but the other part of NeurIPS being an industry friendly conference is all the startups that show up to hire and promote their latest and greatest products and papers! As a startup-friendly podcast, we of course were ready with our mics to talk to everyone we could track down. In lieu of an extended preamble, we encourage you to listen and click through all the interviews and show notes, all of which have been curated to match the references mentioned in the episode.","& Show Notes *  [00:01:26]  Jonathan Frankle - Chief Scientist,  MosaicML/Databricks * see also  the Mosaic/MPT-7B episode * $1.3B  MosaicML x Databricks acquisition *  [00:22:11 ] Lin Qiao - CEO,  Fireworks AI *  Fireworks Mixtral *  [00:38:24]  Aman Sanger - CEO,  Anysphere (Cursor) * see also  the Cursor episode *  $8m seed from OpenAI * Tweet:  Request-level memory-based KV caching * Tweet:  GPT-4 grading and Trueskill ratings for rerankers *  [00:51:14]  Aravind Srinivas - CEO,  Perplexity * 1m app installs on  iOS  and  Android *  pplx-online api 7b and 70b models *  Shaan Puri/Paul Graham Fierce Nerds story * [01:04:26] Will Bryk - CEO,  Metaphor * “ Andrew Huberman may have singlehandedly ruined the SF social scene ” * [01:12:49] Jeremy Howard - CEO,  Answer.ai * see also  the End of Finetuning episode *  Jeremy’s podcast with Tanishq Abraham ,  Jess Leao *  Announcing Answer.ai with $10m from Decibel VC *  Laundry Buddy , Nov 2023 AI  Meme of the Month  * [01:37:13]  Joel Hestness  - Principal Scientist,  Cerebras *  CerebrasGPT , all  the Cerebras papers we discussed * [01:56:34] Jason Corso - CEO,  Voxel51 *  Open Source FiftyOne project *  CVPR Survival Guide * [02:02:39] Brandon Duderstadt - CEO,  Nomic.ai *  GPT4All ,  Atlas ,  Demo * [02:12:39] Luca Antiga - CTO,  Lightning.ai *  Pytorch Lightning ,  Lightning Studios ,  LitGPT * [02:29:46] Jay Alammar - Engineering Fellow,  Cohere *  The Illustrated Transformer   Get full access to Latent Space at  www.latent.space/subscrib",It seems like you might have made a typo or there's an error in the text you want to be summarized. Could you please provide the podcast transcript or the relevant details you need summarized?,"I'm sorry, but you didn't provide a transcript to analyze for AI research insights. Could you please provide the transcript or details needed for me to help you?",,"1. News
2. Event Recaps
3. Monthy Recaps
4. Finetuning
5. Open Source
6. Hardware
7. Startups
8. Coding Tools
9. LLM Tooling
10. AI Engineer Career",News,Event Recaps,Monthy Recaps,Finetuning,Open Source,Hardware,Startups,Coding Tools,LLM Tooling,AI Engineer Career,0.1,0.2224744871,0.0
NeurIPS 2023 Recap — Best Papers,https://www.latent.space/p/neurips-2023-papers,Alessio + swyx,"Sat, 23 Dec 2023 23:44:13 GMT",12026,https://substackcdn.com/feed/podcast/1084089/post/139971607/4f181e565ee0b6df421c5adfab3d1430.jpg,https://api.substack.com/feed/podcast/139971607/26b6512678ee9011a4167f5f11b6c908.mp3,"We are running an  end of year listener survey ! Please let us know any feedback you have, what episodes resonated with you, and guest requests for 2024!  Survey link here . NeurIPS 2023  took place from Dec 10–16 in New Orleans. The Latent Space crew was onsite for as many of the talks and workshops as we could attend (and more importantly,  hosted cocktails  and parties after hours)! Picking from the  3586 papers  accepted to the conference ( available online ,  full schedule  here) is an impossible task, but we did our best to present an audio guide with brief commentary on each. We also recommend  MLContests.com NeurIPS recap  and  Seb Ruder’s NeurIPS primer  and  Jerry Liu’s paper picks . We also found  the VizHub guide  useful for a t-SNE clustering of papers. Lots also happened in the arxiv publishing world outside NeurIPS, as  highlighted by Karpathy , especially DeepMind’s  Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models . Jan 2024 update: we also  strongly recommend   ‘s pick of the year’s  10 best papers ,  including Pythia . We’ll start with the  NeurIPS Best Paper Awards , and then go to a selection of non-awarded but highly influential papers, and then arbitrary personal picks to round out the selection. Where we were able to do a poster session interview, please scroll to the relevant show notes for images of their poster for discussion. We give Chris Ré the last word due to the Mamba and StripedHyena state space models drawing particular excitement but still being too early to assess impact.","* [0:01:19] Word2Vec (Jeff Dean, Greg Corrado) * [0:15:28] Emergence Mirage (Rylan Schaeffer) * [0:28:48] DPO ( Rafael Rafailov ) * [0:41:36] DPO Poster Session ( Archit Sharma ) * [0:52:03] Datablations (Niklas Muennighoff) * [1:00:50] QLoRA (Tim Dettmers) * [1:12:23] DataComp (Samir Gadre) * [1:25:38] DataComp Poster Session (Samir Gadre, Alex Dimakis) * [1:35:25] LLaVA (Haotian Liu) * [1:47:21] LLaVA Poster Session (Haotian Liu) * [1:59:19] Tree of Thought (Shunyu Yao) * [2:11:27] Tree of Thought Poster Session (Shunyu Yao) * [2:20:09] Toolformer ( Jane Dwivedi-Yu ) * [2:32:26] Voyager (Guanzhi Wang) * [2:45:14] CogEval (Ida Momennejad) * [2:59:41] State Space Models (Chris Ré) Papers covered *  Distributed Representations of Words and Phrases and their Compositionality  (Word2Vec)  Tomas Mikolov · Ilya Sutskever · Kai Chen · Greg Corrado · Jeff Dean . The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several improvements that make the Skip-gram model more expressive and enable it to learn higher quality vectors more rapidly. We show that by subsampling frequent words we obtain significant speedup, and also learn higher quality representations as measured by our tasks. We also introduce Negative Sampling, a simplified variant of Noise Contrastive Estimation (NCE) that learns more accurate vectors for frequent words compared to the hierarchical softmax. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of Canada'' and ""Air'' cannot be easily combined to obtain ""Air Canada''. Motivated by this example, we present a simple and efficient method for finding phrases, and show that their vector representations can be accurately learned by the Skip-gram model. * Some  notable reflections from Tomas Mikolov  - and debate over  the Seq2Seq paper credit  with Quoc Le *  Are Emergent Abilities of Large Language Models a Mirage? (Schaeffer et al.) .  Emergent abilities are abilities that are present in large-scale models but not in smaller models and are hard to predict. Rather than being a product of models’ scaling behavior, this paper argues that emergent abilities are mainly an artifact of the choice of metric used to evaluate them. Specifically, nonlinear and discontinuous metrics can lead to sharp and unpredictable changes in model performance. Indeed, the authors find that when accuracy is changed to a continuous metric for arithmetic tasks where emergent behavior was previously observed, performance improves smoothly instead.  So while emergent abilities may still exist, they should be properly controlled and researchers should consider how the chosen metric interacts with the model. *  Direct Preference Optimization: Your Language Model is Secretly a Reward Model (Rafailov et al.) * While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model.  *  In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training , essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning.  * Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train. See also  on DPO: and  recent Twitter discussions *  Scaling Data-Constrained Language Models (Muennighoff et al.) * The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models.  We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data . However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at  https://github.com/huggingface/datablations . * 2 minute  poster session presentation   video *  QLoRA: Efficient Finetuning of Quantized LLMs (Dettmers et al.).   * This paper proposes QLoRA, a more memory-efficient (but  slower ) version of LoRA that uses several optimization tricks to save memory. They train a new model, Guanaco, that is fine-tuned only on a single GPU for 24h and outperforms previous models on the Vicuna benchmark.  Overall, QLoRA enables using much fewer GPU memory for fine-tuning LLMs.  Concurrently, other methods such as  4-bit LoRA quantization  have been developed that achieve similar results. *  DataComp: In search of the next generation of multimodal datasets (Gadre et al.) * Multimodal datasets are a critical component in recent breakthroughs such as CLIP, Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem,  we introduce DataComp, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl.  Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets.  * Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DataComp workflow leads to better training sets.  Our best baseline, DataComp-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI's CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute.  We release \datanet and all accompanying code at www.datacomp.ai. *  Visual Instruction Tuning (Liu et al) * Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data.  * By instruction tuning on such generated data,  we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding. * Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available. *  Tree of Thoughts: Deliberate Problem Solving with Large Language Models  (Yao et al) * Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role.  * To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving.  * ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. * Our experiments show that ToT significantly enhances language models’ problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%.  * Code repo with all prompts:  https://github.com/princeton-nlp/tree-of-thought-llm . *  Toolformer: Language Models Can Teach Themselves to Use Tools  (Schick et al) * LMs exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller specialized models excel.  * In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds.  * We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.  * This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar.  * Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities. *  Voyager: An Open-Ended Embodied Agent with Large Language Models  (Wang et al) * We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components:  * 1) an automatic curriculum that maximizes exploration,  * 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and  * 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement.  * Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. Voyager discovers new Minecraft items and skills continually by self-driven exploration, significantly outperforming the baselines. *  Evaluating Cognitive Maps and Planning in Large Language Models with CogEval (Momennejad et al) * Recently an influx of studies claims emergent cognitive abilities in large language models (LLMs). Yet, most rely on anecdotes, overlook contamination of training sets, or lack systematic Evaluation involving multiple tasks, control conditions, multiple iterations, and statistical robustness tests. Here we make two major contributions.  * First, we propose CogEval, a cognitive science-inspired protocol for the systematic evaluation of cognitive capacities in LLMs. The CogEval protocol can be followed for the evaluation of various abilities.  *  * Second, here we follow CogEval to systematically evaluate cognitive maps and planning ability across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base our task prompts on human experiments, which offer both established construct validity for evaluating planning, and are absent from LLM training sets. *  * We find that, while LLMs show apparent competence in a few planning tasks with simpler structures, systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and falling in loops. These findings do not support the idea of emergent out-of-the-box planning ability in LLMs. This could be because LLMs do not understand the latent relational structures underlying planning problems, known as cognitive maps, and fail at unrolling goal-directed trajectories based on the underlying structure. Implications for application and future directions are discussed. *  Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Albert Gu, Tri Dao) * Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a  key weakness of such models is their inability to perform content-based reasoning , and make several improvements.  * First, simply  letting the SSM parameters be functions of the input  addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.  * Second, even though this change prevents the use of efficient convolutions,  we design a hardware-aware parallel algorithm in recurrent mode . We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).  *  Mamba enjoys fast inference (5x higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences.  As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics.  On language modeling, our Mamba-1.4B model outperforms Transformers of the same size and matches Transformers twice its size , both in pretraining and downstream evaluation. *    Get full access to Latent Space at  www.latent.space/subscrib",It seems like there might have been an error with your request. Could you please provide the podcast transcript or details about the podcast episode that you would like summarized?,"I'm sorry, but you haven't provided a transcript for me to analyze. Could you please provide the text you'd like me to extract AI research insights from?",,"1. News
2. Event Recaps
3. Monthy Recaps
4. Startups
5. Open Source
6. AI Engineer Career
7. Multimodal
8. Hardware
9. Finetuning
10. Coding Tools",News,Event Recaps,Monthy Recaps,Startups,Open Source,AI Engineer Career,Multimodal,Hardware,Finetuning,Coding Tools,-0.2224744871,0.44494897420000007,0.1
The AI-First Graphics Editor - with Suhail Doshi of Playground AI,https://www.latent.space/p/suhail-doshi,Alessio + swyx,"Wed, 20 Dec 2023 20:31:17 GMT",3429,https://substackcdn.com/feed/podcast/1084089/post/139926717/0bc55586c2dad820c64a8426a3341a4c.jpg,https://api.substack.com/feed/podcast/139926717/539de53dc38b0493fa56c848002c21d0.mp3,"We are running an  end of year survey  for our listeners! Please let us know any feedback you have, what episodes resonated with you, and guest requests for 2024!  Survey link here! Before language models became all the rage in November 2022, image generation was the hottest space in AI (it was the subject of  our first piece on Latent Space !) In our interview with  Sharif Shameem  from  Lexica  we talked through the launch of StableDiffusion and the early days of that space. At the time, the toolkit was still pretty rudimentary: Lexica made it easy to search images, you had the  AUTOMATIC1111 Web UI  to generate locally, some HuggingFace spaces that offered inference, and eventually DALL-E 2 through OpenAI’s platform, but not much beyond basic text-to-image workflows. Today’s guest,  Suhail Doshi , is trying to solve this with  Playground AI , an image editor reimagined with AI in mind. Some of the differences compared to traditional text-to-image workflows: *  Real-time preview rendering using consistency : as you change your prompt, you can see changes in real-time before doing a final rendering of it. *  Style filtering:  rather than having to prompt exactly how you’d like an image to look, you can pick from a whole range of filters both from Playground’s model as well as Stable Diffusion (like RealVis, Starlight XL, etc). We talk about this at 25:46 in the podcast. *  Expand prompt:  similar to DALL-E3, Playground will do some prompt tuning for you to get better results in generation. Unlike DALL-E3, you can turn this off at any time if you are a prompting wizard *  Image editing:  after generation, you have tools like a magic eraser, inpainting pencil, etc. This makes it easier to do a full workflow in Playground rather than switching to another tool like Photoshop. Outside of the product, they have also trained a new model from scratch,  Playground v2 , which is fully open source and  open weights  and allows for commercial usage.  They benchmarked the model against  SDXL  across 1,000 prompts and found that humans preferred the Playground generation 70% of the time. They had similar results on  PartiPrompts : They also created a new benchmark,  MJHQ-30K , for “aesthetic quality”: We introduce a new benchmark,  MJHQ-30K , for automatic evaluation of a model’s aesthetic quality. The benchmark computes FID on a high-quality dataset to gauge aesthetic quality. We curate the high-quality dataset from Midjourney with 10 common categories, each category with 3K samples. Following common practice, we use aesthetic score and CLIP score to ensure high image quality and high image-text alignment. Furthermore, we take extra care to make the data diverse within each category. Suhail was pretty open with saying that Midjourney is currently the best product for imagine generation out there, and that’s why they used it as the base for this benchmark.  I think it's worth comparing yourself to maybe the best thing and try to find like a really fair way of doing that. So I think more people should try to do that. I definitely don't think you should be kind of comparing yourself on like some Google model or some old SD, Stable Diffusion model and be like, look, we beat Stable Diffusion 1.5. I think users ultimately want care, how close are you getting to the thing that people mostly agree with? [00:23:47] We also talked a lot about Suhail’s founder journey from starting Mixpanel in 2009, then going through YC again with Mighty, and eventually sunsetting that to pivot into Playground. Enjoy! Show Notes *  Suhail’s Twitter *  “Starting my road to learn AI” *  Bill Gates book trip *  Playground *  Playground v2 Announcement *  $40M raise announcement * “ Running infra dev ops for 24 A100s ” *  Mixpanel * Mighty *  “I decided to stop working on Mighty” *  Fast.ai *  Civit",* [00:00:00] Intros * [00:02:59] Being early in ML at Mixpanel * [00:04:16] Pivoting from Mighty to Playground and focusing on generative AI * [00:07:54] How DALL-E 2 inspired Mighty * [00:09:19] Reimagining the graphics editor with AI * [00:17:34] Training the Playground V2 model from scratch to advance generative graphics * [00:21:11] Techniques used to improve Playground V2 like data filtering and model tuning * [00:25:21] Releasing the MJHQ30K benchmark to evaluate generative models * [00:30:35] The limitations of current models for detailed image editing tasks * [00:34:06] Using post-generation user feedback to create better benchmarks * [00:38:28] Concerns over potential misuse of powerful generative models * [00:41:54] Rethinking the graphics editor user experience in the AI era * [00:45:44] Integrating consistency models into Playground using preview rendering * [00:47:23] Interacting with the Stable Diffusion LoRAs community * [00:51:35] Running DevOps on A100s * [00:53:12] Startup ideas?,"In this episode of the Latent Space podcast, co-hosts Alessio, a partner and CTO-in-Residence at Decibel Partners, and Swyx, founder of Smol AI, talk with guest Suhail Doshi. Suhail is the founder of Playground and previously co-founded Mixpanel and Mighty. He shares his journey from Mixpanel, where they experimented with machine learning to predict user behavior, to founding Mighty, aimed at enhancing browsing experiences, and eventually starting Playground, focused on pushing the boundaries of AI in image generation. 

Suhail discusses the early challenges and explorations of using AI in Mixpanel for predicting user engagement and the evolution of his interest in AI, leading him to train with Fast AI courses. The conversation then shifts to Mighty's vision to improve computing efficiency and the shift toward Playground, highlighting the role of AI models in creating efficient computational solutions and the fascination with generative AI’s potential.

Suhail also delves into Playground's mission, emphasizing high-quality image generation and the importance of user-friendly interfaces for AI tools, contrasting traditional text-based prompts with more intuitive, visual interactions for users. The discussion covers the challenges and intricacies of building powerful AI models and operational infrastructures, including the advantages and limits of existing tools and training methods in AI development.

The podcast wraps up with Suhail's insights on the future of AI, discussing the potential for undiscovered modalities beyond current focuses in vision, language, and audio, pondering the possibilities of sophisticated models that could simulate physical and human experiences with high fidelity. He also shares advice for aspiring AI founders, encouraging hands-on experimentation and learning aligned with one's curiosity and project goals.

This episode offers a glimpse into the journey of an AI entrepreneur navigating the challenges and opportunities in AI development, from machine learning applications in analytics to envisioning future AI capabilities that could transform our interaction with technology.","1. Development of AI models for predicting user churn, conversion rates, and anomaly detection using logistic regression at Mixpanel.
2. The concept of running browser sessions from centralized data centers to address computing limitations, leading to the concept of novel computational models, which influenced the foundation of Mighty.
3. The shift towards considering AI and massive parallel computations as solutions for enhancing computing capabilities beyond traditional hardware limitations.
4. Insights into creating Playground, emphasizing graphic creativity through AI, with a focus on underinvested areas like graphics compared to language models.
5. Introduction of Playground V2, a model trained from scratch, open-sourced to contribute to community-driven innovation especially in image generation.
6. Discussion on the fundamental importance and potential of understanding and expanding AI model capabilities beyond current limitations, including novel modalities and comprehensive world modeling.","**1. On the importance of data and predictive features in AI applications**
- [00:00:59] **Suhail**: ""perhaps we could predict things. So we built two or three different features, I think we built a feature where we could predict whether users would churn from your product. We made a feature that could predict whether users would convert, we built a feature that could do anomaly detection...most of it boiled down to just like, using logistic regression, and it never quite seemed very groundbreaking in the end.""

**2. Shifting compute and the realization of limitations in current computing architectures**
- [00:03:50] **Suhail**: ""Why do I need to think about upgrading my computer ever...it seems like a lot of applications are just now in the browser...the browser was effectively becoming more or less our operating system over time...we learned...most of a web app is just going to be single-threaded JavaScript, then the only thing you could do...make a better CPU.""

**3. On unsolved challenges and future directions in AI model capabilities**
- [00:17:43] **Suhail**: ""character consistency...multitask editing...style transfer is still very limited. Just even reasoning around images, you know, what's going on in an image, that kind of thing. Things are still very, very underpowered, very nascent. So therefore the utility is very, very limited.""

**4. Reflecting on the nascent stage of AI and the evolving benchmarks for evaluating models**
- [00:32:43] **Suhail**: ""My sense is that we're going to take all the things that we notice that the users kind of were failing at and try to find like new ways to measure that...because we're not putting these in papers and trying to like win...You know, I think users ultimately want care, how close are you getting to the thing that people mostly agree with?""

**5. On the potential of exploring beyond conventional modalities in AI**
- [00:53:35] **Suhail**: ""I think we are lacking imagination as a species in this regard...there are probably far more modalities than meets the eye...there's some very difficult problems, like a true actual, not a meta world model, but an actual world model that truly maps everything that's going in terms of like physics and fluids and all these various kinds of interactions.""

These quotes underscore the ongoing trials, realizations, and aspirations that can inspire AI engineers to rigorously explore and innovate within the domain, especially focusing on data utilization, compute optimization, improving AI model capabilities, the importance of evolving benchmarks, and the exploration beyond known modalities.","1. Startups
2. Models
3. Hardware
4. Finetuning
5. AI Engineer Career
6. UX
7. LLM Tooling
8. Open Source
9. Multimodal
10. Coding Tools",Startups,Models,Hardware,Finetuning,AI Engineer Career,UX,LLM Tooling,Open Source,Multimodal,Coding Tools,0.1224744871,0.2224744871,0.1
"The ""Normsky"" architecture for AI coding agents — with Beyang Liu + Steve Yegge of SourceGraph",https://www.latent.space/p/sourcegraph,Steve Yegge and Beyang Liu,"Thu, 14 Dec 2023 18:48:22 GMT",4777,https://substackcdn.com/feed/podcast/1084089/post/139766004/678c84ee06687fbc645c5e819af2e519.jpg,https://api.substack.com/feed/podcast/139766004/c0ad8bfde2971b1dffed911f471cecbe.mp3,"We are running an  end of year survey  for our listeners. Let us know any feedback you have for us, what episodes resonated with you the most, and guest requests for 2024!   RAG has emerged as one of the key pieces of the AI Engineer stack. Jerry from LlamaIndex  called it a “hack” , Bryan from Hex  compared it to “a recommendation system from LLMs” , and even LangChain  started with it .  RAG is crucial in any AI coding workflow. We talked about context quality for code in our  Phind episode . Today’s guests,  Beyang Liu  and  Steve Yegge  from  SourceGraph , have been focused on code indexing and retrieval for over 15 years. We locked them in our new studio to record a 1.5 hours masterclass on the history of code search, retrieval interfaces for code, and how they get  SOTA 30% completion acceptance rate  in their  Cody  product by being better at the “bin packing problem” of LLM context generation.  Google Grok → SourceGraph → Cody While at Google in 2008, Steve built Grok, which lives on today as  Google Kythe . It allowed engineers to do code parsing and searching across different codebases and programming languages. (You might remember  the infamous Google Platforms Rant  from Steve’s time at Google, and his  2021 followup on GCP ).  Beyang was an intern at Google at the same time, and Grok became the inspiration to start SourceGraph in 2013. The two didn’t know eachother personally until Beyang brought Steve  out of retirement  9 years later to join him as VP Engineering. Fast forward 10 years,  SourceGraph has become to best code search tool out there and raised $223M along the way .  Nine months ago, they open sourced  SourceGraph Cody , their AI coding assistant. All their code indexing and search infrastructure allows them to get SOTA results by having better RAG than competitors: *  Code completions  as you type that achieve an industry-best  Completion Acceptance Rate (CAR) as high as 30%  using a context-enhanced open-source LLM (StarCoder) *  Context-aware chat  that provides the option of using  GPT-4 Turbo, Claude 2, GPT-3.5 Turbo, Mistral 7x8B, or Claude Instant , with more model integrations planned *  Doc and unit test generation , along with  AI quick fixes  for common coding errors *  AI-enhanced natural language code search , powered by a hybrid dense/sparse vector search engine  There are a few pieces of infrastructure that helped Cody achieve these results: Dense-sparse vector retrieval system  For many people, RAG = vector similarity search, but there’s a lot more that you can do to get the best possible results. From their release: "" Sparse vector search "" is a fancy name for keyword search that potentially incorporates LLMs for things like ranking and term expansion (e.g., ""k8s"" expands to ""Kubernetes container orchestration"", possibly weighted as in  SPLADE ):  * Dense vector retrieval makes use of embeddings, the internal representation that LLMs use to represent text. Dense vector retrieval provides recall over a broader set of results that may have no exact keyword matches but are still semantically similar.  * Sparse vector retrieval is very fast, human-understandable, and yields high recall of results that closely match the user query.  * We've found the approaches to be complementary. There’s a very good  blog post by Pinecone  on SPLADE for sparse vector search if you’re interested in diving in. If you’re building RAG applications in areas that have a lot of industry-specific nomenclature, acronyms, etc, this is a good approach to getting better results. SCIP In 2016, Microsoft announced the  Language Server Protocol (LSP)  and the Language Server Index Format (LSIF). This protocol makes it easy for IDEs to get all the context they need from a codebase to get things like file search, references, “go to definition”, etc.  SourceGraph developed SCIP, “a better code indexing format than LSIF”: *  Simpler and More Efficient Format : SCIP utilizes Protobuf instead of JSON, which is used by LSIF. Protobuf is more space-efficient, simpler, and more suitable for systems programming.  *  Better Performance and Smaller Index Sizes : SCIP indexers, such as scip-clang, show enhanced performance and reduced index file sizes compared to LSIF indexers (10%-20% smaller) *  Easier to Develop and Debug : SCIP's design, centered around human-readable string IDs for symbols, makes it faster and more straightforward to develop new language indexers.  Having more efficient indexing is key to more performant RAG on code.  Show Notes *  Sourcegraph *  Cody *  Copilot vs Cody *  Steve’s Stanford seminar on Grok *  Steve’s blog *  Grab *  Fireworks *  Peter Norvig *  Noam Chomsky * Code search *  Kelly Norton *  Zoekt *  v0.dev See also our past episodes on  Cursor ,  Phind ,  Codeium  and  Codium  as well as  the GitHub Copilot keynote at AI Engineer Summit .","* [00:00:00] Intros & Backgrounds * [00:05:20] How Steve's work on Grok inspired SourceGraph for Beyang * [00:08:10] What's Cody? * [00:11:22] Comparison of coding assistants and the capabilities of Cody * [00:16:00] The importance of context (RAG) in AI coding tools * [00:21:33] The debate between Chomsky and Norvig approaches in AI * [00:30:06] Normsky: the Norvig + Chomsky models collision * [00:36:00] The death of the DSL? * [00:40:00] LSP, Skip, Kythe, BFG, and all that fun stuff * [00:53:00] The SourceGraph internal stack * [00:58:46] Building on open source models * [01:02:00] SourceGraph for engineering managers? * [01:12:00] Lightning Round","In the podcast, hosts Alessio and Swyx, of the Latent Space podcast, invite guests Beyang and Steve from Sourcegraph to discuss the evolution and impact of AI in software development. Beyang, having a background in computer vision and having worked at notable companies like Palantir and Google, co-founded Sourcegraph with Quinn, aiming to index all code on the internet to alleviate pain points in dealing with large complex codebases at Palantir and its clients. Steve, who has worked at Amazon and Google, and later joined Grab as head of engineering, shares insights on his journey and the success of Grab due to its execution and westernized approach that appealed globally.

The conversation delves into the development of code search and intelligence tools like Google's Grok and its open-source successor Kythe, as well as other projects like Hound and Zoekt that have inspired Sourcegraph's indexing and code intelligence capabilities. The discussion also touches upon the one-year anniversary of ChatGPT and the potential of AI coding agents like Sourcegraph's Cody, a tool designed to make coding more efficient by offering context-aware code suggestions and automating mundane tasks like generating unit tests or documentation.

Furthermore, the podcast covers the importance and challenges of data pre-processing in AI, the role and trajectory of open-source models in the AI landscape, and the evolving dynamics of code creation, management, and understanding in the face of rapid advancements in AI technologies. Steve and Beyang emphasize the necessity for developers to adapt to these technologies, suggesting an impending paradigm shift in software development practices. They envision a future where AI not only aids in code generation but also plays a pivotal role in understanding and managing the complexity of large codebases.","1. The best AI development tools in the future will need to leverage forms of intelligence that include, but are not limited to, natural language processing, vision models, and code analysis, embodying a synthesis of Chomsky's formal systems and Norvig's data-driven models, potentially termed as ""Normsky architecture.""

2. Reliable and first-try accurate code generation, even for simple functions, remains a significant and unsolved challenge that must be tackled for the feasibility of more autonomous, agentic software development processes.

3. The rapid acceleration of AI and big language model (LLM) capabilities, particularly in code generation and understanding complex codebases, surpassed initial expectations and offer transformative tools for software development.

4. Developing effective and efficient developer productivity monitoring and evaluation tools, which correlate individual contributions to broader business metrics and outcomes, remains a critical need within the industry.

5. The future of AI coding assistants and dev tools may explore multimodal interactions (e.g., integrating architecture diagrams, entity relationship diagrams, or continuous screen monitoring), pushing beyond current usage patterns and triggering based on user actions.

6. The creation of a large, clean dataset of both naturally occurring and synthetic code data beneficial for training more accurate and efficient AI models for code generation and understanding remains an underserved area, with significant potential for impact.

7. A noticeable trend towards the integration and appreciation of open-source models in the AI development tools ecosystem, suggesting a future where collaboration and shared innovation drive advancements in coding assistance technology.

8. The notion of ""agentic execution"" for software development, where multi-step processes are fully automated without human intervention, while highly sought after, may not be fully realizable with current transformer-based language models alone, indicating a need for further advances in AI and computational technologies.","Given the extensive and insightful discussion, here's a selection of actionable quotes that might impact the thinking or behavior of an AI engineer:

1. **On the importance of precise context for AI coding assistants** (around [01:09:53] by Beyang): 
   - ""We think the core differentiator is really the quality of the context, which is hard to describe succinctly. It's a bit like saying, what's the difference between Google and AltaVista? There's not like a quick checklist of features that you can rattle off, but it really just comes down to all the attention and detail that we've paid to making that context work well and be high quality and fast for human devs.""

2. **On the future of code generation and AI's role** (around [01:22:32] by Beyang):
   - ""I think we are working towards that. Like, that is the eventual goal (fully automated software development). I think it's specifically the approach of, like, hey, can we have a transformer-based LM alone be the kind of, like, backbone or the orchestrator of these agentic flows? Where we're a little bit more bearish today.""

3. **On leveraging AI for developer productivity** (around [01:11:22] by Beyang):
   - ""The work that you did impacted these core metrics because these features were tied to those and therefore we can afford to invest more in this part of the organization. And that's what we really want to drive towards.""

Each of these quotes directly addresses the evolution and application of AI in engineering environments, offering valuable insights into practical and strategic considerations for AI engineers.","1. Models
2. Agents
3. Open Source
4. AI Engineer Career
5. Coding Tools
6. LLM Tooling
7. RHLF 201
8. Hardware
9. Finetuning
10. Multimodal",Models,Agents,Open Source,AI Engineer Career,Coding Tools,LLM Tooling,RHLF 201,Hardware,Finetuning,Multimodal,0.3449489742,0.2224744871,0.0
"The Busy Person's Intro to Finetuning & Open Source AI - Wing Lian, Axolotl",https://www.latent.space/p/axolotl,Wing LIan and Alex Volkov,"Fri, 08 Dec 2023 16:16:07 GMT",3858,https://substackcdn.com/feed/podcast/1084089/post/139607453/f95ee35d5ec11050e7ac5006511e8c39.jpg,https://api.substack.com/feed/podcast/139607453/afd770993b7bc907e4751045f8e38f63.mp3,"The Latent Space crew will be  at NeurIPS on Tuesday ! Reach out with any parties and papers of interest. We have also been incubating  a smol daily AI Newsletter  and  Latent Space University  is making progress. Good open models like  Llama 2  and  Mistral 7B  (which has just released  an 8x7B MoE model ) have enabled their own sub-industry of finetuned variants for a myriad of reasons: *  Ownership & Control  - you take responsibility for serving the models *  Privacy  - not having to send data to a third party vendor *  Customization  - Improving some attribute (censorship, multiturn chat and chain of thought, roleplaying) or benchmark performance (without cheating) Related to improving benchmark performance is the ability to use smaller (7B, 13B) models, by matching the performance of larger models, which have both cost and inference latency benefits. Core to all this work is finetuning, and the emergent finetuning library of choice has been Wing Lian’s  Axolotl . Axolotl Axolotl is an LLM fine-tuner supporting SotA techniques and optimizations for a variety of common model architectures: It is used by many of the leading open source models: *  Teknium : OpenHermes, Trismigestus, CollectiveCognition *  OpenOrca : Mistral-OpenOrca, Mistral-SlimOrca *  Nous Research : Puffin, Capybara, NousHermes *  Pygmalion : Mythalion, Pygmalion *  Eric Hartford : Dolphin, Samantha *  DiscoResearch : DiscoLM 120B & 70B *  OpenAccess AI Collective : Manticore, Minotaur, Jackalope, Hippogriff As finetuning is very formatting dependent, it also provides prompt interfaces and formatters between  a range of popular model formats  from Stanford’s Alpaca and   Steven Tey’s ShareGPT  (which led to  Vicuna ) to the more NSFW Pygmalion community. Nous Research Meetup We last talked about Nous at  the DevDay Recap  at the e/acc “ banger rave ”. We met Wing at the Nous Research meetup at the a16z offices in San Francisco, where they officially announced their company and future plans: Including Nous Forge: Show Notes We’ve already covered the nuances of  Dataset Contamination  and  the problems with “Open Source” in AI , so we won’t rehash those topics here but do read/listen to those if you missed it. *  Axolotl GitHub and Discord *  The Flan paper and dataset *  StackLlama  model and blogpost *  Multipack paper *  Our episode with Tri Dao *  Mamba  state space models -  Tri Dao  and  Albert Gu","* [00:00:00] Introducing Wing * [00:02:34] SF Open Source AI Meetup * [00:04:09] What is Axolotl? * [00:08:01] What is finetuning? * [00:08:52] Open Source Model Zoo * [00:10:53] Benchmarks and Contamination * [00:14:29] The Case for Open Source AI * [00:17:34] Orca and OpenOrca * [00:23:36] DiscoLM and Model Stacking * [00:25:07] Datasets and Evals over Models * [00:29:15] Distilling from GPT4 * [00:33:31] Finetuning - LoRA, QLoRA, ReLoRA, GPTQ * [00:41:55] Axolotl vs HF Transformers * [00:48:00] 20x efficiency with StackLlama and Multipack * [00:54:47] Tri Dao and Mamba * [00:59:08] Roadmap for Axolotl * [01:01:20] The Open Source AI Community","The podcast features Wing Lian discussing his journey and contributions to the open-source AI community, primarily focusing on his work with Axolotl and the Open Access AI Collective. He shares his background, starting from scraping apartment websites for listings as a college student to becoming deeply involved in AI and open-source projects. Wing's significant contributions include maintaining Axolotl, a tool that has gained traction within the community for its role in model training and development. The conversation also delves into the evolving landscape of open source AI, including advancements and collaborative efforts mentioned, such as work on fine-tuning techniques like LoRa, QLoRa, and Reinforcement Learning with Human Feedback (RLHF). The guests discuss the significance of open-source models, methodologies for fine-tuning, and the impact of recent developments like Mamba—an attentionless model architecture. Wing emphasizes the importance of community engagement and contributions to further develop and improve tools like Axolotl, highlighting the collective effort in advancing open-source AI technologies.","1. Introduction of the concept of finesse tuning models, where a base or foundational model is specifically tuned to improve its performance for particular tasks or styles of responses.
2. Fine tuning can be done in an efficient and parameter-efficient manner using methods like LoRa, QLoRa, and ReLoRa, which allow further training with less computational cost.
3. Multipack and StackLlama techniques offer significant improvements in training efficiency by optimizing how data is fed into the models, potentially offering up to 20x efficiency gains.
4. The creation of models such as OpenOrca to replicate Orca's chain-of-thought reasoning capabilities, highlighting the importance of open reproductions and improvements in AI models.
5. Highlighting the role and benefits of an open-source AI community in contributing to the rapid iteration and improvement of models, as well as the critical nature of clean and open datasets for training these models.
6. The introduction of Mamba, an attentionless model architecture promising significant performance improvements and efficiency, particularly for larger models and longer contexts.
7. The importance of community contributions to open-source AI projects like Axolotl for its development, maintenance, and innovation.","Here are the most actionable quotes that might impact the thinking or behavior of an AI Engineer from the transcript provided:

1. **[00:37:47] Wing Lian**: ""For me, it was just releasing models was a way to try and, like, continue to build out the product, figure out what I needed to put into the product, how do I make it faster, and, if you've got to, like, go and debug your product, you may as well have it do something useful.""

2. **[00:54:57] Alex Volkov**: ""Between running to this open source AI, driving here in Uber, like, you already put out something that I saw that, that you started. Wait, what? Something today?""

   **[00:55:07] Wing Lian**: ""Well, I mean, so like tree and I forget who the other author is on that paper. They had released sort of the modeling code on, on GitHub.""

3. **[01:01:10] Wing Lian**: ""Given that sort of like, if I follow that through, that thought through, it's like, well, companies don't want to use this if it's hard for them to like, if given their specific use cases, right, they might need something specific in the workflow that they, and I, what I don't want is to have is them having to fork it...""

4. **[01:02:38] Wing Lian**: ""So, for me, the biggest thing is, like, I'm looking, I'm always looking for contributors... A lot of the, sort of like, if I had somebody that could have gone and done Mamba for me, that would make my life a hundred times easier, right?""

These quotes encapsulate the mentality of continuous improvement, the fast-paced nature of AI development, the importance of community contributions, and the challenges of creating a developer-friendly platform that can adapt to various use cases.","1. Open Source
2. Finetuning
3. Hardware
4. Models
5. FlashAttention
6. Multimodal
7. Coding Tools
8. AI Engineer Career
9. Benchmarks 101
10. Datasets 101",Open Source,Finetuning,Hardware,Models,FlashAttention,Multimodal,Coding Tools,AI Engineer Career,Benchmarks 101,Datasets 101,0.022474487099999996,0.0,0.1
Notebooks = Chat++ and RAG = RecSys! — with Bryan Bischof of Hex Magic,https://www.latent.space/p/bryan-bischof,Alessio + swyx,"Wed, 29 Nov 2023 21:10:00 GMT",3114,https://substackcdn.com/feed/podcast/1084089/post/139219943/fc8866b6bb85965f8a080d586a0306fd.jpg,https://api.substack.com/feed/podcast/139219943/7f42cfa90813e2d04b79afc7436661c8.mp3,"Catch us at Modular’s ModCon next week  with Chris Lattner , and  join our community ! 2024 note: Hex is now hiring  AI Engineers . Due to  Bryan ’s very wide ranging experience in data science and AI across Blue Bottle (!), StitchFix, Weights & Biases, and now  Hex  Magic, this episode can be considered a two-parter. Notebooks = Chat++ We’ve talked a lot about AI UX (in our  meetups ,  writeups , and  guest posts ), and today we’re excited to dive into a new old player in AI interfaces: notebooks!  Depending on your background, you either  Don’t Like  or you  Like  notebooks — they are the most popular example of Knuth’s  Literate Programming  concept, basically a collection of cells; each cell can execute code, display it, and share its state with all the other cells in a notebook. They can also simply be Markdown cells to add commentary to the analysis.  Notebooks have a long history but most recently became popular from iPython evolving into  Project Jupyter , and a wave of notebook based startups from  Observable  to  DeepNote  and  Databricks  sprung up for the modern data stack. The first wave of AI applications has been very chat focused (ChatGPT, Character.ai, Perplexity, etc). Chat as a user interface has a few shortcomings, the major one being the inability to edit previous messages. We enjoyed Bryan’s takes on why notebooks feel like “Chat++” and how they are building Hex Magic: *  Atomic actions vs Stream of consciousness : in a chat interface, you make corrections by adding more messages to a conversation (i.e. “Can you try again by doing X instead?” or “I actually meant XYZ”). The context can easily get messy and confusing for models (and humans!) to follow. Notebooks’ cell structure on the other hand allows users to go back to any previous cells and make edits without having to add new ones at the bottom.  *  “Airlocks” for repeatability:  one of the ideas they came up with at Hex is “airlocks”, a collection of cells that depend on each other and keep each other in sync. If you have a task like “Create a summary of my customers’ recent purchases”, there are many sub-tasks to be done (look up the data, sum the amounts, write the text, etc). Each sub-task will be in its own cell, and the airlock will keep them all in sync together. *  Technical + Non-Technical users:  previously you had to use Python / R / Julia to write notebooks code, but with models like GPT-4, natural language is usually enough. Hex is also working on lowering the barrier of entry for non-technical users into notebooks, similar to how Code Interpreter is doing the same in ChatGPT.  Obviously notebooks aren’t new for developers ( OpenAI Cookbooks  are a good example), but haven’t had much adoption in less technical spheres. Some of the shortcomings of chat UIs + LLMs lowering the barrier of entry to creating code cells might make them a much more popular UX going forward. RAG = RecSys! We also talked about the LLMOps landscape and why it’s an “iron mine” rather than a “gold rush”:  I'll shamelessly steal [this] from a friend, Adam Azzam from Prefect. He says that  [LLMOps] is more of like an iron mine than a gold mine  in the sense of there is a lot of work to extract this precious, precious resource. Don't expect to just go down to the stream and do a little panning. There's a lot of work to be done. And frankly, the steps to go from this resource to something valuable is significant. Some of my favorite takeaways: *  RAG as RecSys for LLMs : at its core, the goal of a RAG pipeline is finding the most relevant documents based on a task. This isn’t very different from traditional recommendation system products that surface things for users. How can we apply old lessons to this new problem? Bryan cites fellow AIE Summit speaker and Latent Space Paper Club host  Eugene Yan  in  decomposing the retrieval problem into retrieval, filtering, and scoring/ranking/ordering : As AI Engineers increasingly find that  long context has tradeoffs , they will also have to relearn age old lessons that vector search is  NOT  all you need and a good  systems not models  approach is essential to scalable/debuggable RAG. Good thing Bryan has just written t he first O’Reilly book about modern RecSys , eh? *  Narrowing down evaluation:  while “hallucination” is a easy term to throw around, the reality is more nuanced. A lot of times, model errors can be automatically fixed: is this JSON valid? If not, why? Is it just missing a closing brace? These smaller issues can be checked and fixed before returning the response to the user, which is easier than fixing the model. *  Fine-tuning isn’t all you need:  when they first started building Magic, one of the discussions was around fine-tuning a model. In our  episode with Jeremy Howard  we talked about how fine-tuning leads to loss of capabilities as well. In notebooks, you are often dealing with domain-specific data (i.e. purchases, orders, wardrobe composition, household items, etc); the fact that the model understands that “items” are probably part of an “order” is really helpful. They have found that GPT-4 + 3.5-turbo were everything they needed to ship a great product rather than having to fine-tune on notebooks specifically. Definitely recommend listening to this one if you are interested in getting a better understanding of how to think about AI, data, and how we can use traditional machine learning lessons in large language models.  The AI Pivot For more Bryan, don’t miss his fireside chat at the AI Engineer Summit: Show Notes *  Hex Magic *  Bryan’s new book: Building Recommendation Systems in Python and JAX *  Bryan’s whitepaper about MLOps *  “Kitbashing in ML” , slides from his talk on building on top of foundation models * “ Bayesian Statistics The Fun Way ” by  Will Kurt *  Bryan’s Twitter *  “Berkeley man determined to walk every street in his city” * People: *  Adam Azzam *  Graham Neubig *  Eugene Yan *  Even Oldridge","* [00:00:00] Bryan’s background * [00:02:34] Overview of Hex and the Magic product * [00:05:57] How Magic handles the complex notebook format to integrate cleanly with Hex * [00:08:37] Discussion of whether to build vs buy models - why Hex uses GPT-4 vs fine-tuning * [00:13:06] UX design for Magic with Hex's notebook format (aka “Chat++”) * [00:18:37] Expanding notebooks to less technical users * [00:23:46] The ""Memex"" as an exciting underexplored area - personal knowledge graph and memory augmentation * [00:27:02] What makes for good LLMops vs MLOps * [00:34:53] Building rigorous evaluators for Magic and best practices * [00:36:52] Different types of metrics for LLM evaluation beyond just end task accuracy * [00:39:19] Evaluation strategy when you don't own the core model that's being evaluated * [00:41:49] All the places you can make improvements outside of retraining the core LLM * [00:45:00] Lightning Round","In this episode of the Latent Space Podcast, host Alessio, Partner and CTO-in-Residence of Decibel Partners, has a conversation with Bryan Bischof, an accomplished individual with a rich background in software engineering, data science, and AI. Bryan shares insights from his experience leading data efforts at Blue Bottle Coffee, where he contributed to demand forecasting and analytics and navigated the process of being acquired by Nestle. At Stitch Fix, he explored the capabilities of generative models for creative tasks like writing product descriptions. Bryan currently heads AI at Hex, focusing on enhancing the data science platform, Hex, with AI capabilities through a product called Magic.

Magic aims to integrate AI into Hex's notebook environment seamlessly, making data analysis workflows more efficient and accessible. Bryan emphasizes the importance of AI usability, mentioning how even those early in the development of AI integrations saw significant productivity boosts. He discusses the evolution of AI models and their application in tools like Hex, touching on the nuanced approach to using general models versus fine-tuning for specific tasks. Bryan believes that the broad applicability of these models makes them invaluable for creating versatile and powerful data analysis platforms.

The conversation also covers the importance of evaluating AI models and operationalizing AI within products, highlighting the blend of engineering and machine learning skills required to build effective AI solutions. Bryan's vision for AI's future includes making powerful AI tools more accessible to a wider range of users, suggesting a move towards AI-enhanced applications that empower users rather than replace them.

Alessio and Bryan also delve into the potential of AI to revolutionize self-service data analysis and the responsibilities that come with it, including governance and ensuring data accuracy. They discuss the current AI gold rush, comparing it to the era of building infrastructure for big data analytics and caution against prioritizing tools over actual application and impact.

Lastly, Bryan shares his personal interests outside of work, revealing his passion for walking and exploring every road and trail in his vicinity, illustrating his curiosity and appreciation for the world around him.

Overall, the podcast effectively conveys Bryan Bischof's wealth of experience in AI, data science, and software engineering, his contributions to the field, and his forward-thinking perspective on the role of AI in enhancing data analysis and accessibility for a broader audience.","1. Integration of AI to enhance existing platforms with capabilities such as co-pilot-esque functions and dynamic AI interactions.
2. Evolution of AI from basic coding assistance in SQL and Python to advanced tasks like function calling and agent workflows.
3. The importance of fine-tuning AI models for general usage across diverse data analysis scenarios.
4. The role of Recommender Systems (RecSys) in effectively augmenting Language Learning Models (LLMs) with Retrieval-Augmented Generation (RAG) for task-specific enhancements.
5. The potential of AI to create a personal Memex (memory extender) for enhancing human memory and information retrieval through natural language interfaces.
6. The shift towards AI in the Loop as a more user-centeric approach in utilizing AI for problem-solving, highlighting the user as the main agent with AI as a tool.
7. The importance of rigorous evaluation (evals) in AI model deployment to ensure the reliability and accuracy of AI-generated outputs.
8. The emergence and potential of integrating AI capabilities into hardware, suggesting a closer integration of AI technologies with everyday physical devices.
9. The necessity for broad experimentation and adoption of AI tools across various user demographics, emphasizing the paradigm shift induced by advanced AI in various aspects of life and work.","- **[00:45:45] - ""I have walked every road in Berkeley. And I mean like every part of every road even, not just like the binary question of, have you been on this road? I have this little app that I use called Wanderer, which just lets me like kind of keep track of everywhere I’ve been.""**

- **[00:48:25] - ""Well, I did mention that, like, I think this like Memex product is just like incredibly exciting to me. And I think it’s really opportunistic. I think it’s very, very feasible, but I would maybe even extend that a little bit, which is I don’t see enough people getting really enthusiastic about hardware with advanced AI built in.""**

- **[00:51:25] - ""Even though earlier I was talking about sort of like, maybe like not reinventing things and being respectful of the sort of like ML and data science, like ideas. I do want to say that I think everybody should be experimenting with these tools as much as they possibly can...I think however much you’re integrating with these tools or interacting with these tools, and this audience is probably going to be pretty high on that distribution. I would really encourage you to sort of like push this into the other people in your life.""**","1. Models
2. AI Engineer Career
3. Coding Tools
4. Finetuning
5. Hardware
6. Open Source
7. Startups
8. RHLF 201
9. Multimodal
10. Agents",Models,AI Engineer Career,Coding Tools,Finetuning,Hardware,Open Source,Startups,RHLF 201,Multimodal,Agents,-0.1,0.2224744871,0.2
The State of Silicon and the GPU Poors - with Dylan Patel of SemiAnalysis,https://www.latent.space/p/semianalysis,Alessio + swyx,"Fri, 17 Nov 2023 19:32:06 GMT",3181,https://substackcdn.com/feed/podcast/1084089/post/138846476/78c83068487f3d4309ad1593da8957b2.jpg,https://api.substack.com/feed/podcast/138846476/6dbe9fc182e416c28fdc70bf9291829c.mp3,"This episode came together at ~4 hrs notice since Dylan had just landed in SF and we had to setup quickly; you might notice some small audio issues in some segments, we apologize. We’re currently building our own podcast studio for 2024! 🙏  We’re ramping up our presence on  Twitter  and  YouTube  if you’d like to support us. Note:  17k people joined our emergency pod  on Sam Altman’s ouster today. If Charles Dickens was alive in 2024, A Tale of Two Cities might be the divide between the “GPU poor” and the “GPU rich”. We mentioned these terms in some of our  previous   episodes ; they were originally coined by   of  in his  “Gemini Eats the World”  post, put  on blast by Sam Altman . SemiAnalysis are one of the most in depth research and consulting firms in the semis world, and have a unique insight into the design, production, and supply chain of GPUs based on their ground presence in Asia.  In this episode we break down  the State of Silicon : when are more GPUs coming? Are there real GPU alternatives on the way? Should Microsoft buy AMD chips just to scare Jensen? Is there a “GPU poor is beautiful” manifesto? The supply wave is coming The GPU shortage is the talk of the town in the Bay Area, but next year looks a lot better in terms of AI accelerating capacity:  *  NVIDIA  is forecasted to sell over 3 million GPUs next year, about  3x their 2023 sales of about 1 million H100s. *  AMD  is forecasting  $2B of sales  for their new MI300X datacenter GPU. They are also indirectly getting a boost from the work that companies like  Modular  and  tiny  are doing in making it easier to actually use these chips (will ROCm ever catch up?) *  Google ’s  TPUv5  supply is going to increase rapidly going into 2024 *  Microsoft  just announced  Maia 100, a new AI accelerator built “with feedback” from OpenAI.  In the episode we dove deeper into what this means for each of these companies and the GPU consumers, but the TLDR (sadly) is that capacity increases but FLOPS requirements to train the next generation of models will eclipse the one of previous ones.  GPT-3 was 4,000x more FLOPS than GPT-2. Dylan estimates  GPT-4 was trained on 20,000 A100s for ~$500M all-in ; how much will OpenAI spend to train GPT-5? How many GPUs will need to go brrr? In the meantime, the amount of companies looking for GPUs has increased, with Meta rising as one of the de-facto top 3 AI labs in terms of capacity. The pressure to acquire more chips will not ease in 2024.   We also talked about some of the companies trying to displace traditional GPU architectures:  MatX ,  Lemurian Labs ,  Cerebras , etc. The different variables they are fighting on are size of SRAM vs HBM, focusing on memory bandwidth vs memory size, different math representation for kernels, etc, and how the key to this market is whether or not the transformer architecture will still be the #1 in the future. Surviving in the GPU Poor lane A lot of the smaller companies (when compared to $1T+ giants, it’s all relative) are trying hard to fight against the GPU rich, but they can’t quite offer the same scale:  *  HuggingFace  is trying to launch a  training cluster as a service , but it seems to just be a  software wrapper around NVIDIA’s GDX Cloud , as they don’t actually own that much GPU supply. The max option for GPUs to use is 1,000 in their form. *  Databricks ’  “GPU-enabled clusters”  run on AWS, and the largest one listed there is only powered by 8 NVIDIA A10Gs. The Mosaic team is also doing research on running on AMD cards with  some promising results , but they seem to be pushing up to just 128 cards, which isn’t much. *  Together  actually has 4,424 H100s live in production, which is quite sizable but still nothing compared to the 100,000 that Meta is putting online.   Take LLaMA2 as an example; the 70B model was trained on 2T tokens. Using the highest accelerator count on HuggingFace it’d take ~43 days to train the model from scratch and it’d cost ~$2M. That doesn’t include all the data and prep work. In the meantime, Zuck is probably burning tens of thousands of H100s to train LLaMA3, which will surely have much higher performance than whatever a GPU poor company can train in the same time span.   The good news, is that there’s a ton of opportunity for the GPU poors to shine, especially around fine-tuning. Most of the open source models coming out are one-size-fits-all, and there’s a ton of opportunity for startups to take them and tailor them to their customers, or to specific tasks or use cases to build vertical applications. The other area of improvement is data quality; Mistral showed how you can build a high quality small model with less FLOPs by feeding it better data.  The key to differentiation won’t be GPUs, but tokens.   Show Notes *  SemiAnalysis *  Google Gemini Eats The World – Gemini Smashes GPT-4 By 5X, The GPU-Poors *  How Nvidia’s CUDA Monopoly In Machine Learning Is Breaking - OpenAI Triton And PyTorch 2.0 *  AMD MI300 – Taming The Hype – AI Performance, Volume Ramp, Customers, Cost, IO, Networking, Software *  @sama:  incredible google got that semianalysis guy to publish their internal marketing/recruiting chart lol *  Mellanox *  MatX *  Lemurian Labs *  Cerebras * For SRAM / HBM, see our  FlashAttention episode * Suggested readings: *  Moore's Law: The Life of Gordon Moore, Silicon Valley's Quiet Revolutionary *  Chip War by Chris Miller Chapters * Introduction [00:00:00] * Importance of infrastructure for tech companies [00:01:11] * Training costs are irrelevant [00:03:06] * Worldview of GPU-poor vs GPU-rich [00:04:01] * Google's TPU infrastructure [00:08:12] * Alternative hardware like Cerebras and Graphcore [00:17:37] * Partnerships between labs and hardware companies [00:37:15] * Apple's potential in AI [00:40:56] * Concerns over China and Taiwan [00:41:02] * Feasibility of rebuilding the semiconductor supply chain in the US [00:43:22] * Foundational semiconductor readings [00:46:09] * NVIDIA's pivot to AI [00:47:40] * Dylan's writing process [00:48:17] * Using multiple data centers for distributed AI training [00:52:36] Transcript Alessio : Hey, everyone. Welcome to the Latent Space Podcast. This is Alessio, partner and CTO of Residence at  Decibel Partners . I'm joined by my co-host Swyx, founder of  Smol AI . [00:00:16] Swyx : And today we have Dylan Patel and welcome. So you are the author of the extremely popular Semi-Analysis blog. We have both had a little bit of claim to fame in breaking details of GPT-4. George Hotz came on our pod and talked about the mixture of experts thing and then you had a lot more detail. [00:00:29] Dylan : To be clear, I talked about mixture of experts in January, it's just people didn't really notice it. Yeah. I guess. [00:00:35] Swyx : I don't know. You went into a lot more detail and I'd love to dig into some of that. [00:00:38] Dylan : Yeah. Thank you so much. I've been doing consulting in the industry, semiconductor industry since 17. 2021 got bored and in November I started writing a blog and then like 2022 I was good and started hiring folks for my firm. And then all of a sudden 2023 happens and it's like the perfect intersection. I used to do data science, but not like AI, not really like multivariable progression is not AI. Right. But also I've been involved in the semiconductor industry for a long, long time, posting about it online since I was 12. Right. You know, all of a sudden this all kind of came to fruition. So it's cool to have the blog sort of blow up in that way. [00:01:11] Swyx : I used to cover semis at Belyasny as well. And it was for a long time, it was just the mobile cycle. And then a little bit of PCs, but like not that much. And then maybe some cloud stuff, you know, like public cloud, you know, semiconductor stuff. But it really wasn't anything until this wave. And I was actually listening to you on one of the previous podcasts that you've done. And it was surprising that high-performance computing also kind of didn't really take off. Like AI is just the first form of high-performance computing that worked. [00:01:37] Dylan : One of the theses I've had for a long time that I think people haven't really caught on, but it's coming to fruition now is that the largest tech companies in the world, their software is important, but actually having and operating a very efficient infrastructure is incredibly important. And so, you know, people talk about, you know, hey, Amazon is great, AWS is great because yes, it is easy to use and they've built all these things. But behind the scenes, they've done a lot on the infrastructure that is super custom that Microsoft, Azure and Google Cloud just don't even match in terms of efficiency. If you think about the cost to rent out SSD space, so the cost to rent, you know, offer database service on top of that, obviously, a cost to rent out a certain level of CPU performance. Amazon has a massive advantage there. And likewise, like Google spent all this time doing that in AI, right, with their TPUs and infrastructure there and an optical switches and all this sort of stuff. And so in the past, it wasn't immediately obvious. I think with AI, especially like how scaling laws are going, it's like incredibly important for infrastructure is like so much more important. And then like when you just think about software cost, right, like the cost structure of it, there was always a bigger component of R&D and like SAS businesses, you know, all over SF, all these SAS businesses did crazy good because, you know, they just start as they grow and then all of a sudden they're so freaking profitable for each incremental new customer. And AI software looks like it's going to be very different, in my opinion, right? Like the R&D cost is much lower in terms of people, but the cost of goods sold in terms of actually operating the service, I think will be much higher. And so in that same sense, infrastructure matters a ton. [00:03:02] Swyx : And I think you wrote once that training costs effectively don't matter. [00:03:06] Dylan : Yeah. In my opinion, I think that's a little bit spicy, but yeah, it's like training costs are irrelevant, right? Like GPT-4, right, like 20,000 A100s, that's, that's like, I know it sounds like a lot of money. The supercomputer, it's, it's, oh, it's slightly more, but yeah, I think the 500 million is a fair enough number. I mean, if you think about just the pre-training, right, three months, 20,000 A100s at, you know, a dollar an hour is like, that is way less than 500 million, right? But of course there's data and all this sort of stuff. [00:03:33] Alessio : So people that are watching this on YouTube, they can see a GPU-poor and a GPU-rich hat on the table, which is inspired by your, yeah, your Google Gemini, it's the world blog post. So Sam, did you know that this thing was going to blow up so much? Sam Altman even tweeted about it, he said, incredible Google got the semi-analysis guide to publish their internal marketing recruiting chart. And yeah, tell people who are the GPU-poors, who are the GPU-rich, like what's this framework that they should think about? [00:04:01] Dylan : So it's, it's, you know, some of this work we've been doing for a while is just on infrastructure and like, hey, like when something happens, I think it's like a sort of competitive advantage of our firm, right, myself and my colleagues is we go from software all the way through to like low-level manufacturing, and it's like, who, you know, oh, Google's actually ramping up TPU production massively, right? And like, I think people in AI would be like, well, duh, but like, okay, like who, who has the capability of figuring out the number? Well, one, you could just get Google to tell you, but they don't, they won't tell you, right? That's like a very closely guarded secret. And most people that work at Google DeepMind don't even know that number, right? Two, you go through the supply chain and see what they've placed in order. Three is sort of like, well, who's actually winning from this? Hey, oh, Celestica's building these boxes. Wow. Oh, interesting. Oh, Google's involved in testing for them. Oh, okay. Oh, this company's providing design IP to them. Okay. That's very valuable in a monetary sense, but you know, you have to understand the whole technology stack. But on the flip side, right, is, well, why is Google building all these? What could they do with it? And what does that mean for the world? Especially in SF, right? Like, I'm sure you folks have been to parties. If we just brag about how many TPUs they have, like, it's happened to me multiple times where someone's just like, I'm just witnessing a conversation where somebody from Meta is bragging about how many TPUs they have versus someone from another firm that it's like, or like a startup person's like, dude, can you believe we just acquired, we have 512 H100s coming online in August. And it's like, oh, cool. Like, you know, going through the supply chain, it's like, dude, you realize there's 400,000 manufactured last quarter and like 530,000 this quarter being sold of H100s. And it's like, oh crap, that's a lot. That's a lot of GPUs. But then like, oh, how does that compare to Google? And like, there's one way to look at the world, which is just like, hey, scale is all you need. Like obviously data matters. Obviously all this stuff matters. I think any data set, a larger model will just do better. I think it's going to be more expensive, but it's going to do better. Okay, there's all these GPUs going into production. NVIDIA is going to sell well over 3 million total GPUs next year, over a million H100s this year alone. There's a lot of GPU capacity coming online. It's an incredible amount. And well, what are people doing? What are people working on? I think it's very important to like, just think about what are people working on, right? What actually are you building that's going to advance? What is monetizable? But what also makes sense? And so like, a lot of people were doing things that I felt counterproductive, right? In a world where in less than a year, there's going to be more than 4 million high-end GPUs out there. I mean, we can talk about the concentration of those GPUs, but if you're doing really valuable work as a good person, right, like you're contributing in some way, should you be focused on like, well, I don't have access to any of those 4 million GPUs, right? I actually only have access to gaming GPUs. Should I focus on like being able to fine tune a model on that, right? Like, no, it's not really that important. Or like, should I be focused on batch one inference on a cloud GPU? Like, no, that's like pointless. Like, why would you do batch size one inference on an H100? That's just like ridiculously dumb. There's a lot of counterproductive work. And at the same time, there's a lot of things that people should be doing. I mean, obviously, most people don't have resources, right? And I love the open source and I want the open source to win. And I hate the people who want to like, no, we're X lab and we think this is the only way you should do it. And if people don't do it this way, they should be regulated against it and all this kind of stuff. So I want the open source to win, right? Like companies like Mistral and like what Meta are doing, you know, Mosaic and all these folks together. All these people doing, you know, huge stuff with open source, you know, want them to succeed. But it's like, there's certain things that are, you know, like hyper focusing on leaderboards and hugging face. No, like truthful QA is a garbage benchmark. Some of the models that are very high on there, if you use it for five seconds, you're like, this is garbage. There was things I wanted to say. Also, you know, we're in a world where compute matters a lot. Google is going to have more compute than any other company in the world, period. By like a large, large factor. It's just like framing it into that like mindset of like, Hey, like, what are the counterproductive things? What do I think personally? Or what have people told me that are involved in this should they focus on the pace of acceleration from 2020 to 2022 is less than 2022 to 2024, you know, GP two to four, two to four is like 2020 to 2022, right? Is less than I think from GPT four in 2022, which is when it was trained, right? What open AI and Google and, and Anthropic do in 2025, right? Like I think the pace of acceleration is increasing and it's just good to like, think about, you know, that sort of stuff. [00:08:12] Alessio : That makes sense. And the chart that Sam mentioned is about, yeah, Google TPU B fives completely overtaking open AI by orders of magnitude. Let's talk about the TPU a bit. We had Tris Landner on the show, which I know, you know, he used to work on TensorFlow and Google. And he did mention that the goal of Google is like make TPUs go fast with TensorFlow, but then he also had a post about PyTorch dealing the thunder. How do you see that changing? If like, now that a lot of the compute will be TPU based and Google wants to offer some of that to the public to Google internally. [00:08:44] Dylan : And I think, you know, as obviously on JAX and XLA and all that kind of stuff externally, like they've done a really good job. Wouldn't say like TPUs through PyTorch XLA is amazing, but it's, it's not bad, right? Like some of the numbers they've shown, some of the, you know, code they've shown for TPU V5E, which is not the TPU V5 that I was referring to, which is in the sort of the post, the TPU poor post is referring to, but TPU V5E is like the new one, but it's mostly, mostly an inference chip. It's a small chip. It's, it's about half the size of a TPU V5. That chip, you know, you can get very good performance on of LLAMA 70B inference. Very, very good performance when you're using PyTorch and XLA. Now of course you're going to get better if you go JAX XLA, but I think Google is doing a really good job after the restructuring of focusing on external customers too. Probably won't focus too much on TPU V5 for everyone externally, but V5E, we're also building a million of those, right? Hey, a lot of companies are using them, right? Or will be using them because it's going to be an incredibly cheap form of compute. I think the world of frameworks and all that, right? Like that's obviously something a researcher should talk about, not myself, but you know, the stats are clear that PyTorch is way, way dominating everything. But JAX is like doing well. Like there's external users of JAX. The forever shouldn't be that the person doing PyTorch level code, right? That high should also be writing custom CUDA kernels, right? There should be, you know, different layers of abstraction where people hyper optimize and make it much easier for everyone to innovate on separate stacks, right? And then every once in a while, someone comes through and pierces through the layers of abstraction and innovates across multiple or a group of people. But I think frameworks are important. Compilers are important, right? Chris Lattner's, what he's doing is really cool. I don't know if it'll work, but it's super cool and it certainly works on CPUs. We'll see about accelerators. Likewise, there's OpenAI's Triton, like what they're trying to do there. And you know, everyone's really coalescing around Triton, third-party hardware vendors. There's Palace. I don't want to mischaracterize it, but you can write in Palace and it'll go through, you can lower level code and it'll work to TPUs and GPUs, kind of like Triton, but it's like there's a backend for Triton. I don't know exactly everything about it, but I think there's a lot of innovation happening on make things go faster, right? How do you go burr? Is every single person working in ML, it would be a travesty if they had to write like custom CUDA kernels always, right? Like that would just slow down productivity, but at the same time, you kind of have to. [00:10:53] Swyx : By the way, I like to quantify things when you say make things go burr. Is there a target range of MFU that you typically talk about? [00:10:59] Dylan : Yeah, there's sort of two metrics that I like to think about a lot, right? So in training, everyone just talks about MFU, right? But then on inference, right, which I think is one LLM inference will be bigger than training or multimodal, whatever, bubble inference will be bigger than training, probably next year, in fact, at least in terms of GPUs deployed. And the other thing is like, you know, what's the bottleneck when you're running these models? The simple, stupid way to look at it is training is, you know, there's six flops floating point operations you have to do for every byte you read in, right? Every parameter you read in. So if it's FP8, then it's a byte, if it's FP16, it's two bytes, whatever, right? On training, but on inference side, the ratio is completely different. It's two to one, right? There's two flops per parameter that you read in and parameters, maybe one byte, right? But then when you look at the GPUs, the GPUs are very, very different ratio. The H100 has 3.35 terabytes a second of memory bandwidth, and it has a thousand teraflops of FP16, BFLIP16, right? So that ratio is like, I'm sorry, I'm going to butcher the math here and people are going to think I'm dumb, but 256 to one, right, call it 256 to one if you're doing FP16. And same applies to FP8, right, because anyways, per parameter read to number of floating point operations, right? If you quantize further, but you also get double the performance on that lower quantization. That does not fit the hardware at all. So if you're just doing LLM inference at batch one, then you're always going to be under utilizing the flops. You're only paying for memory bandwidth. And the way hardware is developing, that ratio is actually only going to get worse. H200 will come out soon enough, which will help the ratio a little bit, improve memory bandwidth more than improves flops, just like the A180 gig did versus the A140 gig. But then when the B100 comes out, the flops are going to increase more than memory bandwidth. And when future generations come out and the same with AMD side, right, MI300 versus 400, as you move on generations, just due to fundamental like semiconductor scaling, DRAM memory is not scaling as fast as logic has been. And you can do a lot of interesting things on the architecture. So you're going to have this problem get worse and worse and worse. And so on training, it's very, you know, who cares, right? Because my flops are still my bottleneck most of the time. I mean, memory bandwidth is obviously a bottleneck, but like, well, you know, batch sizes are freaking crazy, right? Like people train like 2 million batch sizes, it's trivial, right? Like that's what Lama, I think did, Lama 70B was 2 million batch size. Unlike you talk to someone at one of the frontier labs and they're like, just 2 million, 2 million token batch size, right? That's crazy, or sequence, sorry. But when you go to inference side, well, it's impossible to do one, to do 2 million batch size. Also your latency would be horrendous if you tried to do something that crazy. So you kind of have this like differing problem where on training, everyone just kept talking MFU, model flop utilization, right? How many flops, six times the number of parameters, basically, more or less. And then what's the quoted number, right? So if I have 312 teraflops out of my A100 and I was able to achieve 200, that's really good, right? You know, some people are achieving higher, right? Some people are achieving lower. That's a very important like metric to think about. Now you have like people thinking MFU is like a security risk, but on inference, MFU is not nearly as important, right? It's memory bandwidth utilization. You know, batch one is, you know, what memory bandwidth can I achieve, right? Because as I increase batch from batch size one to four to eight to even 256, right, it's sort of where the crossover happens on an H100 inference wise, where it's flops limiting you more and more. But like you should have very high memory bandwidth utilization. So when people talk about A100s, like 60% MFU is decent. On H100s, it's more like 40, 45% because the flops increased more than the memory bandwidth. But people over time will probably get above 50% on H100, on MFU, on training. But on inference, it's not being talked about much, but MBU, model bandwidth utilization is the important factor, right? Above my 3.35 terabytes a second memory bandwidth on my H100, can I get two? Can I get three? Right? That's the important thing. And right now, if you look at everyone's inference stuff, I dogged on this in the GPU poor thing, right? But it's like hugging faces libraries are actually very inefficient, like incredibly inefficient for inference. You get like 15% MBU on some configurations, like eight A100s and LLAMA 70B, you get like 15%, which is just like horrendous. Because at the end of the day, your latency is derived from what memory bandwidth you can effectively get, right? So if you're doing LLAMA 70 billion, 70 billion parameters, if you're doing it int8, okay, that's 70 gigabytes a second, gigabytes you need to read for every single inference, every single forward pass, plus the attention, but again, we're simplifying it. 70 gigabytes you need to read for every forward pass, what is an acceptable latency for a user to have? I would argue 30 milliseconds per token. Some people would argue lower, right? But at the very least, you need to achieve human reading level speeds and probably a little bit faster, because we like their skin, to have a usable model for chatbot style applications. Now there's other applications, of course, but chatbot style applications, you want it to be human reading speed. So 30 tokens per second, 30 tokens per second is 33, or 30 tokens, milliseconds per token is 33 tokens per second, times 70 is, let's say three times seven is 21, and then add two zeros to 2,100 gigabytes a second, right? To achieve human reading speed on LLAMA 70B, right? So one, you can never achieve LLAMA 70B human reading speed on, even if you had enough memory capacity on a model, on an A100, right? Even an H100 to achieve human reading speed, right? Of course, you couldn't fit it because it's 80 gigabytes versus 70 billion parameters, so you're kind of butting up against the limits already, 70 billion parameters being 70 gigabytes at int8 or fp8. You end up with one, how do I achieve human reading level speeds, right? So if I go with two H100s, then now I have, you know, call it six terabytes a second of memory bandwidth, if I achieve just 30 milliseconds per token, then I'm, you know, which is 33 tokens per second, which is 30, you know, is three terabytes a second, was it three, three times, 21, 2.1 terabytes a second of memory bandwidth, then I'm only at like 30% bandwidth utilization. So I'm not using all my flops on batch one anyways, right? Because 70, you know, the flops that you're using there is tremendously low relative to inference, and I'm not actually using a ton of the tokens on inference. So with two H100s, I only get 30 milliseconds a token, that's a really bad result. You should be striving to get, you know, so upwards of 60%, and that's like 60% is kind of low too, right? Like, I've heard people getting 70, 80% model bandwidth utilization. And then, you know, obviously you can increase your batch size from there and your model bandwidth utilization will start to fall as your flops utilization increases, but, you know, you have to pick the sweet spot for where you want to hit on the latency curve for your user. Obviously, as you increase batch size, you get more throughput per GPU, so that's more cost effective. There's a lot of like things to think about there, but I think those are sort of the two main things that people want to think about, and there's obviously a ton with regards to like networking and inner GPU connection, because most of the useful models don't run on a single GPU. They can't run on a single GPU. [00:17:37] Swyx : Is there a TPU equivalent of Mellanox? [00:17:39] Dylan : The Google TPU is like super interesting because Google has been working with Broadcom, who's the number one networking company in the world, right? So Mellanox was nowhere close to number one. They had a niche that they were very good at, which was the network card, the card that you actually put in the server, but they didn't do much. They didn't have, they weren't doing successfully in the switches, right? Which is, you know, you connect all the networks cards to switches, and then the switches to all the servers. So Mellanox was not that great. I mean, it was good. They were doing good, and NVIDIA bought them, you know, in 19, I believe, or 18, but Broadcom has been number one in networking for a decade plus, and Google partnered with them on making the TPU, right? So Google does a lot of the design, especially on the ML hardware side, on how you pass stuff around internally on the chip, but Broadcom does a lot on the network side, right? They specifically, how to get really high connection speed between two chips, right? They've done a ton there, and obviously Google works a ton there too, but this is sort of Google's like less discussed partnership that's truly critical for them, and Google's tried to get away from them many times. Their latest target to get away from Broadcom is 2027, right? But like, you know, that's four years from now. Chip design cycle's four years, so they already tried to get away in 2025, and that failed. They have this equivalent of very high speed networking. It works very differently than the way GPU networking does, and that's important for people who code on a lower level. [00:18:52] Swyx : I've seen this described as the ultimate rate limit on how big models can go. It's not flops, it's not memory, it's networking. Like it has the lowest scaling laws, the lowest Moore's laws, and I don't know what to do about that because no one else has any solutions. [00:19:06] Dylan : Yeah, yeah, so I think what you're referring to is that like network speed is increased Much slower than the other two. Than flops, yeah, and bandwidth, yeah, yeah. And yeah, that's a tremendous problem in the industry, right? That's why NVIDIA bought a networking company, that's why Broadcom is working on Google's chip right now, but of course on Meta's internal AI chip, which they're on the second generation of, working on that, and what's the main thing that Meta's doing interesting is networking stuff, right? Multiplying tensors is kind of, there's a lot of people who've made good matrix multiply units, right? But it's about like getting good utilization out of those and interfacing with the memory and interfacing with other chips really efficiently makes designing these chips very hard. Most of the startups obviously have not done that really well. [00:19:46] Alessio : I think the startup's point is the most interesting, right? You mentioned companies that are GPU poor, they raise a lot of money, and there's a lot of startups out there that are GPU poor and did not raise a lot of money. What should they do? How do you see like the space dividing? Are we just supposed to wait for like the big labs to do a lot of this work with a lot of the GPUs? What's like the GPU poor's beautiful version of the article? [00:20:12] Dylan : Open AI, who everyone would be like, oh yeah, they have more GPUs than anyone else, right? But they have a lot less flops than Google, right? That was the point of the thing, but not just them, it's like, okay, it's like a relative totem pole, right? And of course, Google doesn't use GPUs as much for training and inference, they do use some, but mostly TPUs. So kind of like, the whole point is that everyone is GPU poor because we're going to continue to scale faster and faster and faster and faster, and compute will always be a bottleneck, just like data will always be a bottleneck. You can have the best data set in the world and you can always have a better one. And same with, you have the biggest compute system in the world, but you'll always want a better one. And so it's like, there's things that like Mistral, they trained a fricking awesome model on relatively fewer GPUs, right? And now they're scaling up higher and higher and higher, right? There's a lot that the GPU poor can do though, right? We all have phones, we all have laptops, right? There is a world for running GPUs or models on device. The replet folks are trying to do stuff like that. Their models, they can't follow scaling laws, right? Why? Because there's a fundamental limit to how much memory bandwidth and capacity you can get on a laptop or a phone. You know, I mentioned the ratio of flops to bandwidth on a GPU is actually really good compared to like a MacBook or like a phone. To run Llama 70 billion requires two terabytes a second of memory bandwidth, 2.1 at human reading speed. Yeah, but my phone has like 50 gigabytes a second. Your laptop, even if you have an M1 Ultra has what, like, I don't remember, like a couple hundred gigabytes a second of memory bandwidth. You can't run Llama 70B just by doing the classical thing. So there's like, there's stuff like speculative decoding, you know, together did something really cool. And they put it in the open source, of course, Medusa, right? Like things like that, that are, you know, they work on batch size one, they don't work on batch size, you know, high. And so there's like the world of like cloud inference. And so in the cloud, it's all about what memory bandwidth and MFU I can achieve. Whereas on the edge, I don't think Google is going to deploy a model that I can run on my laptop to help me with code or help me with, you know, X, Y, Z, they're always going to want to run it in a cloud for control. Or maybe they let it run on the device, but it's like only their pixel phone, you know, it's kind of like a walled garden thing. There's obviously a lot of reasons to do other things for security, for openness, to not be at the whims of a trillion dollar plus company who wants my data, right? You know, there's a lot of stuff to be done there. And I think folks like Repl.it, they open source their model, right? Things like together, I just mentioned, right, that developing Medusa, that didn't take much GPU at all, right? That's very, well, they do have quite a few GPUs, they made a big announcement about having 4,000 H100s, that's still relatively poor, right, when we're talking about hundreds of thousands of like the big labs, like OpenAI, and so on and so forth, or millions of TPUs like Google, but still, they were able to develop Medusa with probably just one server, one server with eight GPUs in it. And its usefulness of something like Medusa, something like speculative decoding is, is on device, right? And that's what like a lot of people can focus on, you know, people can focus on all sorts of things like that. I don't know, right? Like a new model architecture, right? Like, are we only going to use transformers? I'm pretty told to think like transformers are it, right? My hardware brain can only know something that loves hardware, right? People should continue to try and innovate on that, right? Like, you know, asynchronous training, right? Like that kind of stuff is like, super, super interesting. I think it's Tim Demeters. He had like the- Demeers? [00:23:09] Swyx : The same guy as Kylo Ren. [00:23:10] Dylan : Yes, he had the swarm paper and petal. That research is super cool. The universities will never have much compute, but like, hey, to prepare to do things to, you know, all these sorts of stuff, like they should try to build, you know, super large models. Like, you look at what Tsinghua University is doing in China, actually, they open sourced their model to I think the largest like by parameter count, at least open source models. I mean, of course, they didn't train it on much data, but it's like, you know, it's like you could do some cool stuff like that. I don't know. I think there's a lot that people can focus on. One, scaling out a service to many, many users. Distribution is very important. So figuring out distribution, right? Like figuring out useful fine tunes, right? Like doing LLMs that OpenAI will never make, sorry for the crassness, a porn DALL-E 3, right? Open source is doing crazy stuff with stable diffusion, right? Right? Like, I don't know. Yeah, but it's like, it's like, and there's a legitimate market. I think there's a couple of companies who make tens of millions of dollars of revenue from LLMs or diffusion models for porn, right? Or, or, you know, that kind of stuff. Like, I mean, there's a lot of stuff that people can work on that will be successful businesses or doesn't even have to be a business, but can advance humanity tremendously. That doesn't require crazy scale. [00:24:10] Alessio : How do you think about the depreciation of like the hardware versus the models? If you buy a H100, sure, the next year's is going to be better, but like at least the hardware is good. If you're spending a lot of money on like training a smaller model, it might be like super obsolete in like three months. And you've got now all this compute coming online. I'm just curious if like companies should actually spend the time to like, you know, fine tune them and like work on them where like the next generation is going to be out of the box so much better. [00:24:37] Dylan : Unless you're fine tuning for on-device use, I think fine tuning current existing models, especially the smaller ones is a useless waste of time because the cost of inference is actually much cheaper than you think once you achieve good MBU and you batch at a decent size, which any successful business in the cloud is going to achieve, you know, and then two, fine tuning like people like, oh, you know, this 7 billion parameter model, if you fine tune it on a data set is almost as good as 3.5, right? Why don't you fine tune 3.5 and look at your performance, right? And like, there's nothing open source that is anywhere close to 3.5 yet. There will be. People also don't quite grasp. Falcon was supposed to be, Falcon 140B. It's less parameters than 3.5. And also, I don't know about the exact token count, but I believe it. Do we know the parameters of 3.5? It's not 175 billion. People keep saying this. [00:25:25] Swyx : No. Because we know 3, but we don't know 3.5. [00:25:27] Dylan : 3.5. [00:25:28] Swyx : It's definitely smaller. [00:25:29] Dylan : No, it's bigger than 175. I think it's sparse. MOE. I'm pretty sure. And yeah, you can, you can do some like gating around the size of it by looking at their inference latency. Well, what's the theoretical bandwidth if they're running it on this hardware and doing tensor parallel in this way? So they have this much memory bandwidth and maybe they get, maybe they're awesome and they get 90% memory bandwidth utilization. I don't know. That's an upper bound and you can see the latency that 3.5 gives you, especially at like off peak hours or if you do fine tuning and you have your, if you have a private enclave, they'll like my Azure will quote you latency. So you can, you can figure out how many parameters per forward pass, which I think is somewhere in the like 50 to 40 billion range, but I could be very wrong. That's just like my guess based on that sort of stuff. You know, 50 ish. And actually I think open source will have models of that quality. I mean, I assume Mosaic or like Meadow will open source and Mistral will be able to open source models of that quality. And furthermore, right? Like if you just look at the amount of compute, obviously data is very important and the ability, all these tricks and dials that you turn to be able to get good MFU and good MBO, right? Like depending on inference or training is, there's a ton of tricks. But at the end of the day, there's like 10 companies that have enough compute in one single data center to be able to beat GPT-4, right? Like straight up, like if not today, within the next six months, right? 4,000 H100s is, I think you need about 7,000 maybe. And with some algorithmic improvements that have happened since GPT-4 and some data quality improvements probably, like you could probably get to even like less than 7,000 H100s running for three months to beat GPT-4. Of course, that's going to take a really awesome team, but there's quite a few companies that are going to have that many, right? Open source will match GPT-4, but then it's like, what about GPT-4 Vision? Or what about, you know, 5 and 6 and all these kinds of stuff and like interact tool use and Dolly and like, that's the other thing is like, there's a lot of stuff on tool use that the open source could also do, that the GPT-4 could do. I think there are some folks that are doing that kind of stuff, agents and all that kind of stuff. I don't know. That's way over my head, the agent stuff. [00:27:24] Swyx : Yeah, it's over everyone's head. One more question on this sort of Gemini GPU rich essay. We've had a very wide ranging conversation already, so it's hard to categorize, but I tried to look for the Meena Eats the World document. Oh, it's not public. [00:27:36] Dylan : No, no, no, no, no, no. You've read it. Yeah, I read it. So Noam Shazir is like, I don't know, I think he's like- The GOAT. The GOAT. Yeah, I think he's the GOAT. [00:27:46] Swyx : In one year, he published like switch transformers, like some attention is all you need, obviously, but he also did the speculative decoding stuff. [00:27:53] Dylan : Yeah, exactly. It's like, it's like all this stuff that we were talking about today was like, you know, and obviously there's other people that are awesome that were, you know, helping and all that sort of stuff. Meena Eats the World was basically, he wrote an internal document around the time where Google had Meena, right? But it was like, he wrote it and he was like, basically predicting everything that's happening now, which is that like large language models are going to eat the world, right? In terms of, you know, compute and he's like the total amount of deployed flops within Google data centers will be dominated by large language models. Back then, a lot of people thought he was like silly for that, right? Like internally at Google. But you know, now if you look at it, it's like, oh wait, millions of TPUs. You're right. You're right. You're right. Okay. We're totally getting dominated by like both, you know, Gemini training and inference, right? Like, you know, total flops being dominated by LLMs is completely right. [00:28:36] Swyx : So my question was, he had a bunch of predictions in there. Do you think there were any like underrated predictions that may not have yet have come true? Was he wrong on anything? [00:28:44] Dylan : Meena sucked, right? If you'd look at the total flops, right? You know, parameters times tokens times six, it's like tiny, tiny fraction of GPT-2, which came out just a few months later, which was like, okay, so he was right about everything, but like, maybe he knew about GPT-2. I have no clue. OpenAI clearly was like way ahead of Google on LLM scaling. Even then, people didn't really recognize it back in GPT-2 days, maybe. The number of people that recognized it was maybe hundreds, tens. [00:29:10] Alessio : So we talked about transformer alternatives. The other thing is GPU alternatives. The CPU is obviously one, but there's Cerebras, there's Graphcore, there's MAD-X, Lemurian Labs, there's a lot of them. Thoughts on what's real, who's alive, who's kind of like a zombie company walking. [00:29:27] Dylan : You know, I mentioned like transformers were the architecture that won out, but I think, you know, the number of people who recognized that in 2020 was, you know, as you mentioned, probably hundreds, right? For natural language processing, maybe in 2019 at least, right? You think about a chip design cycle, it's like years, right? So it's kind of hard to bet your architecture on the type of model that develops. But what's interesting about all the first wave AI hardware startups, you know, there's a ratio of memory, capacity, compute, and memory bandwidth, right? Everyone kind of made the same bet, which is, I have a lot of memory on my chip, which is A, really dumb, because the models have grew way past that, right? Even Cerebras, right? You know, like I'm talking about like Graphcore, it's called SRAM, which is the memory on chip, much lower density, but much higher speeds versus, you know, DRAM, memory off chip. And so everyone was betting on more memory on chip and less memory off chip, right? And to be clear, right, for image networks and models that are small enough to just fit on your chip, that works. That is a superior architecture, but scale, right, scale, scale, scale, scale. NVIDIA was the only company that bet on the other side of more memory bandwidth and more memory capacity external, also the right ratio of memory bandwidth versus capacity. A lot of people like Graphcore specifically, right, that ton of memory on chip, and then they had a lot more memory off chip, but that memory off chip was a much lower bandwidth. Same applies to Samanova, same applies to Cerebras. They had no memory off chip, but they thought, hey, I'm going to make a chip the size of a wafer, right? Like, you know, those guys, they're silly, right? Hundreds of megabytes, we have 40 gigabytes. There's no, you know, and then, oh, crap, models are way bigger than 40 gigabytes, right? The ones that people deploy. Everyone bet on sort of the left side of this curve, right? The interesting thing is that there's new age startups like Lumerium, like MedEx, I won't get into what they're doing, but they're making much more rational bets. I don't know, you know, it's hard to say with a startup, like, it's going to work out, right? Obviously there's tons of risk embedded, but those folks, you know, Jay Duane of Lumerium and like Mike and Rainier, they understand models, they understand how they work. And if transformers continue to reign supreme, whatever innovations those folks are doing on hardware are going to need to be fitted for that. Or you have to predict what the model architecture is going to look like in a few years, right? You know, and hit that spot correctly. So that's kind of a background on those. But like now you look today, hey, Intel bought Nirvana, which was Naveen Rao's Mosaic ML. He started Mosaic ML and sold it to Databricks recently, obviously leading LLMs and stuff there, AI there. Intel bought that company from him and then shut it down and bought this other AI company. And now that company is kind of, you know, got new chips. They're going to release a better chip than the H100 within the next quarter or so. AMD, they have a GPU, MI300, that will be better than the H100 in a quarter or so. Now that says nothing about how hard it is to program it, but at least hardware-wise on paper, it's better. Why? Because it's, you know, a year and a half later, right, than in the H100 or a year later than the H100, of course, and, you know, a little bit more time and all that sort of stuff. But they're at least making similar bets on memory bandwidth versus flops versus capacity. Following NVIDIA's lead, the questions are like, what is the correct bet for three years from now? How do you engineer that? And will those alternatives make sense? The other thing is, if you look at total manufacturing capacity, right, for this sort of bet, right, you need high bandwidth memory, you need HBM, and you need large five nanometer dies, you know, soon three nanometer, whatever, right? You need both of those components and you need the whole supply chain to go through that. We've written a lot about it, but, you know, to simplify it, NVIDIA has a little bit more than half and Google has like 30%, right, through Broadcom. So it's like the total capacity for everyone else, much lower, and they're all sharing it, right? Amazon's training and inferentia, Microsoft's in-house chip, and, you know, you go down the list and it's like Meta's in-house chip, and also AMD, and also, so all of these companies are sharing like a much smaller slice. Their chips are not as good, or if they are, even though, you know, I mentioned Intel and AMD's chips are better, that's only because they're throwing more money at the problem kind of, right? You know, NVIDIA charges crazy prices, I think everyone knows that. Their gross margins are insane. AMD and Intel and others will charge more reasonable margins, and so they're able to give you more HBM and et cetera for a similar price, and so that ends up letting them beat NVIDIA, if you will, but their manufacturing costs are twice that in some cases, right? In the case of AMD, their manufacturing costs are MI300 or more than twice that of H100, and it only beats H100 by a little bit from, you know, performance stuff I've seen. So it's like, you know, it's tough for anyone to like bet the farm on a alternative hardware supplier, right? Like, in my opinion, like, you should either just like be like, you know, a lot of like ex-Google startups are just using TPUs, right? And hey, that's Google Cloud, you know, after moving the TPU team into the cloud team, infrastructure team, sort of, they're much more aggressive on external selling, and so you companies like, even see companies like Apple using TPUs for training LLMs, as well as TPUs, but either bet heavily on TPUs, because that's where the capacity is, bet heavily on GPUs, of course, and stop worrying about it, and leverage all this amazing open source code that is optimized for NVIDIA. If you do bet on AMD or Intel or any of these startups, then you better make damn sure you're really good at low-level programming, and damn sure you also have a compelling business case, and that the hardware supplier is giving you such a good deal that it's worth it. And also, by the way, NVIDIA's releasing a new chip in, you know, they're going to announce it in March, and they're going to release it and ship it Q2, Q3 next year anyways, right? And that chip will probably be three or four times as good, right? And maybe it'll cost twice as much, or 50% more. I hear it's 3x the performance on an LLM, and 50% more expensive, is what I hear. So it's like, okay, yeah, nothing is going to compete with that, even if it is 50% more expensive, right? And then you're like, okay, well, that kicks the can down further, and then NVIDIA's moving to a yearly release cycle, so it's like very hard for anyone to catch up to NVIDIA, really, right? So, you know, investing all this in other hardware, like, if you're Microsoft, obviously, who cares if I spend $500 million a year on my internal chip? Who cares if I spend $500 million a year on AMD chips, right? Like, if it lets me knock the price of NVIDIA GPUs down a little bit, puts the fear of God within Jensen Huang, right, like, you know, then it is what it is, right? And likewise, you know, with Amazon, and so on and so forth, you know, of course, their hope is that their chips succeed, or that they can actually have an alternative that is much cheaper than NVIDIA. To throw a couple hundred million dollars at a company, you know, as product is completely reasonable. And in the case of AMD, I think it'll be more than a couple hundred million dollars, right? But yeah, I think alternative hardware is like, it really does hit like sort of a peak hype cycle, kind of end of this year, early next year, because all NVIDIA has is H100, and then H200, which is just better, more memory bandwidth, higher memory capacity, H100, right? But that doesn't beat what, you know, AMD are doing, it doesn't beat what, you know, Intel's Gaudi 3 does, but then very quickly after, NVIDIA will crush them. And then those other companies are gonna take two years to get to their next generation. You know, it's just a really tough place. And no one besides, you know, the main thing about hardware is like, hey, that bet I talked about earlier is like, you know, that's very oversimplified, right? Just memory bandwidth flops and memory capacity. There's a whole lot more bets. There's 100 different bets that you have to make and guess correctly to get good hardware, not even have better hardware than NVIDIA get close to them. And that takes understanding models really, really well. That takes understanding so many different aspects, whether it's power delivery or cooling or design, layout, all this sort of stuff. And it's like, how many companies can do everything here, right? It's like, I'd argue Google probably understands models better than NVIDIA, I don't think people would disagree. I'm an NVIDIA understands hardware better than Google. And so you end up with like, Google's hardware is competitive, but like, does Amazon understand models better than NVIDIA? I don't think so. And does Amazon better understand hardware better than NVIDIA? No. I also have the opinion that the labs are useful partners, they're convenient partners. They're not going to buddy up as close as people think, right? I don't even think like, I expect in the next few years that the OpenAI Microsoft probably falls apart too. I mean, they'll still continue to use GPUs and stuff there. But like, I think that the level of closeness you see today is probably the closest they get. [00:37:15] Swyx : At some point, they become competitive if OpenAI becomes its own cloud. [00:37:18] Dylan : The level of value that they deliver to the world, if you talk to anyone there, they truly believe it'll be tens of trillions, if not hundreds of trillions of dollars, right? In which case, obviously, you know, I know weird corporate structure aside, you know, this is the same playing field as companies like Microsoft and Google. Google wants to also deliver hundreds of trillions of dollars of value. And it's like, obviously you're competing and Microsoft wants to do the same and you're going to compete. In general, right, like these lab partnerships are going to be nice, but they're probably incentivized to, you know, hey, NVIDIA, you should, you know, can you design the hardware in this way? It doesn't work like that. It works like this. And they're like, oh, so this is the best compromise. Right? Like, I think OpenAI would be stupid not to do that with NVIDIA, but also with AMD. But also, hey, like how much time, and Microsoft's internal silicon, but it's like, how much time do I actually have? Right? Like, you know, should I do that? Should I spend all my, you know, super, super smart people's time and limited, you know, this caliber of person's time doing that? Or should they focus on like, hey, can we get like asynchronous training to work? Or like, you know, figure out this next multimodal thing? Or I don't know. I don't know. Right? Right? Or should I eke out 5% more MFU and work on designing the next supercomputer? Right? Like, these kind of things, how much more valuable is that? Right? So it's like, you know, it's tough to see, you know, even OpenAI helping Microsoft enough to get their knowledge of models. So, so, so good. Right? Like, Microsoft's going to announce their chip soon. It's worse performance than the H100, but the cost effectiveness of it is better for Microsoft internally, just because they don't have to pay the NVIDIA tax. But again, like by the time they ramp it and all these sorts of things, and oh, hey, that only works on a certain size of models. Once you exceed that, then it's actually, you know, again, better for NVIDIA. So it's like, it's really tough for OpenAI to be like, yeah, we want to bet on, on Microsoft. Right? Like, and hey, we have, you know, I don't know, what's their number of people they have now? Like 700 people, you know, of which how many do low level code? Do I want to have separate code bases for this and this and this and this? And, you know, it's like, it's just like a big headache to, I don't know, I think it'd be very difficult to see anyone truly pivoting to anything besides a GPU and a TPU, especially if you have, if you need that scale. And that scale that the lab, at least the labs, right, require is absurd. Google says millions, right, of TPUs. OpenAI will say millions of GPUs, right? Like I truly do believe they think that that number of next generation GPUs, right? Like the numbers that we're going to get to are like, I bet you, I mean, I don't know, but I bet Sam Alton would say, yeah, we're going to build a hundred billion dollar supercomputer in three years or two years, right? And like after GPT-5 releases, if he goes to the market and says like, hey, I want to raise a hundred billion dollars at $500 billion valuation, I'm sure the market would give it to him, right? Like, and then they build that supercomputer, right? Like, I mean, like, I think that's like truly the path we're on. And so it's hard to, hard to imagine. Yeah. I don't know. [00:40:00] Swyx : One point that you didn't touch on and Taiwan companies are famously very chatty about the fruit company. Should we take Apple seriously at all in this game or they're just in a different world altogether? [00:40:10] Dylan : I respect their products, but like, I don't think Apple will ever release a model that you can get to say really bad things. There's all these jailbreaks, but also like as soon as they happen, like, you know, it gets fed back into OpenAI's like platform and it gets them, it's like being public and open is accelerating their like ability to make a better and better model, right? Like the RLHF and all this kind of stuff. I don't see how Apple can do that structurally, like as a company, like the fruit company ships perfect products or like, or else, right? That's why everyone loves iPhones, right? And all these like open source firms and like all these folks are doing exactly that, right? Building a bigger and better model every, you know, every few months. And I don't know how Apple gets on that train, but you know, at the same time, there's no company that has more powerful distribution, right? [00:40:56] Swyx : Are people in Taiwan concerned that it will come to a point where China will just claim Taiwan? [00:41:02] Dylan : I think, I think that a lot of people there are not super concerned, but there's some people that are super concerned. I think, I think especially after like, you know, instability across the world and in Europe and in the Middle East and even Africa, if you look at any of the stuff they're building up, it seems very clear. And if you talk to a lot of people, they think China will invade Taiwan in 27 or 26 in April or in September, sort of the best timeframes, right? Like a lot of people believe that's what will happen, right? [00:41:29] Swyx : Maybe the semi-analysis analyst point of view is, is it feasible to build this capacity up in the US? No. [00:41:35] Dylan : No, right? Like people don't understand how fragmented the semiconductor supply chain really is and how many monopolies there are. The US could absolutely shut down the Chinese semiconductor supply chain. They won't. But, and China could absolutely shut down the US one actually, by the way. But more, more relevantly, right, is like, you know, Austria has two companies, like the country of Austria and Europe has two companies that have super high market share and very specific technologies that are required for every single like, like chip period, right? There is no chip that is less than seven nanometer that doesn't get touched by this one Austrian company's tool, right? And there is no alternative. And there's another Austrian, you know, and I, it's, it's, and there's another Austrian company. Likewise, everything two nanometer and beyond will be touched by their tool. And it's like, but both of these companies are like doing well, less than a billion dollars of revenue, right? So it's like, you think it's so inconsequential. No, there's actually like three or four Japanese chemical companies, same, same idea, right? It's like the supply chain is so fragmented, right? Like people only ever talk about where the fabs were, where they actually get produced, but it's like, I mean, TSMC in Arizona, right? TSMC is building a fab in Arizona. It's, it's quite a bit smaller than the fabs in, in, in Taiwan. But even ignoring that, those fabs don't have to ship everything to Taiwan back anyways. And also they have to get what's called a mask from Taiwan and get sent to, get sent to Arizona. And by the way, there's these Japanese companies that make these chemicals that need to ship to, you know, like TOK and Shinetsu and, you know, it's like, and, and hey, it needs this tool from Austria no matter what it's like, oh wow, wait, actually like the entire supply chain is just way too fragmented. You can't like re-engineer and rebuild it on a snap, right? It's just like that. It's just complex to do that. Semiconductors are more complex than any other thing that humans do, without a doubt. There's more people working in that supply chain with XYZ backgrounds and more money invested every year and R&D plus CapEx, you know, it's like, it's just by far the most complex supply chain that humanity has. And to think that we could rebuild it in a few years is absurd. [00:43:22] Swyx : In an alternate universe, the US kept Morris Chang. I mean, people, right? Like it was just one guy. Yeah. [00:43:29] Dylan : In an alternative universe, Texas Instruments communicated to Morris Chang that he would become CEO. And so he never goes to Taiwan and you know, blah, blah, blah. Right. Yeah. No. But I, you know, that's just also, I think, I think the world would probably be further behind in terms of technology development if that didn't happen, right? Like technology proliferation is how you accelerate the pace of innovation, right? So the, you know, the dissemination to, oh, wow, hey, it's not just a bunch of people in Oregon at Intel that are leading everything, right? Or, you know, hey, a bunch of people in Samsung Korea, right? Or Shinshu, Taiwan, right? It's actually all three of those plus all these tool companies across the country and the Netherlands and in Japan and the US and, you know, it's millions of people innovating on a disseminated technology that's led us to get here, right? I don't even think, you know, if Morris Chang didn't go to Taiwan, would we even be at 5 nanometer? Would we be at 7 nanometer? Probably not, right? So there's a lot of things that, you know, happened because of that, right? [00:44:22] Alessio : Let's get a quick lightning round on semi-analysis branded one. So the first one is what are like foundational readings that people that are listening today should read to get up to speed on like semis? [00:44:34] Dylan : I think the easiest one is the PyTorch 2.0 and Triton one that I did. You know, there's the advanced packaging series. There's the Google infrastructure supremacy piece. I think that one's really critical because it explains Google's infrastructure quite a bit from networking through chips, through all that sort of history of the TPU a little bit. Maybe like AMD's MI300 piece, it talks a lot about the one that we did on that are very good. And then obviously like, you know, like, I don't know, probably like Chip Wars by Chris Miller who doesn't recommend that book, right? It's a really good book, right? I mean, like I would say Gordon Moore's book is freaking awesome because you got to think about right, like, you know, LLM scaling laws are like Moore's law on crack, right? Kind of like, you know, in a different sense, like, you know, if you think about all of human productivity gains since the 70s is probably just off of the base of semiconductors and technology, right? Of course, of course, people across the world are getting, you know, access to oil and gas and all this sort of stuff. But like, at least in the Western world, since the 70s, everything has just been mostly innovated because of technology, right? Oh, we're able to build better cars because semiconductors enable us to do that. Or be able to build better software because we're able to connect everyone because semiconductors enabled that, right? That is like, I think that's why it's the most important industry in the world. But like seeing the frame of mind of what Gordon Moore has written, you know, he's got a couple, you know, papers, books, et cetera, right? Only the paranoid survive, right? Like I think, I think like that philosophy and thought process really translates to the now modern times, except maybe, you know, humanity has been an exponential S-curve and this is like another exponential S-curve on top of that. So I think that's probably a good, good readings to do. [00:46:09] Swyx : Has there been an equivalent pivot? So Gordon, like that classic tale was more of like the pivot to memory. [00:46:16] Dylan : From memory to logic. Yeah. [00:46:18] Swyx : Yeah. And then was there, has there been an equivalent pivot in Semi's history of that magnitude? [00:46:24] Dylan : I mean, like, you know, some people would argue that like, you know, Jensen, you know, he basically didn't care about, he only cared about, you know, like gaming and 3D professional visualization and like rendering and things like that until like he started to learn about AI. And then all of a sudden he's going to like universities, like you want some GPUs, here you go. Right. Like, I think there's even stories of like, you know, not so long ago, NeurIPS, when it used to have the more unfortunate name, he would go there and just give away GPUs to people. Right. Like there's like stuff like that. Like, you know, very grassroots, like pivoting the company. Now like you, you look on gaming forums and it's like, everybody's like, oh, NVIDIA doesn't even care about us. They only care about AI and it's like, yes, you're right. They only care. They mostly only care about AI and the gaming innovations are only because of like, they're putting more AI into it. Right. It's like, but also like, hey, they're doing a lot of ship design stuff with AI. And, you know, I think, I think that's like, not, I don't know if it's equivalent pivot quite yet, but, you know, because the digital, you know, logic is a pretty big innovation, but I think that's a big one. And, you know, likewise, it's like, you know, what did, what did OpenAI do? Right. What did they pivot? How did they pivot? They left the, like, a lot of, most people left the culture of like Google brain and deep mind and decided to build this like company. That's crazy cool. Right. Like it does things in a very different way and like is innovating in a very different way. So you consider that a pivot, even though it's not inside Google. [00:47:40] Swyx : They were on a very different path with like the Dota games and all that before they eventually found like GPTs as the, as the thing. So it was a full, like started in 2015 and then like really pivoted in 2019 to be like, all right, we're the GPT company. Yeah. Yeah. If I could classify them, I don't, I'm sure there's OpenAI people who are yelling at me right now. Okay. So just a general question about, you know, I'm a fellow writer on, on Substack. You are obviously managing your consulting business while you're also publishing these amazing posts. How do you, what's your writing process? How do you source info? Like when do you sit down and go like, here's the theme for the week. Do you, do you have a pipeline going out? Just anything you can describe. [00:48:17] Dylan : I'm thankful for my, you know, my teammates cause they are actually awesome. Like, and they're much more, um, you know, directed focused to working on one thing, you know, or not one thing, but a number of things, right. Like, you know, someone who's this expert on X and Y and Z and the semiconductor supply chain. So that really helps with the, the, that side of the business. I most of the times only write when I'm very excited or, you know, it's like, Hey, like we should work on this and we should write about this. So like, you know, one of the most recent posts as we did was we explained the manufacturing process for 3D NAND, you know, flash storage, uh, gate all around transistors and 3D DRAM and all this sort of stuff. Cause there's a company in Japan that's going public, Kokusai Electric, right. It was like, okay, well we should do a post about this and we should explain this. But like, it's like, okay, we, you know, and so Myron, he did all that work, Myron and she, and most of the work and awesome. But like, usually it's like, there's a few, like very long in-depth back burner type things, right? Like that took a long time, took, you know, over a month of research and Myron knows this stuff already really well, right? Like there's stuff like that that we do and that like builds up a body of work for our consulting and some of the reports that we sell that aren't, you know, newsletter posts. But a lot of times the process is also just like, well, like Meena Eats the World is the culmination of reading that, having done a lot of work on the supply chain around the TPU ramp and co-osts and HBM capacities and all this sort of stuff to be able to, you know, figure out how many units and that Google's ordering all sorts of stuff. And then like, also like looking at like open sources, like all just that, all that culminated in like, I wrote that in four hours, right? I sent it to a couple of people and they were like, no, change this, this, this, oh, you know, add this. Cause that's really going to piss off, you know, the open source community. I'm like, okay, sure. And then posted it, right? So it's like, there's no like specific process. Unfortunately, like the most viral posts, especially in the AI community are just like those kinds of pieces rather than the, like the really deep, deep, like, you know, obviously like what was in the Gemini Eats the World post, you know, the obvious, Hey, like we, we do deep work and there's a lot more like factual, not leaks, you know, it's just factual research. Hey, we crossed the team. We go to 40 plus conferences a year, right. All the way from like a photo resist conference to a photo mask conference, to a lithography conference all the way up to like AI conferences and you know, all everything in between networking conferences and piecing everything across the supply chain. So it's like, that's like the true, like work and like, yeah, I don't know. It is sometimes bad to like have the infamousness of, you know, only people caring about this or the GPT-4 leak or the Google has no moat leak. Right. It's like, but like, you know, that's just like stuff that comes along. Right. You know, it's really focused on like understanding the supply chain and how it's pivoting and who's the winners, who's the losers, what technologies are inflecting, things like that. Where's the best place to invest resources, you know, sort of like stuff like that and in accelerating or capturing value, et cetera. [00:50:54] Alessio : Awesome. And to wrap, if you had a magic genie that could answer any question that would change your worldview, what question would you ask? [00:51:03] Dylan : That's a tough one. [00:51:04] Swyx : Like you, you operate based on a set of facts about the world right now, then there's maybe some unknowns where you're like, man, if I really knew the answer to this one, I would do so many things differently, or I would think about things very differently. [00:51:18] Dylan : So I'm of the view, at least everything that we've seen so far is that large scale training has to happen in an individual data center with very high speed networking. Now, everything doesn't need to be all to all connected, but you need very high speed networking between all of your, your chips, right? I would love to know, you know, hey, magic genie, how can we build artificial intelligence in a way that it can use multiple data centers of resources where there is a significantly lower bandwidth between pools of resources, right? Because that would instantly, like one of the big bottlenecks is how much power and how many chips you can get into a single data center. So like, A, Google and OpenAI and Anthropic are working on this, and I don't know if they've solved it yet, but if they haven't solved it yet, then what is the solution? Because that will like accelerate the scaling that can be done by not just like a factor of 10, but like orders of magnitude, because there's so many different data centers, right? Like if you, you know, across the world and, you know, oh, if I could pick up, you know, if I could effectively use 256 GPUs in this little data center here, and then with this big cluster here, you know, how can you make an algorithm that can do that? Like I think that would be like the number one thing I'd be curious to know if, how, what, because that changes the world significantly in terms of how we continue to scale this amazing technology that people have invented over the last, you know, five years. Awesome. [00:52:36] Alessio : Well, thank you so much for coming on, Dylan. [00:52:38] Dylan : Thank you. Thank you. [00:52:46] Alessio : Thank you. [00:52:46]   Get full access to Latent Space at  www.latent.space/subscrib",,"The discussed podcast transcript features a deep dive conversation between host Alessio, CTO of Decibel Partners; co-host Swyx, founder of Smol AI; and guest Dylan Patel, the author of the Semi-Analysis blog. They delve into seminal aspects of AI, particularly focusing on the development and impact of GPT-4 and the semiconductor industry's role in advancing AI technologies. Dylan Patel shares his journey from consulting in the semiconductor industry to starting his influential blog, highlighting the intersection of semiconductors, AI, and data science. He also discusses the increasing importance of infrastructure efficiency behind big tech companies like Amazon and Google, especially in the context of AI's scaling laws.

They explore the notion of GPU-rich and GPU-poor companies, introducing concepts like the distribution of computing resources in the AI research and development space. Dylan elaborates on the competitive advantage of understanding both software and low-level manufacturing, underscoring the strategic movements by big companies to ramp up production of specialized AI hardware like TPUs and the market dynamics this creates.

Further, the conversation turns to the technical and operational nuances of AI development, such as training costs, inference optimization, and the challenges of achieving high model performance within the constraints of current hardware capabilities. Dylan provides insights into the underlying architecture and strategic decisions of companies pioneering in AI hardware, shedding light on the global semiconductor supply chain's complexities and its critical role in sustaining the AI revolution.

The discussion encapsulates the urgent need for innovations that transcend current limitations in hardware and software, aiming for a paradigm shift that could dramatically scale AI's capabilities. Through the lens of semiconductor industry expertise, the podcast illuminates the fusion of technological, economic, and strategic factors driving the future of AI.","1. The importance of high-performance, efficient infrastructure for AI advancements.
2. Training costs for AI models, such as GPT-4, are seen as relatively insignificant compared to operational costs.
3. The ""GPU-poors"" vs. ""GPU-rich"" dynamic affects AI research and development access to computing resources.
4. The rapid increase in GPU production and availability impacts AI model training and experimentation.
5. The need for models to efficiently utilize both computational power (MFUs) and memory bandwidth (MBUs).
6. Speculative decoding and innovative approaches as opportunities for those with limited access to high-powered GPUs.
7. The significance of frameworks like PyTorch and emerging tools like Triton for accelerating AI computations.
8. The partnership between Google and Broadcom highlights the intricacy of networking in AI hardware.
9. The growing influence of large language models (LLMs) in consuming computational resources and shaping technological infrastructures.
10. Observations on the semiconductor industry and its critical role in supporting AI advancements, including the fragmented and specialized nature of the semiconductor supply chain.","[00:43:22] Dylan: ""Semiconductors are more complex than any other thing that humans do, without a doubt. There's more people working in that supply chain with XYZ backgrounds and more money invested every year in R&D plus CapEx...it's by far the most complex supply chain that humanity has. And to think that we could rebuild it in a few years is absurd.""

[00:48:17] Dylan: ""I most of the times only write when I'm very excited...My publication is not just a bunch of leaks...it's focused on understanding the supply chain and how it's pivoting and who's the winners, who's the losers, what technologies are inflecting, things like that.""

[00:51:03] Dylan: ""I would love to know...how can we build artificial intelligence in a way that it can use multiple data centers of resources where there is a significantly lower bandwidth between pools of resources, right? Because that would instantly, like one of the big bottlenecks is how much power and how many chips you can get into a single data center.""","1. AI Engineer Career
2. Hardware
3. Models
4. Open Source
5. Multimodal
6. Finetuning
7. Coding Tools
8. LLM Tooling
9. Startups
10. News",AI Engineer Career,Hardware,Models,Open Source,Multimodal,Finetuning,Coding Tools,LLM Tooling,Startups,News,0.2224744871,0.4449489742,0.1
AGI is Being Achieved Incrementally (DevDay Recap - cleaned audio),https://www.latent.space/p/devday-recap-clean,Alessio + swyx,"Wed, 08 Nov 2023 07:27:00 GMT",8500,https://substackcdn.com/feed/podcast/1084089/post/138928216/47ced6844308d53e2d6fc86226240c5a.jpg,https://api.substack.com/feed/podcast/138928216/d53af1811700412c52f98f4872d1a88f.mp3,"We left a high amount of background audio in  the Devday podcast , which many of you loved, but we definitely understand that some of you may have had trouble with it. Listener  Klaus Breyer  ran it through  Auphonic  with speech islolation and we figured we’d upload it as a backdated pod for people who prefer this. Of course it means that our speakers sound out of place since they now sound like they are talking loudly in a quiet room. Let us know in the comments what you think?","the cleaned part is only part 2: *  [00:55:09] Part II: Spot Interviews * [00:55:59] Jim Fan (Nvidia) - High Level Takeaways * [01:05:19] Raza Habib (Humanloop) - Foundation Model Ops * [01:13:32] Surya Dantuluri (Stealth) - RIP Plugins * [01:20:53] Reid Robinson (Zapier) - AI Actions for GPTs * [01:30:45] Div Garg (MultiOn) - GPT4V for Agents * [01:36:42] Louis Knight-Webb (Bloop.ai) - AI Code Search * [01:48:36] Shreya Rajpal (Guardrails) - Guardrails for LLMs * [01:59:00] Alex Volkov (Weights & Biases, ThursdAI) - ""Keeping AI Open"" * [02:09:39] Rahul Sonwalkar (Julius AI) - Advice for Founders   Get full access to Latent Space at  www.latent.space/subscrib","It seems like you wanted to provide a podcast transcript for me to summarize, but you have only entered ""e."" Could you please provide the transcript or details about the podcast so I can assist you properly?","I'm sorry, I can't provide a response as the provided input appears to be incomplete or incorrect. Please provide the necessary information or details for assistance.",,"1. News
2. Event Recaps
3. Monthy Recaps
4. Startups
5. AI Engineer Career
6. Finetuning
7. Open Source
8. Models
9. Hardware
10. Coding Tools",News,Event Recaps,Monthy Recaps,Startups,AI Engineer Career,Finetuning,Open Source,Models,Hardware,Coding Tools,-0.1224744871,0.22247448710000003,0.1
"AGI is Being Achieved Incrementally (OpenAI DevDay w/ Simon Willison, Alex Volkov, Jim Fan, Raza Habib, Shreya Rajpal, Rahul Ligma, et al)",https://www.latent.space/p/devday,"Simon Willison, Alex Volkov, Raza Habib, and Rahul Sonwalkar","Wed, 08 Nov 2023 00:49:53 GMT",8553,https://substackcdn.com/feed/podcast/1084089/post/138678364/b414d993bab19b257206f002aedf1d0c.jpg,https://api.substack.com/feed/podcast/138678364/b25ca34e7fe06eea04701649bd1d19d8.mp3,"SF folks: join us at the AI Engineer Foundation’s  Emergency Hackathon  tomorrow and consider  the Newton  if you’d like to cowork in the heart of  the Cerebral Arena . Our community page is  up to date  as usual! ~800,000 developers watched OpenAI Dev Day, ~8,000 of whom listened along live on  our ThursdAI x Latent Space , and ~800 of whom got tickets to attend in person: OpenAI’s first developer conference easily surpassed most people’s lowballed expectations - they simply did everything  short of announcing GPT-5 , including: *  ChatGPT  (the consumer facing product) * GPT4 Turbo already in ChatGPT (running faster, with  an April 2023 cutoff ), all noticed by users weeks before the conference * Model picker eliminated, God Model chooses for you *  GPTs  - “tailored version of ChatGPT for a specific purpose” - stopping short of “Agents”. With custom instructions, expanded knowledge, and actions, and an intuitive  no-code GPT Builder UI  (we tried all these  on our livestream yesterday  and found some issues, but also were able to  ship interesting GPTs very quickly ) and a GPT store with revenue sharing (an important criticism we focused on in  our episode on ChatGPT Plugins ) *  API  (the developer facing product) * APIs for Dall-E 3, GPT4 Vision, Code Interpreter ( RIP Advanced Data Analysis ), GPT4 Finetuning and (surprise!) Text to Speech * many thought each of these would take much longer to arrive * usable  in curl  and  in playground *  BYO Interpreter + Async Agents? *  Assistant API : stateful API backing “GPTs” like apps, with support for calling multiple tools in parallel, persistent Threads (storing message history, unlimited context window  with some asterisks ), and uploading/accessing Files (with a possibly-too- simple RAG algorithm , and expensive pricing) *  Whisper 3 announced and open sourced  ( HuggingFace recap ) * Price drops for a bunch of things! *  Misc : Custom Models for big spending ( $2-3m ) customers, Copyright Shield, Satya The progress here  feels  fast, but it is mostly (incredible) last-mile execution on model capabilities that we already knew to exist.  On reflection it is important to understand that the one guiding principle of OpenAI, even more than being Open (we address that in part 2 of today’s pod), is that slow takeoff of AGI is the best scenario for humanity, and that  this is what slow takeoff looks like : When introducing GPTs, Sam was careful to assert that “gradual iterative deployment is the best way to address the safety challenges with AI”: This is why, in fact, GPTs and Assistants are  intentionally underpowered , and it is a useful exercise to consider what else OpenAI continues to consider dangerous (for example, many people consider a while(true) loop a core driver of an agent, which GPTs conspicuously lack, though Lilian Weng of OpenAI  does not ). We convened the crew to deliver the best recap of OpenAI Dev Day in Latent Space pod style, with a 1hr deep dive with  the Functions pod crew  from 5 months ago, and then another hour with past and future guests live from the venue itself, discussing various elements of how these updates affect their thinking and startups. Enjoy! Show Notes *  swyx live thread  (see pinned messages in Twitter Space for extra links from community) * Newton AI Coworking  Interest Form  in the heart of  the Cerebral Arena","* [00:00:00] Introduction *  [00:01:59] Part I: Latent Space Pod Recap * [00:06:16] GPT4 Turbo and Assistant API * [00:13:45] JSON mode * [00:15:39] Plugins vs GPT Actions * [00:16:48] What is a ""GPT""? * [00:21:02] Criticism: the God Model * [00:22:48] Criticism: ChatGPT changes * [00:25:59] ""GPTs"" is a genius marketing move * [00:26:59] RIP Advanced Data Analysis * [00:28:50] GPT Creator as AI Prompt Engineer * [00:31:16] Zapier and Prompt Injection * [00:34:09] Copyright Shield * [00:38:03] Sharable GPTs solve the API distribution issue * [00:39:07] Voice * [00:44:59] Vision * [00:49:48] In person experience *  [00:55:11] Part II: Spot Interviews * [00:56:05] Jim Fan (Nvidia - High Level Takeaways) * [01:05:35] Raza Habib (Humanloop) - Foundation Model Ops * [01:13:59] Surya Dantuluri (Stealth) - RIP Plugins * [01:21:20] Reid Robinson (Zapier) - AI Actions for GPTs * [01:31:19] Div Garg (MultiOn) - GPT4V for Agents * [01:37:15] Louis Knight-Webb (Bloop.ai) - AI Code Search * [01:49:21] Shreya Rajpal (Guardrails.ai) - on Hallucinations * [01:59:51] Alex Volkov (Weights & Biases, ThursdAI) - ""Keeping AI Open"" * [02:10:26] Rahul Sonwalkar (Julius AI) - Advice for Founders","Swyx introduces the podcast live from the Newton, a new AI coworking space, and shares excitement about the OpenAI Dev Day, mentioning the vibrant reactions and the significant announcements made. Swyx emphasizes the role of podcasts in capturing the oral history and reactions of technology developments in real-time. This episode features a two-part structure: the first part recaps the OpenAI Dev Day with insights from developers and the second part includes interviews with previous guests and new voices providing their perspectives on the event and its announcements.

Alessio, Swyx, Alex Volkov, and Simon Willison discuss the OpenAI Dev Day, focusing on the exhaustive and innovative features released by OpenAI, including longer context lengths for GPT-4 Turbo, improved API functionalities, and advancements in AI models' ability to handle tasks more efficiently. They explore the implications of these developments for developers, businesses, and the broader tech community, expressing excitement about the potential for new applications and improved user experiences.

The discussion covers the challenges and opportunities presented by the new features, such as the need for documentation on the internal workings of models, the potential for simplified programming through natural language, and the expansion of use cases for OpenAI's technology. The group also delves into the significance of the announcements for the future of AI development, the importance of community engagement in shaping technology, and the potential impact on the competitive landscape.

Additionally, Alex Volkov provides insights into the post-Dev Day parties, contrasting the official OpenAI event with an unofficial gathering focused on keeping AI open. He highlights the different vibes and missions of these gatherings, emphasizing the importance of open-source AI models and the vibrant community working on them.

Rahul Sonwalkar shares his experience and reflections on the Dev Day, focusing on the significance of speed improvements in GPT-4 Turbo and the potential of multiple function calling and JSON output formatting for developers. He discusses applications for these features in his work with Julius AI and the importance of reducing response times for user engagement.

The podcast captures the enthusiasm and diverse perspectives within the AI community following OpenAI's Dev Day, offering a snapshot of the current state of AI development and its future direction.","1. OpenAI's Assistance API introduces new capabilities, among which are longer context lengths, cheaper pricing, and the introduction of Threads, allowing users to manage state more efficiently.
2. GPT-4 Turbo enhancements include even longer context lengths and improvements in speed and cost-effectiveness.
3. Document parsing via the API, allowing users to upload PDFs and extract structured data.
4. The launch of JSON mode, enabling models to return structured data in JSON format, making data extraction and manipulation more streamlined.
5. Development on code interpretation and execution within the models, opening up possibilities for more interactive and dynamic AI applications.
6. Introduction of Vision API, allowing for image-based inputs and enhancing multimodal interactions.
7. The debut of Assistant creation tools, simplifying the process of designing and deploying specialized GPTs for various tasks, supported by an ecosystem aimed at broader, less technical user engagement.
8. Structured data output and improved function calling capabilities, significantly expanding the models' utility for developers.
9. The emphasis on natural language as the future of programming, suggesting a paradigm shift towards no-code and low-code solutions powered by AI.
10. OpenAI hints at a broader strategy of creating a marketplace or 'app store' for GPTs, indicating a move towards commoditizing AI applications and potentially transforming how AI solutions are developed and distributed.","[00:57:11] ""I think it's going to be this new programming, where we'll just use natural language, and then refine it through dialogues. And I think that's the most natural way to do programming in the future, and the GPD App Store is showing us a glimpse of it.""

[01:16:16] ""I think people misunderstand function calling. It's essentially tool use. And if you can tell the AI, hey, you can give me multiple tools to use at once, I think that's going to unlock different applications than before. Because before it was just like, okay, this is a task, tell me one tool and what's the input for it. But if the AI can now use multiple tools in parallel... you can first of all have more specialized tools. And then get more specialized instructions for each tool. Yeah. It's just going to unlock a lot of cool applications that previously weren't possible.""

[01:22:32] ""The whole creating of an assistant and running that through an assistant is today's support. You can do that literally right now... From first message to second message... we can just see the funnel, where, if it's faster, the code runs faster... Of course there's vision stuff, there's code interpreter as a tool in the API. But, I think what will bring the most applications is actually the, the speed. Because there are so many things, if you look at our numbers, users are not patient. They want an answer, and they want an answer quick.""

[02:14:28] ""As a founder, or as, so, who's the audience? Is it like, other founders, or is it? Other founders, and people are just interested in how you are, you're processing this... What I'll tell them is, Man, so many people give up before even getting started. And that happens a lot. Don't do that. Sure you can change your idea. You can find new things to work on. But... To get something to work properly, it takes some effort. How I'm processing it?... It's like risk, right? If you, if you want to build a risk-free startup, you probably don't want to work on startups... So I'm having so much fun. The way I'm thinking about this is like, whoa, there's all these new different things I could do now. I could build. That's so exciting to me. And I'm pumped.""","1. Models
2. Finetuning
3. Coding Tools
4. AI Engineer Career
5. Startups
6. Hardware
7. Open Source
8. LLM Tooling 
9. FlashAttention 
10. Transformers Math",Models,Finetuning,Coding Tools,AI Engineer Career,Startups,Hardware,Open Source,LLM Tooling ,FlashAttention ,Transformers Math,0.0,0.0,0.20000000000000004
Beating GPT-4 with Open Source LLMs — with Michael Royzen of Phind,https://www.latent.space/p/phind,Michael Royzen,"Fri, 03 Nov 2023 16:04:49 GMT",4041,https://substackcdn.com/feed/podcast/1084089/post/138498146/22d8fdaf44def0a15a852a2b3b4b318a.jpg,https://api.substack.com/feed/podcast/138498146/8dad855b3584915ec1771a3fb2f4bbb2.mp3,"At the  AI Pioneers Summit  we announced  Latent Space Launchpad , an AI-focused accelerator in partnership with  Decibel . If you’re an AI founder of enterprise early adopter, fill out  this form  and we’ll be in touch with more details.  We also have a lot of events coming up as we wrap up the year, so make sure to check out our  community events  page and come say hi! We previously interviewed the founders of many developer productivity startups embedded in the IDE, like  Codium AI ,  Cursor , and  Codeium . We also covered  Replit’s (former) SOTA model , replit-code-v1-3b and most recently had Amjad and Michele announce  replit-code-v1_5-3b at the AI Engineer Summit . Much has been  speculated  about the StackOverflow traffic drop since ChatGPT release, but the experience is still not perfect. There’s now a new player in the “search for developers” arena:  Phind . Phind’s goal is to help you find answers to your technical questions, and then help you implement them. For example  “What should I use to create a frontend for a Python script?”  returns a list of frameworks as well as links to the sources. You can then ask follow up questions on specific implementation details, having it write some code for you, etc. They have both a web version and a  VS Code integration They recently were  top of Hacker News  with the announcement of their latest model, which is now the #1 rated model on the  BigCode Leaderboard , beating their previous version: TLDR Cheat Sheet: * Based on CodeLlama-34B, which is trained on 500B tokens * Further fine-tuned on 70B+ high quality code and reasoning tokens * Expanded context window to 16k tokens * 5x faster than GPT-4 (100 tok/s vs 20 tok/s on single stream) * 74.7% HumanEval vs 45% for the base model We’ve talked before about HumanEval being limited in a lot of cases and how it needs to be complemented with  “vibe based” evals . Phind thinks of evals alongside two axis:  *  Context quality : when asking the model to generate code, was the context high quality? Did we put outdated examples in it? Did we retrieve the wrong files? *  Result quality:  was the code generated correct? Did it follow the instructions I gave it or did it misunderstand some of it? If you have bad results with bad context, you might get to a good result by working on better RAG. If you have good context and bad result you might either need to work on your prompting or you have hit the limits of the model, which leads you to fine tuning (like they did).  Michael was really early to this space and started working on CommonCrawl filtering and indexing back in 2020, which led to a lot of the insights that now power Phind. We talked about that evolution, his experience at YC, how he got Paul Graham to invest in Phind and invite him to dinner at his house, and how Ron Conway connected him with Jensen Huang to get access to more GPUs! Show Notes *  Phind *  BigScience T0 *  InstructGPT Paper *  Inception-V3 *  LMQL *  Marginalia Nu *  Mistral AI * People: *  Paul Graham  (pg) *  Ron Conway *  Yacine Jernite  from HuggingFace *  Jeff Delaney",* [00:00:00] Intros & Michael's early interest in computer vision * [00:03:14] Pivoting to NLP and natural language question answering models * [00:07:20] Building a search engine index of Common Crawl and web pages * [00:11:26] Releasing the first version of Hello based on the search index and BigScience T0 model * [00:14:02] Deciding to focus the search engine specifically for programmers * [00:17:39] Overview of Phind's current product and focus on code reasoning * [00:21:51] The future vision for Phind to go from idea to complete code * [00:24:03] Transitioning to using the GPT-4 model and the impact it had * [00:29:43] Developing the Phind model based on CodeLlama and additional training * [00:32:28] Plans to continue improving the Phind model with open source technologies * [00:43:59] The story of meeting Paul Graham and Ron Conway and how that impacted the company * [00:53:02] How Ron Conway helped them get GPUs from Nvidia * [00:57:12] Tips on how Michael learns complex AI topics * [01:01:12] Lightning Round,"The ""Latent Space Podcast"" episode features a conversation between hosts Alessio and Swyx, and their guest, Michael Royzen from Phind. The episode kicks off with introductions and a brief discussion about recording in San Francisco during a hot October while the Blue Angels fly overhead. Michael shares his journey from being a high school entrepreneur who founded SmartLens, a computer vision startup in 2017, to his current venture, Phind. 

He discusses the technical evolution and motivation behind SmartLens, driven by the release of TensorFlow, the deep learning revolution, and improvements in computer vision models. Michael's interest in deep learning was sparked by an Apple WWDC conference he attended as a student scholar, leading him to create a model capable of recognizing a wide variety of objects running efficiently on iPhones without internet connectivity. 

SmartLens gained some attention, including a feature in TechCrunch, though its usage eventually tapered off. Despite this, it still maintains a user base that pays for the service through a subscription model, even without requiring server support. Further exploration into AI and deep learning led Michael to delve into natural language processing (NLP) and develop enterprise products, drawing him closer to Hugging Face and the world of transformer models.

Michael's path took a pivotal turn towards creating Phind after being inspired by various advancements in AI, including the integration of retrieval models with large language models (LLMs) for internet-scale question-answering systems. Phind has been developed as a system for programmers, aimed at providing accurate answers and solutions by processing extensive amounts of data and leveraging the latest AI models. The product has evolved to include various interfaces such as a web platform and a VS Code extension, catering to a broad spectrum of programming queries and challenges.

Throughout the podcast, insights into the founding story, technical breakthroughs, and the user-centric approach of Phind are shared, alongside discussions on future potential and the impact of AI on programming and problem-solving.","1. The transition from LLMs being glorified summarization models to powerful reasoning engines happened much faster than anticipated.
2. Prioritizing solving hallucinations in AI models is crucial for applications that need completely correct answers, like coding.
3. Exploring reinforcement learning for correctness rather than just avoiding harmfulness could be a significant advancement.
4. There's interest in more structured ways to evaluate LLM outputs to distinguish the sources of error effectively.
5. Applied research is merging with product development at Phind to utilize the latest AI techniques for a better search and reasoning product.
6. There's an exploration of formal grammars to restrict and modify model's decoding strategies to conform to specific grammars for improved correctness.
7. The integration of local models and hardware optimizations, like quantization and NVIDIA's FP8 implementation, is being explored for efficient local model running.
8. There's potential in developing a more structured prompting language or markup for more precise interactions with LLMs.
9. The origins of startup ideas should stem from deeply impactful insights or revelations that speak to founders on a meaningful level.","1. **Michael Royzen on the importance of addressing real user needs through technical innovation** (timecode not provided): ""I didn't even talk to users or do any of those things that you're supposed to do. I was just mainly interested in building a type of backend I've never built before.""
   
2. **On leveraging advancements in natural language processing for practical applications** (timecode not provided): ""It absolutely blew my mind... You could just ask a model a question, provide no extra context, and it would know what to do and just give you the answer.""

3. **On iterative development and learning from user feedback** (timecode not provided): ""But for the first time, it was working reasonably well... And that was the very first iteration of Hello. So I ended up doing a show HN on Hacker News in January 2022 of that system. Our fine tune T0 model connected to our Elasticsearch index... And I think that's the first example that I'm aware of a LLM search engine model that's effectively connected to like a large enough index that I consider like an internet scale.""

4. **Michael on the evolutionary path of AI technologies and addressing complex problems** (timecode not provided): ""And so we've optimized everything that we do for that. And that's a big reason of why we've built Phind specifically for programmers, as opposed to saying like, you know, we're kind of a search engine for everyone because as these models get more capable, we're very interested in seeing kind of what the emergent properties are in terms of reasoning, in terms of being able to solve complex multi-step problems.""

These quotes highlight the innovation process, the impact of AI on developing practical solutions, the importance of iterative learning, and focusing on complex problem-solving capabilities in AI development.","1. Models
2. Coding Tools
3. AI Engineer Career
4. Finetuning
5. Datasets 101
6. Open Source
7. Hardware
8. UX
9. Startups
10. News",Models,Coding Tools,AI Engineer Career,Finetuning,Datasets 101,Open Source,Hardware,UX,Startups,News,0.0,0.0,0.2
Powering your Copilot for Data – with Artem Keydunov of Cube.dev,https://www.latent.space/p/cube,Artyom Keydunov,"Thu, 26 Oct 2023 16:33:59 GMT",2328,https://substackcdn.com/feed/podcast/1084089/post/138284391/6d43635b72bf0d4c25c1fcd3c44abbd3.jpg,https://api.substack.com/feed/podcast/138284391/d5de3ed28a1d0ce1c6023a3f06653601.mp3,"The first workshops and talks from the AI Engineer Summit are  now up ! Join the >20k viewers  on YouTube , find  clips on Twitter  (we’re also clipping  @latentspacepod ), and  chat with us on Discord ! Text-to-SQL was one of the first applications of NLP.  Thoughtspot  offered “Ask your data questions” as their core differentiation compared to traditional dashboarding tools. In a way, they provide a much friendlier interface with your own structured (aka “tabular”, as in “SQL tables”) data, the same way that RLHF and Instruction Tuning helped turn the GPT-3 of 2020 into the ChatGPT of 2022. Today, natural language queries on your databases are a commodity.  There are 4 different ChatGPT plugins that offer this, as well as a bunch of startups like one of our previous guests,  Seek.ai . Perplexity originally started with a similar product in 2022:   In  March 2023  LangChain wrote a blog post on LLMs and SQL highlighting  why they don’t consistently work : *  “LLMs can write SQL, but they are often prone to making up tables, making up field” *  “LLMs have some context window which limits the amount of text they can operate over” *  “The SQL it writes may be incorrect for whatever reason, or it could be correct but just return an unexpected result.” For example, if you ask a model to “return all active users in the last 7 days” it might hallucinate a `is_active` column, join to an `activity` table that doesn’t exist, or potentially get the wrong date (especially in leap years!). We previously talked to  Shreya Rajpal at Guardrails AI , which also supports Text2SQL enforcement. Their approach was to run the actual SQL against your database and then use the error messages to improve the query:   Semantic Layers to the rescue Cube  is an  open source  semantic layer which recently  integrated with LangChain  to solve these issues in a different way. You can use YAML, Javascript, or Python to create definitions of different metrics, measures and dimensions for your data:   Creating these metrics and passing them in the model context limits the possibility for errors as the model just needs to query the `active_users` view, and Cube will then expand that into the full SQL in a reliable way. The downside of this approach compared to the Guardrails one for example is that it requires more upfront work to define metrics, but on the other hand it leads to more reliable and predictable outputs.  The promise of adding a great semantic layer to your LLM app is irresistible - you greatly minimize hallucinations, make much more token efficient prompts, and your data stays up to date without any retraining or re-indexing. However, there are also difficulties with implementing semantic layers well, so we were glad to go deep on the topic with Artem as one of the leading players in this space!",* [00:00:00] Introductions * [00:01:28] Statsbot and limitations of natural language processing in 2017 * [00:04:27] Building Cube as the infrastructure for Statsbot * [00:08:01] Open sourcing Cube in 2019 * [00:09:09] Explaining the concept of a semantic layer/Cube * [00:11:01] Using semantic layers to provide context for AI models working with tabular data * [00:14:47] Workflow of generating queries from natural language via semantic layer * [00:21:07] Using Cube to power customer-facing analytics and natural language interfaces * [00:22:38] Building data-driven AI applications and agents * [00:25:59] The future of the modern data stack * [00:29:43] Example use cases of Slack bots powered by Cube * [00:30:59] Using GPT models and limitations around math * [00:32:44] Tips for building data-driven AI apps * [00:35:20] Challenges around monetizing embedded analytics * [00:36:27] Lightning Round,"In this episode of the Latent Space podcast, hosts Swyx and Alessio engage in a comprehensive discussion with Artem Keydunov, co-founder of Cube, about the evolution of Cube from Statsbot, the challenges and advancements in data analytics, natural language processing, and the integration of AI in understanding and querying data. Artem reminisces about the inception of Statsbot, aiming to bring diverse data insights directly into Slack, tackling the challenges of natural language queries, and transitioning to developing Cube as a solution to create a semantic layer for easier data analysis. They explore the limitations of early machine learning models and the advancements leading to today's large language models (LLMs) that significantly improve natural language understanding and query generation. 

The conversation delves into the technicalities of building and using a semantic layer for data analysis, highlighting Cube's evolution into an open-source project and its significant role in the modern data stack. They touch upon the difficulties of monetizing embedded analytics and the impact of AI on automating and simplifying data workflows, from querying to transformation, emphasizing the potential for AI to augment human capabilities in data analysis rather than replace them.

Artem discusses the current trends in AI-driven analytics, such as the development of smarter chatbots and agents for internal data analysis and customer interaction, and the growing importance of a semantic layer in providing context to data for both humans and AI models. He envisions the future of data analytics where natural language interfaces become a standard feature in BI tools and analytics solutions, facilitated by the integration of AI models. Artem encourages the exploration of AI in analytics, emphasizing the rapid advancements in AI and its potential to revolutionize data understanding and decision-making processes.

Listeners are left with insights into the challenges and opportunities in leveraging AI for analytics, the importance of developing robust data infrastructure and semantic layers, and the potential for AI to transform the way businesses and individuals interact with and derive value from data.","1. Statsbot evolved from a Slack bot for analytics to Cube, focusing on semantic layers and open-source analytics infrastructure.
2. Early text-to-SQL attempts were limited by the lack of advanced large language models (LLMs) and natural language understanding.
3. The shift from Regex-based models to more complex natural language processing for text-to-SQL conversions.
4. The introduction of Cube as an open-source project to address the limitations faced by Statsbot, particularly in defining metrics and dimensions for data.
5. The evolution of language models has significantly improved the feasibility of building intelligent chatbots and analytics tools.
6. The concept of semantic layers as critical for translating natural language queries into actionable database queries.
7. The increasing role of AI and LLMs in the modern data stack, highlighting a shift towards natural language interfaces in analytics and business intelligence (BI).
8. The challenges of monetizing embedded analytics, despite its potential for creating user-facing data insights.
9. The importance of considering software engineering principles and practices in AI development, highlighting the unique aspects of AI compared to traditional software projects.
10. The potential of AI to augment data workflows, including data transformation and integration, by enabling more natural interfaces for non-technical users.","1. **On Using Foundation Models and Contextual Indexing**:
   - ""The first key step is to do some sort of indexing. That's what I was referring to, like write a book about your data, right? Describe in a text format what your data is about, right? Like what metrics it has, dimensions, what is the structures of that, what a relationship between those metrics, what are potential values of the dimensions."" [00:12:26]

2. **On the Importance of a Semantic Layer**:
   - ""Our take is that and my take is that semantic layer is just really good place for this context to leave because you need to give this context to the humans. You need to give that context to the AI system anyway, right? So that's why you define metric once and then, you know, like you teach your AI system what this metric is about."" [00:11:01]

3. **On Building Resilient AI Applications**:
   - ""I would just recommend going through to warehouse as soon as possible. I think a lot of people feel that MySQL can be a warehouse, which can be maybe on like a lower scale, but definitely not from a performance perspective. So just kind of starting with a good warehouse, a query engine, Lakehouse, that's probably like something I would recommend starting from a day zero."" [00:31:32]

4. **On Collaboration in Semantic Layers**:
   - ""I think the way I think about it is that at the end of the day, semantic layer is a code base. And in Cube, it's essentially a code base, right? It's not just a set of YAML files with pythons. I think code is never perfect, right? It's never going to be perfect. It will have a lot of, you know, like revisions of code. We have a version control, which helps it's easier with revisions. So I think we should treat our metrics and semantic layer as a code, right?"" [00:16:53]

5. **On the Scope of AI in Data Engineering**:
   - ""In terms of examples, I think a lot of people building different, you know, like agents or chatbots. You have a company that built an internal Slack bot that sort of answers questions, you know, like based on the data in a warehouse. And then like a lot of people kind of go in and like ask that chatbot this question. Is it like a real big use case? Maybe."" [00:17:42]

These quotes highlight strategic perspectives on leveraging foundational models, the vital role of semantic layers, the importance of choosing the right data infrastructure, collaboration methods in modeling, and the current and potential applications of AI in data engineering.","1. Open Source
2. AI Engineer Career
3. Models
4. Coding Tools
5. DataSets 101
6. Startups
7. Hardware
8. Multimodal
9. Agents
10. Benchmarks 101",Open Source,AI Engineer Career,Models,Coding Tools,DataSets 101,Startups,Hardware,Multimodal,Agents,Benchmarks 101,-0.3224744871,0.2224744871,0.1
The End of Finetuning — with Jeremy Howard of Fast.ai,https://www.latent.space/p/fastai,Alessio + swyx,"Thu, 19 Oct 2023 20:14:36 GMT",4155,https://substackcdn.com/feed/podcast/1084089/post/138050038/2547a55e45f7464ca70280d989e3fa41.jpg,https://api.substack.com/feed/podcast/138050038/80c92152442002d8002afad9e703db1b.mp3,"Thanks to the  over 17,000 people  who have joined the first AI Engineer Summit! A full recap is coming. Last call to fill out  the State of AI Engineering survey ! See our Community page for  upcoming meetups in SF, Paris and NYC . This episode had good interest  on Twitter  and was discussed on  the Vanishing Gradients podcast . Fast.ai’s  “Practical Deep Learning” courses  been watched by over >6,000,000 people, and the  fastai library  has over 25,000 stars on Github. Jeremy Howard, one of the creators of Fast, is now one of the most prominent and respected voices in the machine learning industry; but that wasn’t always the case.  Being non-consensus and right  In 2018, Jeremy and Sebastian Ruder published  a paper  on ULMFiT (Universal Language Model  Fine-tuning ), a 3-step transfer learning technique for NLP tasks:  The paper demonstrated that pre-trained language models could be fine-tuned on a specific task with a relatively small amount of data to achieve state-of-the-art results. They trained a 24M parameters model on WikiText-103 which was beat most benchmarks. While the paper had great results, the methods behind weren’t taken seriously by the community:  “Everybody hated fine tuning. Everybody hated transfer learning. I literally did tours trying to get people to start doing transfer learning and nobody was interested, particularly after GPT showed such good results with zero shot and few shot learning […] which I was convinced was not the right direction, but who's going to listen to me, cause as you said, I don't have a PhD, not at a university… I don't have a big set of computers to fine tune huge transformer models.” Five years later, fine-tuning is at the center of most major discussion topics in AI (we covered some like  fine tuning vs RAG  and  small models fine tuning ), and we might have gotten here earlier if Jeremy had OpenAI-level access to compute and distribution. At heart, Jeremy has always been  “GPU poor” : “I've always been somebody who does not want to build stuff on lots of big computers because most people don't have lots of big computers and I hate creating stuff that most people can't use.” This story is a good reminder of how some of the best ideas are hiding in plain sight; we recently covered  RWKV  and will continue to highlight the most interesting research that isn’t being done in the large labs.  Replacing fine-tuning with continued pre-training Even though fine-tuning is now mainstream, we still have a lot to learn. The issue of “ catastrophic forgetting ” and potential solutions have been brought up  in   many   papers : at the fine-tuning stage, the model can forget tasks it previously knew how to solve in favor of new ones.  The other issue is apparent memorization of the dataset even after a single epoch, which Jeremy covered  Can LLMs learn from a single example?  but we still don’t have the answer to.  Despite being the creator of ULMFiT, Jeremy still professes that there are a lot of open questions on finetuning: “So I still don't know how to fine tune language models properly and I haven't found anybody who feels like they do.” He now advocates for  ""continued pre-training""  - maintaining a diversity of data throughout the training process rather than separate pre-training and fine-tuning stages. Mixing instructional data, exercises, code, and other modalities while gradually curating higher quality data can avoid catastrophic forgetting and lead to more robust capabilities (something we covered in  Datasets 101 ). “Even though I originally created three-step approach that everybody now does, my view is it's actually wrong and we shouldn't use it… the right way to do this is to fine-tune language models, is to actually  throw away the idea of fine-tuning . There's no such thing. There's only  continued pre-training.   And pre-training is something where from the very start, you try to include all the kinds of data that you care about, all the kinds of problems that you care about, instructions, exercises, code, general purpose document completion, whatever. And then as you train, you gradually curate that, you know, you gradually make that higher and higher quality and more and more specific to the kinds of tasks you want it to do. But  you never throw away any data …. So yeah, that's now my view, is I think ULMFiT is the wrong approach. And that's why we're seeing a lot of these so-called alignment tax…  I think it's actually because people are training them wrong. An example of this phenomena is  CodeLlama , a LLaMA2 model finetuned on 500B tokens of code: while the model is much better at code, it’s worse on generic tasks that LLaMA2 knew how to solve well before the fine-tuning.  In the episode we also dive into all the places where open source model development and research is happening (academia vs Discords - tracked on  our Communities list  and on  our survey ), and how Jeremy recommends getting the most out of these diffuse, pseudonymous communities (similar to  the Eleuther AI Mafia ). Show Notes * Jeremy’s Background *  FastMail *  Optimal Decisions *  Kaggle *  Enlitic *  fast.ai *  Rachel Thomas *  Practical Deep Learning *  fastai for PyTorch *  nbdev *  fastec2  (the underrated library we describe) *  Can LLMs learn from a single example? * the  Kaggle LLM Science Exam  competition, which “challenges participants to answer difficult science-based questions written by a Large Language Model”. *  Sebastian Ruder *  Alec Radford *  Sylvain Gugger *  Stephen Merity *  Chris Lattner *  Modular.ai / Mojo *  Jono Whittaker *  Zeiler and Fergus paper *  ULM Fit *  DAWNBench *  Phi-1 *  Code Llama *  AlexNet","* [00:00:00] Intros and Jeremy’s background * [00:05:28] Creating ULM Fit - a breakthrough in NLP using transfer learning * [00:06:32] The rise of GPT and the appeal of few-shot learning over fine-tuning * [00:10:00] Starting Fast.ai to distribute AI capabilities beyond elite academics * [00:14:30] How modern LMs like ChatGPT still follow the ULM Fit 3-step approach * [00:17:23] Meeting with Chris Lattner on Swift for TensorFlow at Google * [00:20:00] Continued pre-training as a fine-tuning alternative * [00:22:16] Fast.ai and looking for impact vs profit maximization * [00:26:39] Using Fast.ai to create an ""army"" of AI experts to improve their domains * [00:29:32] Fast.ai's 3 focus areas - research, software, and courses * [00:38:42] Fine-tuning memorization and training curve ""clunks"" before each epoch * [00:46:47] Poor training and fine-tuning practices may be causing alignment failures * [00:48:38] Academia vs Discords * [00:53:41] Jeremy's high hopes for Chris Lattner's Mojo and its potential * [01:05:00] Adding capabilities like SQL generation through quick fine-tuning * [01:10:12] Rethinking Fast.ai courses for the AI-assisted coding era * [01:14:53] Rapid model development has created major technical debt * [01:17:08] Lightning Round AI Summary (beta) This is the first episode we’re trying this. Here’s an overview of the main topics before you dive in the transcript.  * Jeremy's background and philosophies on AI * Studied philosophy and cognitive science in college * Focused on ethics and thinking about AI even 30 years ago * Believes AI should be accessible to more people, not just elite academics/programmers * Created fast.ai to make deep learning more accessible * Development of transfer learning and ULMFit * Idea of transfer learning critical for making deep learning accessible * ULMFit pioneered transfer learning for NLP * Proposed training general language models on large corpora then fine-tuning - this became standard practice * Faced skepticism that this approach would work from NLP community * Showed state-of-the-art results on text classification soon after trying it * Current open questions around fine-tuning LLMs * Models appear to memorize training data extremely quickly (after 1 epoch) * This may hurt training dynamics and cause catastrophic forgetting * Unclear how best to fine-tune models to incorporate new information/capabilities * Need more research on model training dynamics and ideal data mixing * Exciting new developments * Mojo and new programming languages like Swift could enable faster model innovation * Still lots of room for improvements in computer vision-like innovations in transformers * Small models with fine-tuning may be surprisingly capable for many real-world tasks * Prompting strategies enable models like GPT-3 to achieve new skills like playing chess at superhuman levels * LLMs are like computer vision in 2013 - on the cusp of huge new breakthroughs in capabilities * Access to AI research * Many key convos happen in private Discord channels and forums * Becoming part of these communities can provide great learning opportunities * Being willing to do real work, not just talk about ideas, is key to gaining access * The future of practical AI * Coding becoming more accessible to non-programmers through AI assistance * Pre-requisite programming experience for learning AI may no longer be needed * Huge open questions remain about how to best train, fine-tune, and prompt LLMs","In the Latent Space Podcast episode, hosts Alessio and Swyx welcome Jeremy Howard, a significant figure in machine learning, all the way from Australia. They discuss a range of topics from Jeremy's unconventional academic journey at the University of Melbourne and his early work experiences at McKinsey, to his entrepreneurial ventures like founding both Optimal Decisions and FastMail in June 1989. Jeremy shares his journey of being at the forefront of using deep learning in medicine with Enlitic and later his venture into education and democratizing AI through Fast.ai, which he co-founded with Rachel Thomas. His work in promoting COVID-19 mask usage and his thoughts on using small businesses to mitigate failure risks were also highlighted.

Jeremy details his significant contribution to the field through the development of ULMFit, a precursor to multitask learning and Transformers, emphasizing the importance of transfer learning in making deep learning technology more accessible. He discusses the early skepticism around making deep learning accessible to non-experts and how Fast.ai aimed to change that narrative by providing practical skills in deep learning through their courses. Despite the success, Jeremy mentions the shifting focus towards models like GPT and the challenges he faced in advocating for transfer learning and fine-tuning models over zero-shot or few-shot learning approaches, which dominated the field for years.

Moreover, Jeremy touches upon his work with Kaggle and the surprise at the adoption of Fast.ai within major tech companies and AI research labs. His perspective on the importance of making deep learning technology accessible to a broader audience resonates through the conversation.

The discussion transitions into modular language model development, where Jeremy gives insights into the current state and future directions of AI research, mentioning his concerns about the lack of deep understanding of how language models learn and the need for more efficient and affordable AI solutions. He advocates for making coding and AI more accessible to all, reflecting on the potential for AI to democratize technology further and enable more people to contribute to its development positively.

Overall, the podcast episode presents a comprehensive view of Jeremy Howard's impactful work in AI and deep learning, emphasizing the importance of accessibility, education, and continuous exploration in the field.","1. AI Training Acceleration - There was a discussion about the development and acceleration of AI training capabilities. This aligns with broader trends in AI where training large models is becoming increasingly efficient and fast, though the specific insights or methods around acceleration weren't detailed in this conversation.
   
2. Modular AI Development - The conversation touched on modular, language-framework-level AI development, led by initiatives like Mojo, spearheaded by Chris Latner. These initiatives aim to make AI more accessible and customizable, potentially leading to more innovations and tailored AI solutions.
   
3. AI in Non-Physical Tasks - It was noted that AI capabilities in non-physical tasks have greatly expanded, underscoring AI's potential to outperform humans in various cognitive and analytical functions, which could have significant implications for many industries and professions.
   
4. Small Model Development - The importance of developing and fine-tuning smaller AI models was emphasized, suggesting this as a practical and accessible area for AI researchers to contribute, especially in scenarios where computational resources are limited.
   
5. Unexplored Training Dynamics in Language Models - The conversation highlighted the need for deeper understanding of how language models learn, including exploring training dynamics. This points toward a significant area of AI research that could lead to more efficient and effective model training processes.
   
6. Data Curation and Mixing for Model Training - There was a call for more rigorous analysis of what kinds of data are needed at what levels and at what times during model training. This includes understanding the mix of data needed to maintain or enhance a model's capabilities, which is crucial for optimizing AI training protocols.
   
7. Accessibility of AI Technologies - A strong message was given about the importance of making AI technologies accessible to a broader audience, beyond elite groups or individuals. This underscores a belief in the potential of widespread AI access to contribute positively to society.
   
8. Retrieval-Augmented Generation - The discussion touched on Retrieval-Augmented Generation (RAG) as an area being explored for enhancing AI’s capabilities. However, RAG was also indicated as an inefficient hack suggesting a need for further improvement and innovation.
   
9. Use of Synthetic Data - Mention was made of the use of synthetic data in training AI models, which opens up possibilities for training on data that models might not encounter naturally. It points toward exploring new training methodologies that could significantly impact model efficacy.
   
10. AI’s Role in Chess Playing - The conversation included an interesting insight into AI's potential in playing chess, showcasing the ability of language models to potentially outperform traditional chess engines with the right prompting strategies. This serves as a unique example of AI's capabilities extending into areas previously dominated by specialized algorithms.",". **[00:05:28]** ""And we weren't sure if it was possible either, but we kind of felt like we had to give it a go because the alternative was we were pretty sure that deep learning was on its way to becoming, you know, the most or one of the most, you know, important technologies in human history. And if the only people that could use it were a handful of computer science PhDs, that seemed like A, a big waste and B, kind of dangerous.""

2. **[00:18:17]** ""you know, every way. Like the reason it took me so long to try it was because I asked all my friends in NLP if this could work. And everybody said, no, it definitely won't work.""

3. **[00:22:16]** ""I've always been somebody who does not want to build stuff on lots of big computers because most people don't have lots of big computers and I hate creating stuff that most people can't use.""

4. **[00:28:56]** ""I think it shows you don't necessarily need a huge amount of money and a huge team of people to make an impact.""

5. **[00:39:01]** ""And so we went back and like, okay, let's just run some experiments, you know, cause nobody seems to have actually published anything about this.""

6. **[01:14:57]** ""Google now does have a backup plan, you know, they have Jax, which was never a strategy. It was just a bunch of people who also recognized TensorFlow 2 as s**t and they just decided to build something else.""

7. **[01:19:58]** ""How do language models learn? You know, what are the training dynamics?""

These quotes are selected based on their relevance to key discussions and insights into AI and machine learning, particularly concerning deep learning, the challenges of accessibility and democratization, training dynamics, and how the field is evolving.","1. LLM Tooling
2. Finetuning
3. Models
4. AI Engineer Career
5. Coding Tools
6. Open Source
7. Multimodal
8. Hardware
9. RHLF 201
10. FlashAttention",LLM Tooling,Finetuning,Models,AI Engineer Career,Coding Tools,Open Source,Multimodal,Hardware,RHLF 201,FlashAttention,0.3449489742,0.2224744871,0.0
Why AI Agents Don't Work (yet) - with Kanjun Qiu of Imbue,https://www.latent.space/p/imbue,Kanjun Qiu,"Sat, 14 Oct 2023 21:15:58 GMT",3902,https://substackcdn.com/feed/podcast/1084089/post/137874596/d0ffa0af37a2e045cfd1f24211b7bd11.jpg,https://api.substack.com/feed/podcast/137874596/1d4bb9d64416e350adf14925dad36d74.mp3,"Thanks to the  over 11,000 people  who joined us for the first AI Engineer Summit! A full recap is coming, but you can 1) catch up on the fun and videos on  Twitter  and  YouTube , 2) help us reach 1000 people for the first comprehensive  State of AI Engineering  survey and 3) submit projects for the new  AI Engineer Foundation . See our Community page for  upcoming meetups in SF, Paris, NYC, and Singapore .   This episode had good interest  on Twitter . Last month, Imbue was crowned as AI’s newest unicorn foundation model lab,  raising a $200m Series B at a >$1 billion valuation . As “stealth” foundation model companies go, Imbue (f.k.a. Generally Intelligent) has stood as an enigmatic group given they have no publicly released models to try out. However, ever since  their $20m Series A last year  their goal has been to “develop generally capable AI agents with human-like intelligence in order to solve problems in the real world”. From RL to Reasoning LLMs Along with their Series A, they announced  Avalon ,  “A Benchmark for RL Generalization Using Procedurally Generated Worlds”.  Avalon is built on top of the open source Godot game engine, and is  ~100x faster  than Minecraft to enable fast RL benchmarking and a clear reward with adjustable game difficulty. After a while, they realized that pure RL isn’t a good path to teach reasoning and planning. The agents were able to learn mechanical things like opening complex doors, climbing, but  couldn’t go to higher level tasks . A pure RL world also doesn’t include a language explanation of the agent reasoning, which made it hard to understand why it made certain decisions. That pushed the team more towards the “ models for reasoning ” path: “The second thing we learned is that  pure reinforcement learning is not a good vehicle for planning and reasoning . So these agents were able to learn all sorts of crazy things: They could learn to climb like hand over hand in VR climbing, they could learn to open doors like very complicated, like multiple switches and a lever open the door,  but they couldn't do any higher level things . And they couldn't do those lower level things consistently necessarily. And as a user,  I do not want to interact with a pure reinforcement learning end to end RL agent.  As a user, like I need much more control over what that agent is doing.” Inspired by  Chelsea Finn ’s work on  SayCan  at Stanford, the team pivoted to have their agents do the reasoning in natural language instead. This development parallels the large leaps in reasoning that humans have developed as the scientific method: “ We are better at reasoning now than we were 3000 years ago . An example of a reasoning strategy is noticing you're confused. Then when I notice I'm confused, I should ask: *  What was the original claim that was made?  *  What evidence is there for this claim?  *  Does the evidence support the claim?  *  Is the claim correct?  This is like a reasoning strategy that was developed in like the 1600s, you know, with like the advent of science. So that's an example of a reasoning strategy. There are tons of them. We employ all the time, lots of heuristics that help us be better at reasoning. And we can generate data that's much more specific to them.“ The Full Stack Model Lab One year later, it would seem that the pivot to reasoning has had tremendous success, and Imbue has now reached a >$1B valuation, with participation from Astera Institute, NVIDIA, Cruise CEO Kyle Vogt, Notion co-founder Simon Last, and others. Imbue tackles their work with a “full stack” approach: *  Models.  Pretraining  very large (>100B parameter) models , optimized to perform well on internal reasoning benchmarks, with  a ~10,000 Nvidia H100 GPU cluster  lets us iterate rapidly on everything from training data to architecture and reasoning mechanisms. *  Tools and Agents.  Building internal productivity tools from  coding agents  for fixing type checking and linting errors, to sophisticated systems like  CARBS  (for  hyperparameter tuning and network architecture search). *  Interface Invention.  Solving agent trust and collaboration (not merely communication) with humans by creating better abstractions and interfaces — IDEs for users to program computers in natural language. *  Theory.   Publishing research  about the theoretical underpinnings of self-supervised learning, as well as  scaling laws  for machine learning research. Kanjun believes we are still in the “bare metal phase” of agent development, and they want to take a holistic approach to building the “operating system for agents”. We loved diving deep into the Imbue approach toward solving the AI Holy Grail of reliable agents, and are excited to share our conversation with you today!","* [00:00:00] Introductions * [00:06:07] The origin story of Imbue * [00:09:39] Imbue's approach to training large foundation models optimized for reasoning * [00:12:18] Imbue's goals to build an ""operating system"" for reliable, inspectable AI agents * [00:15:37] Imbue's process of developing internal tools and interfaces to collaborate with AI agents * [00:17:27] Imbue's focus on improving reasoning capabilities in models, using code and other data * [00:19:50] The value of using both public benchmarks and internal metrics to evaluate progress * [00:21:43] Lessons learned from developing the Avalon research environment * [00:23:31] The limitations of pure reinforcement learning for general intelligence * [00:28:36] Imbue's vision for building better abstractions and interfaces for reliable agents * [00:31:36] Interface design for collaborating with, rather than just communicating with, AI agents * [00:37:40] The future potential of an agent-to-agent protocol * [00:39:29] Leveraging approaches like critiquing between models and chain of thought * [00:45:49] Kanjun's philosophy on enabling team members as creative agents at Imbue * [00:53:51] Kanjun's experience co-founding the communal co-living space The Archive * [01:00:22] Lightning Round Show Notes *  Imbue *  Avalon *  CARBS (hyperparameter optimizer) *  Series B announcement *  Kanjun/Imbue’s Podcast *  MIT Media Lab * Research mentioned: *  Momentum Contrast *  SimClr *  Chelsea Finn  -  SayCan *  Agent Protocol  - part of the  AI Engineer Foundation *  Xerox PARC *  Michael Nielsen *  Jason Benn *  Outset Capital *  Scenius - Kevin Kelly *  South Park Commons *  The Archive *  Thursday Nights in AI","In the Latent Space Podcast, co-hosts Alessio and Swyx welcome Kanjun Qiu from Imbue (formerly Generally Intelligent). Kanjun, who has a rich background from MIT to Dropbox and as a thrice founder, discusses the rebranding from Generally Intelligent to Imbue, focusing on Imbue's journey into enhancing AI's capacity for human agency and potential. She shares her personal drive outside of work, emphasizing her passion for human agency and the development of systems that boost effectiveness and creativity. Kanjun recounts her path from founding Ember and Sorceress to realizing the potential of AI in augmenting human capabilities and her current work at Imbue towards developing trusted AI agents and interfaces.

Kanjun explores the challenges in creating AI that genuinely augments human abilities, stating that reasoning and trustworthiness are central to deploying effective AI agents in the real world. Imbue's approach involves training large foundation models optimized for reasoning, building robust internal tools for research and development, and fostering a unique company culture centered on unlocking individual superpowers.

The discussion also touches on Kanjun's perspective on the future of personal computing, rekindled through AI agents capable of offloading and enhancing our intellectual tasks. She mentions the importance of developing non-leaky abstractions and user-friendly interfaces for higher reliability and trust in AI systems. Towards the end, Kanjun articulates her vision for AI's role in addressing the world's challenges by providing ""free intellectual energy"" and the necessity of thoughtful regulation and safety measures to ensure AI's positive impact.","1. Imbue, previously Generally Intelligent, focuses on creating AI agents and interfaces that are trustworthy and can perform long-range goals through understanding and employing human reasoning.
2. Sorceress, an AI product developed by Imbue's founders, automated recruitment by finding suitable candidates and facilitating outreach, revealing early insights into creating AI systems that users can trust and rely on.
3. AI research is trending towards self-supervised learning, reflecting a direction where AI models learn in a way akin to human learning, without needing explicit supervision.
4. Imbue aims to rekindle the dream of the personal computer by developing large foundation models optimized for reasoning to create more capable and trustworthy agents.
5. The importance of building non-leaky abstractions in AI agents to ensure reliability and deployability, drawing parallels to software development's evolution towards higher level programming languages.
6. Utilizing code as a form of explicit reasoning data significantly benefits AI model training by providing clear, structured examples of logical processes.
7. The challenge of long-term memory and context in AI models, highlighting the necessity of developing systems that can store and utilize vast amounts of information for complex tasks.
8. The initiative to co-build evaluations for AI agents with other developers, addressing the need for benchmarks that accurately measure an agent's effectiveness in various applications.
9. Imbue's approach of dogfooding, or using its own AI agents internally, as a core strategy for developing practical and effective solutions.
10. The exploration of AI policy and regulation, with Imbue actively participating in analyzing and proposing guidelines to ensure that AI technologies are deployed in ways that are beneficial and not harmful.","1. ""I just really believe that as humans, we can be a lot more than we are. And it's what drives everything."" (Kanjun at [00:02:03])

2. ""What Josh and I were really interested in is how can we get computers that help us do bigger things?"" (Kanjun at [00:06:07])

3. ""A lot of what we do at Imbue is working on interfaces to figure out how do we get to a situation where when you're building and using agents, these agents are trustworthy to the end user."" (Kanjun at [00:05:52])

4. ""Code is the most explicit example of reasoning data on the internet."" (Kanjun at [00:32:08])

5. ""We're in a historic moment where we'll look back and be like, 'Oh my God, the future was invented in these years.'"" (Kanjun at [01:01:09])

6. ""AI is a tool like any technology... the world could be much better, could be much more thoughtful because of that."" (Kanjun at [01:01:53])

7. ""The takeaway is like the future could be really exciting if we can actually get agents that are able to do these bigger things. Reasoning is the biggest blocker plus these sets of abstractions to make things more robust and reliable."" (Kanjun at [01:04:36])

These quotes encapsulate themes of human potential and agency, the transformative power of AI, the importance of creating trustworthy AI agents, the unique contributions of coding to AI reasoning, the significant moment we're in for AI development, the optimistic view of how AI can improve the world, and the challenges and potential of AI agents in the future.","1. AI Engineer Career
2. Agents
3. Models
4. Open Source
5. Coding Tools
6. Multimodal
7. Hardware
8. Startups
9. Finetuning
10. Monthly Recaps",AI Engineer Career,Agents,Models,Open Source,Coding Tools,Multimodal,Hardware,Startups,Finetuning,Monthly Recaps,0.0,0.0,0.2
[AIE Summit Preview #2] The AI Horcrux — Swyx on Cognitive Revolution,https://www.latent.space/p/cogrev-aies,swyx & Alessio,"Sun, 08 Oct 2023 07:30:08 GMT",5388,https://substackcdn.com/feed/podcast/1084089/post/137740646/dc7347cf64fd811ffc942d292a9bb4ac.jpg,https://api.substack.com/feed/podcast/137740646/38e11d709d1655ccab3f3e308f9e3ce1.mp3,"This is a special double weekend crosspost of AI podcasts, helping attendees prepare for the  AI Engineer Summit  next week. After our first friendly  feedswap with the Cognitive Revolution pod , swyx was invited for a full episode to go over the state of AI Engineering and to preview the  AI Engineer Summit Schedule , where we share many former CogRev guests as speakers. For those seeking to understand how two top AI podcasts think about major top of mind AI Engineering topics, this should be the perfect place to get up to speed, which will be a preview of many of the conversations taking place during the topic tables sessions on the night of Monday October 9 at the AI Engineer Summit. While you are listening, there are two things you can do to be part of the AI Engineer experience. One,  join the AI Engineer Summit Slack . Two,  take the State of AI Engineering survey  and help us get to 1000 respondents! Links *  AI Engineer Summit  (Join livestream and Slack community) *  State of AI Engineering Survey  (please help us fill this out to represent you!) *  Cognitive Revolution full episode with Nathan *  swyx’s ai-notes  (featuring Communities in README.md) * We referenced  The Eleuther AI Mafia * This podcast intro voice was AI Anna again, from  our Wondercraft pod !","* ( 00:00:49 ) AI Nathan’s intro  * ( 00:03:14 ) What is an AI engineer?  * ( 00:05:56 ) What backgrounds do AI engineers typically have?  * ( 00:17:13 ) Swyx’s Discord AI project  * ( 00:20:41 ) Key tools for AI engineers  * ( 00:23:42 ) HumanLoop, Guardrails, Langchain  * ( 00:27:01 ) Criteria for identifying capable AI engineers when hiring  * ( 00:30:59 ) Skepticism around AI being a fad and doubts about contributing to AI  * ( 00:34:03 ) AI Engineer Conference speaker lineup  * ( 00:41:14 ) AI agents and two years to AGI  * ( 00:46:04 ) Expectations and disagreement around what AI agent capabilities will work soon  * ( 00:50:12 ) Swyx’s OpenAI thesis  * ( 00:53:03 ) AI safety considerations and the role of AI engineers  * ( 00:56:24 ) Disagreement on whether AI will soon be able to generate code pull requests  * ( 01:01:07 ) AI helping non-technical people to code  * ( 01:01:49 ) Multi-modal Chat-GPT and the future implications  * ( 01:03:33 ) Nathan living in the same dorm as Mark Zuckerberg  * ( 01:04:44 ) Competitive dynamics between OpenAI and other AI model developers  * ( 01:05:39 ) Play.ht vs ElevenLabs  * ( 01:09:20 ) The tension between platforms and developers building on top of them  * ( 01:11:40 ) The best thing startups can do to compete with foundation model providers  * ( 01:16:26 ) User identity/authentication services like Login with OpenAI  * ( 01:19:20 ) Google vs the other live players  * ( 01:20:46 ) AI Horcruxes / Pendants  * ( 01:22:05 ) The concept of an AI app bundle for consumers and developers   Get full access to Latent Space at  www.latent.space/subscrib",It looks like there might have been an error in entering the podcast transcript. Could you please provide the specific text or details you'd like summarized from the podcast?,,,"1. News
2. Event Recaps
3. Monthy Recaps
4. Startups
5. Open Source
6. Finetuning
7. Hardware
8. LLM Tooling
9. Coding Tools
10. AI Engineer Career",News,Event Recaps,Monthy Recaps,Startups,Open Source,Finetuning,Hardware,LLM Tooling,Coding Tools,AI Engineer Career,0.1,0.2224744871,0.0
[AIE Summit Preview #1] Swyx on Software 3.0 and the Rise of the AI Engineer,https://www.latent.space/p/aies-podrocket,Alessio + swyx,"Sat, 07 Oct 2023 07:00:44 GMT",2329,https://substackcdn.com/feed/podcast/1084089/post/137745139/8c3c4b05b03c0f8002cc2062c5b404b3.jpg,https://api.substack.com/feed/podcast/137745139/8828ebd77be6d4579849f2307dabd95c.mp3,"This is a special double weekend crosspost of AI podcasts, helping attendees prepare for the  AI Engineer Summit  next week. Swyx gave a keynote on the Software 3.0 Landscape recently (referenced in  our recent Humanloop episode ) and was invited to go deeper in podcast format, and to preview the  AI Engineer Summit Schedule .  For those seeking to ramp up on the current state of thinking on AI Engineering, this should be the perfect place to start, alongside our upcoming  Latent Space University course  (which is being tested live for the first time at the Summit workshops). While you are listening, there are two things you can do to be part of the AI Engineer experience. One,  join the AI Engineer Summit Slack . Two,  take the State of AI Engineering survey  and help us get to 1000 respondents!  Full transcript available  here !  Links *  AI Engineer Summit  (Join livestream and Slack community) *  State of AI Engineering Survey  (please help us fill this out to represent you!) *  Podrocket full episode  by  Tejas Kumar Show notes *  Explaining Software 1.0, 2.0, and 3.0 * Software 1.0: Hand-coded software with conditional logic, loops, etc. * Software 2.0: Machine learning models like neural nets trained on data * Software 3.0: Using large pre-trained foundation models without needing to collect/label training data *  Foundation Models and Model Architecture * Foundation models like GPT-3/4, Claude, Whisper - can be used off the shelf via API * Model architecture refers to the layers and structure of a ML model * Grabbing a pre-trained model lets you skip data collection and training *  Putting Foundation Models into Production * Levels of difficulty: calling an API, running locally, fully serving high-volume predictions * Key factors: GPU utilization, batching, infrastructure expertise *  The Emerging AI Developer Landscape * AI is becoming more accessible to ""traditional"" software engineers * Distinction between ML engineers and new role of AI engineers * AI engineers consume foundation model APIs vs. developing models from scratch *  The Economics of AI Engineers * Demand for AI exceeds supply of ML experts to build it * AI engineers will emerge out of software engineers learning these skills *  Defining the AI Engineering Stack * System of reasoning: Foundation model APIs * Retrieval augmented generation (RAG) stack: Connects models to data * AI UX: New modalities and interfaces beyond chatbots *  Building Products with Foundation Models * Replicating existing features isn't enough - need unique value * Focus on solving customer problems and building trust *  AI Skepticism and Hype * Some skepticism is healthy, but ""AI blame"" also emerges * High expectations from media/industry creators * Important to stay grounded in real customer needs *  Meaningful AI Applications * Many examples of AI positively impacting lives already * Engineers have power to build and explore - lots of opportunity *  Closing and AI Engineer Summit Details * October 8-10 virtual conference for AI engineers * Speakers from OpenAI, Microsoft, Amazon, etc * Free to attend online   Get full access to Latent Space at  www.latent.space/subscrib",,"It seems like you've provided an incomplete request mentioning just the letter ""e."" To assist you better, could you please provide more details or the actual transcript of the podcast you would like summarized?",,,"1. News
2. Event Recaps
3. Monthly Recaps
4. Benchmarks 101
5. Datasets 101
6. RHLF 201
7. FlashAttention 
8. Transformers Math
9. Models
10. AI Engineer Career",News,Event Recaps,Monthly Recaps,Benchmarks 101,Datasets 101,RHLF 201,FlashAttention ,Transformers Math,Models,AI Engineer Career,-0.1224744871,-0.1224744871,0.1224744871
RAG Is A Hack - with Jerry Liu from LlamaIndex,https://www.latent.space/p/llamaindex,Jerry Liu,"Thu, 05 Oct 2023 16:26:00 GMT",4086,https://substackcdn.com/feed/podcast/1084089/post/137668059/b41862a061cf48479a0003882e58b056.jpg,https://api.substack.com/feed/podcast/137668059/94e65c569e8fcf454463e3de10a24e1b.mp3,"Want to help define  the AI Engineer stack ? >800 folks have weighed in on the top tools, communities and builders for the first  State of AI Engineering  survey, which we will present for the first time at next week’s  AI Engineer Summit . Join us  online ! This post had robust discussion on  HN  and  Twitter . In October 2022, Robust Intelligence hosted an internal hackathon to play around with LLMs which led to the creation of two of the most important AI Engineering tools: LangChain 🦜⛓️ ( our interview with Harrison here ) and LlamaIndex 🦙 by Jerry Liu, which we’ll cover today. In less than a year, LlamaIndex has crossed 600,000 monthly downloads, raised $8.5M from Greylock, has a fast growing open source community that contributes to LlamaHub, and it doesn’t seem to be slowing down. LlamaIndex’s Origin (aka GPT Tree Index) Jerry struggled to make large amounts of data work with GPT-3 (which had a 4,096 tokens context window). Today LlamaIndex is at the forefront of the RAG wave (Retrieval Augmented Generation), but in the beginning Jerry wasn’t focused on embeddings and search, but rather on understanding how models could summarize, link, and reason about data.  On November 5th, Jerry pushed the  first version  to Github under the name “GPT Tree Index”:  The GPT Tree Index first takes in a large dataset of unprocessed text data as input. It then builds up a tree-index in a bottom-up fashion; each parent node is able to summarize the children nodes using a general  summarization prompt ; each intermediate node containing summary text summarizing the components below. Once the index is built, it can be saved to disk and loaded for future use. Then, say the user wants to use GPT-3 to answer a question. Using a  query prompt template , GPT-3 will be able to recursively perform tree traversal in a top-down fashion in order to answer a question. For example, in the very beginning GPT-3 is tasked with selecting between *n* top-level nodes which best answers a provided query, by outputting a number as a multiple-choice problem. The GPT Tree Index then uses the number to select the corresponding node, and the process repeats recursively among the children nodes until a leaf node is reached. […] How is this better than an embeddings-based approach / other state-of-the-art QA and retrieval methods? The intent is not to compete against existing methods. A simpler embedding-based technique could be to just encode each chunk as an embedding and do a simple question-document embedding look-up to retrieve the result. This project is a simple exercise to test how GPT can organize and lookup information. The project attracted a lot of attention early on (the announcement tweet has ~330 likes), but it wasn’t until ~February 2023 that the open source community really started to explode, which was around the same time that LlamaHub was released. LlamaHub made it easy for developers to import data from Google Drive, Discord, Slack, databases, and more into their LlamaIndex projects.   What is LlamaIndex?  As we mentioned, LlamaIndex is leading the charge in the development of the RAG stack. RAG boils down to two parts: *  Indexing  (i.e. how do you load and index the data in your knowledge base) *  Querying  (i.e. how do you surface the data and fit it in the model context)  Indexing To get your data from all your sources to your RAG knowledge base, you can leverage a few tools:  *  Documents / Nodes : A Document is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. A Node is the atomic unit of data in LlamaIndex and represents a “chunk” of a source Document (i.e. one Document has many Node) as well as its relationship to other Node objects. *  Data Connectors : A data connector ingest data from different sources and turn them into Document representations (text and simple metadata). These connectors are offered through  LlamaHub , and there are over 200 of them today. *  Data Indexes : Once you’ve ingested your data, LlamaIndex will help you index the data into a format that’s easy to retrieve. There are many  types of indexes  (Summary, Tree, Vector, etc). Under the hood, LlamaIndex parses the raw documents into intermediate representations, calculates vector embeddings, and infers metadata. The most commonly used index is the  VectorStoreIndex , which can then be paired with any of the vector stores out there ( an example with Chroma ). Querying The RAG pipeline, during the querying phase, sources the most pertinent context from a user's prompt, forwarding it along to the LLM. This equips the LLM with current / private knowledge beyond its foundational training data. LlamaIndex offers adaptable modules tailored for building RAG pathways for Q&A, chatbots, or agent use, since each of them has different requirements. For example, a chatbot should expect the user to interject with follow up questions, while an agent will try to carry out a whole task on its own without user intervention.   Building Blocks *  Retrievers : A retriever defines how to efficiently retrieve relevant context from a knowledge base (i.e. index) when given a query. Vector index is the most popular mode, but there are  other options  like Summary, Tree, Keyword Table, Knowledge Graph, and Document Summary.   *  Node Postprocessors : Once the retriever gets you Node objects back, you will need to do additional work like discarding low similarity ones. There are  many options  here as well, such as `SimilarityPostprocessor` (i.e. drop nodes below a certain similarity score) or `LongContextReorder` which helps avoid the issues raised in the “ Lost in the Middle, U-shaped recollection curve ” paper.  *  Response Synthesizers : Takes a user query and your retrieved chunks, and prompts and LLM with them. There are a few  response modes  here that balance thoroughness and compactness. Pipelines *  Query Engines : A query engine is an end-to-end pipeline that allow you to ask question over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM. This makes it possible to do things like  “Ask panda questions”  by leveraging Panda dataframes as a data source.  *  Chat Engines : A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question & answer). This supports traditional OpenAI-style chat interfaces, as well as more advanced ones like  ReAct . *  Agents : An agent is an automated decision maker (powered by an LLM) that interacts with the world via a set of tools. Agent may be used in the same fashion as query engines or chat engines, but they have the power to both read and write data. For reasoning, you can use either OpenAI Functions or ReAct. Both can leverage the tools offered through LlamaHub for further analysis. RAG vs Finetuning Now that you have a full overview of what LlamaIndex does, the next question is “When should I use this and when should I fine tune?”. Jerry’s TLDR is that “RAG is just a hack”, but a powerful one. Each option has pros and cons: *  Lower investment:  RAG requires almost 0 upfront investment, unlike finetuning which requires data cleaning, model training, increased costs for finetuned inference, etc. *  Stricter access control and higher visibility:  when finetuning, the model learns everything. With RAG, you can decide what documents the index should have access to, making it more secure by default. You are also able to see everything that was passed into the context if a response doesn’t look right. *  Context window limitation:  you can only fit so many tokens into the prompt due to the way models work. Finetuning helps you circumvent that by compressing the knowledge into the model weights rather than putting it in the prompt.    As Jerry says, the best way to know this inside out is  to learn to build RAG from scratch  (without LlamaIndex) - and they have plenty of tutorials on his Twitter and blog to learn this. The other issue is that the math for finetuning isn’t well known yet as we  discussed with Quentin Anthony from Eleuther , so unless you have money and time to invest into exploring fine tuning, you’re better off starting with RAG.   Full YouTube Discussion! Show Notes *  LlamaIndex *  LlamaHub *  SEC Insights *  Robust Intelligence *  Quora’s Poe *  Chroma *  Vespa *  Why should every AI engineer learn to build RAG from scratch ? *  LangChain *  Gorilla *  Lost in the Middle: How Language Models Use Long Contexts","* [00:00:00] Introductions and Jerry’s background * [00:04:30] Starting LlamaIndex as a side project * [00:05:11] Evolution from tree-index to current LlamaIndex and LlamaHub architecture * [00:11:39] Deciding to leave Robust to start the LlamaIndex company and raising funding * [00:20:06] Context window size and information capacity for LLMs * [00:21:34] Minimum viable context and maximum context for RAG * [00:22:52] Fine-tuning vs RAG - current limitations and future potential * [00:24:02] RAG as a hack but good hack for now * [00:26:19] RAG benefits - transparency and access control * [00:27:46] Potential for fine-tuning to take over some RAG capabilities * [00:30:04] Baking everything into an end-to-end trained LLM * [00:33:24] Similarities between iterating on ML models and LLM apps * [00:34:47] Modularity and customization options in LlamaIndex: data loading, retrieval, synthesis, reasoning * [00:40:16] Evaluating and optimizing each component of Lama Index system * [00:46:02] Building retrieval benchmarks to evaluate RAG * [00:47:24] SEC Insights - open source full stack LLM app using LlamaIndex * [00:49:48] Enterprise platform to complement LlamaIndex open source * [00:51:00] Community contributions for LlamaHub data loaders * [00:53:21] LLM engine usage - majority OpenAI but options expanding * [00:56:25] Vector store landscape * [00:59:46] Exploring relationships and graphs within data * [01:03:24] Additional complexity of evaluating agent loops * [01:04:01] Lightning Round","In the ""Latent Space Podcast,"" hosts Alessio and Swyx interview Jerry Liu, a prominent figure in the AI and machine learning community. Throughout the conversation, they delve into Jerry's academic background, which includes a bachelor's in computer science and a certificate in finance from Princeton. They touch on Jerry's professional journey, highlighting his roles as a machine learning engineer at Quora and Uber, as well as his contributions to AI research and development at various companies.

A significant portion of the discussion revolves around Jerry's efforts in making deep learning advancements more accessible through platforms like Quora and his work on generative adversarial networks (GANs), among other topics. They also explore the foundation and objectives of LlamaIndex, Jerry's recent venture that aims to enhance LLM (Large Language Model) applications by addressing challenges related to data accessibility, retrieval, and synthesis. Jerry shares insights on how his experience in AI research and machine learning engineering has informed the development of LlamaIndex, emphasizing the startup's focus on optimizing the interaction between large language models and extensive data sets.

Additionally, they discuss ongoing trends in the AI industry, including the growing importance of fine-tuning and retrieval techniques in LLMs, the potential impacts of evolving model architectures, and the significance of open-source contributions to fostering innovation in the field. The conversation also highlights the collaborative nature of AI development, referencing Jerry's interactions with other experts and companies striving to push the boundaries of what's possible with AI technologies.

Towards the end of the podcast, the discussion shifts to future directions for LlamaIndex and the broader AI landscape. Jerry expresses optimism about the advancements in personalized AI applications and theorizes about the potential convergence of structured and unstructured data querying to enhance the power and precision of AI-driven insights.

Overall, the podcast provides an in-depth look into Jerry Liu's contributions and perspectives on the evolving world of AI and machine learning, offering listeners valuable insights into the challenges, opportunities, and future prospects of the field.","1. The increasing accessibility of generative AI is fundamentally changing how AI research and applications are developed, making it more accessible to a broader range of developers and researchers.
2. The concept of Retrieval-Augmented Generation (RAG) as a hack, but a very effective and crucial one for current AI applications, signaling a potential shift towards more end-to-end optimized models in the future.
3. The idea that fine-tuning and RAG will evolve to include more sophisticated data retrieval beyond embeddings, potentially integrating more complex reasoning and structured data querying.
4. The potential of OpenSource models and platforms to challenge the dominance of proprietary models by OpenAI and other companies, driven by market demand for more open and versatile solutions.
5. The importance of synthetic dataset generation for evaluation, providing a scalable solution for evaluating models more efficiently than traditional human annotation.
6. The emergent need for better tools and practices around RAG development, including understanding how to effectively chunk, parse data, and optimize retrieval mechanisms.
7. The role of personalization and memory in AI, indicating an unexplored territory in AI research that could lead to more personalized AI applications through continuous fine-tuning and innovative model architectures.
8. The potential for new retrieval techniques that blend old information retrieval principles with new LLM capabilities, like chain of thought reasoning and graph-based data representations, for more sophisticated data interactions.","**[00:40:16] Jerry:** ""I think the thing is when people start out, like they will probably use these loaders and it's a great tool to get started. But if you get to a point where you actually want to call like an API that hasn't been supported yet, or, you know, you want to load in stuff in metadata or something that hasn't been directly baked into the logic of a loader itself, people start adding up, like writing their own custom loaders. And that is a thing that we're seeing.""

**[00:42:55] Jerry:** ""I mean, I think for us, it's more, we just have a very flexible like document abstraction that you can fill in with any content that you want.""

**[00:48:53] Jerry:** ""One is just like kind of more detailed guides on like different modular components within it. So if you're like a full stack developer, you can go in and actually take the pieces that you want and actually kind of build your own custom flows.""

**[00:55:49] Jerry:** ""I think part of what I wanted to talk about a conceptual level, especially with the idea of like thinking about what is the minimum context? [...] The way I kind of think about this from a more principled manner is like you have like, there's this concept of like information capacity, just this idea of like entropy, like given any fixed amount of like storage space, like how much information can you actually compact in there?""

**[01:04:23] Jerry:** ""I think just the ability of LLMs to generate believable outputs and for text and also for images. And I think just the whole reason I started hacking around with LLMs, honestly, I felt like I got into it pretty late. I should've gotten into it like early 2022 because UB23 had been out for a while. Like just the fact that there was this engine that was capable of like reasoning and no one was really like tapping into it.""

**[01:06:24] Jerry:** ""I'd probably work on some aspect of, um, like personalization of memory. Like, I think I actually think that I don't think anyone's like, I think a lot of people have thoughts about that, but like, for what it's worth, I don't think the final state will be right. I think it will be some, some like fancy algorithm or architecture where you like bake it into like the architecture of the model itself.""

---

These quotes shine a light on the shifting landscape of AI, embedding system design, and the fascinating future possibilities in AI, especially around the personalization of memory and the capabilities of LLMs beyond current utilizations.","1. Models
2. AI Engineer Career
3. Hardware
4. Finetuning
5. Open Source
6. Agents
7. Multimodal
8. Datasets 101
9. Coding Tools
10. UX",Models,AI Engineer Career,Hardware,Finetuning,Open Source,Agents,Multimodal,Datasets 101,Coding Tools,UX,0.022474487099999996,0.0,0.1
Building the Foundation Model Ops Platform — with Raza Habib of Humanloop,https://www.latent.space/p/humanloop,Raza Habib,"Fri, 29 Sep 2023 19:08:10 GMT",4877,https://substackcdn.com/feed/podcast/1084089/post/137474557/632da0adaf3bce3374be477c6978d750.jpg,https://api.substack.com/feed/podcast/137474557/633821313c316a54cafb7a9d0b34ebe2.mp3,"Want to help define the AI Engineer stack? >500 folks have weighed in on the top tools, communities and builders for the first State of AI Engineering survey! Please fill it out (and help us reach 1000!) The AI Engineer Summit schedule is now live! We are running two Summits and judging two Hackathons this Oct. As usual, see our Discord and community page for all events. A rite of passage for every AI Engineer is shipping a quick and easy demo, and then having to cobble together a bunch of solutions for prompt sharing and versioning, running prompt evals and monitoring, storing data and finetuning as their AI apps go  from playground to production . This happens to be Humanloop’s exact pitch. full show notes: https://latent.space/p/humanloop",* [00:01:21] Introducing Raza * [00:10:52] Humanloop Origins * [00:19:25] What is HumanLoop? * [00:20:57] Who is the Buyer of PromptOps? * [00:22:21] HumanLoop Features * [00:22:49] The Three Stages of Prompt Evals * [00:24:34] The Three Types of Human Feedback * [00:27:21] UI vs BI for AI * [00:28:26] LangSmith vs HumanLoop comparisons * [00:31:46] The TAM of PromptOps * [00:32:58] How to Be Early * [00:34:41] 6 Orders of Magnitude * [00:36:09] Becoming an Enterprise Ready AI Infra Startup * [00:40:41] Killer Usecases of AI * [00:43:56] HumanLoop's new Free Tier and Pricing * [00:45:20] Addressing Graduation Risk * [00:48:11] On Company Building * [00:49:58] On Opinionatedness * [00:51:09] HumanLoop Hiring * [00:52:42] How HumanLoop thinks about PMF * [00:55:16] Market: LMOps vs MLOps * [00:57:01] Impact of Multimodal Models * [00:57:58] Prompt Engineering vs AI Engineering * [01:00:11] LLM Cascades and Probabilistic AI Languages * [01:02:02] Prompt Injection and Prompt Security * [01:03:24] Finetuning vs HumanLoop * [01:04:43] Open Standards in LLM Tooling * [01:06:05] Did GPT4 Get Dumber? * [01:07:29] Europe's AI Scene * [01:09:31] Just move to SF (in The Arena) * [01:12:23] Lightning Round - Acceleration * [01:13:48] Continual Learning * [01:15:02] DeepMind Gato Explanation * [01:17:40] Motivations from Academia to Startup * [01:19:52] Lightning Round - The Takeaway   Get full access to Latent Space at  www.latent.space/subscrib,"It seems like there might have been an error in your input, as you've only provided the letter ""e."" For me to summarize a podcast transcript, I would need more specific content or details about the podcast. Could you please provide more information or correct your input?","I'm sorry, but you haven't provided a transcript to analyze for AI research insights. Could you please provide the text or details of the podcast transcript you're referring to?",,"1. News
2. Monthy Recaps
3. Event Recaps
4. AI Engineer Career
5. Models
6. Coding Tools
7. Finetuning
8. Startups
9. Hardware
10. Open Source",News,Monthy Recaps,Event Recaps,AI Engineer Career,Models,Coding Tools,Finetuning,Startups,Hardware,Open Source,-0.1224744871,0.2224744871,0.1
Heralds of the AI Content Flippening — with Youssef Rizk of Wondercraft.ai,https://www.latent.space/p/wondercraft,Youssef Rizk,"Wed, 20 Sep 2023 17:10:26 GMT",3157,https://substackcdn.com/feed/podcast/1084089/post/137135173/643bab496e958fb06bb181cada1ff294.jpg,https://api.substack.com/feed/podcast/137135173/7f2c9b8769129fe1d93715a9da9f73d2.mp3,"Want to help define the AI Engineer stack? Have opinions on the top tools, communities and builders? We’re collaborating with friends at Amplify to launch  the first State of AI Engineering survey ! Please  fill it out  (and tell your friends)! In March, we started off our GPT4 coverage framing one of this year’s key forks in the road as the “ Year of Multimodal vs Multimodel AI ”. 6 months in, neither has panned out yet. The vast majority of LLM usage still defaults to chatbots built atop OpenAI ( per our LangSmith discussion ), and  rumored GPU shortages  have prevented the broader rollout of GPT-4 Vision. Most ""AI media” demos like  AI Drake  and  AI South Park  turned out heavily human engineered, to the point where the AI label is more marketing than honest reflection of value contributed. However, the biggest impact of multimodal AI in our lives this year has been a relatively simple product - the  daily HN Recap podcast  produced by Wondercraft.ai, a 5 month old AI podcasting startup. As swyx  observed , the “ content flippening ” — an event horizon when  the majority of content you choose to consume is primarily AI generated/augmented  rather than primarily human/manually produced — has now gone from  unthinkable  to  possible . For full show notes, go to: https://latent.space/p/wondercraft",* [00:03:15] What is Wondercraft? * [00:08:22] Features of Wondercraft * [00:10:42] Types of Podcasts * [00:11:44] The Importance of Consistency * [00:14:01] Wondercraft House Podcasts * [00:19:27] Video Translation and Dubbing * [00:21:49] Building Wondercraft in 1 Day * [00:24:25] What is your moat? * [00:30:37] Audio Generation stack * [00:32:12] How Important is it to Sound Human? and AI Uncanny Valley * [00:36:02] AI Watermarking * [00:36:32] The Text to Speech Industry * [00:41:19] Voice Synthesis Research * [00:45:53] AI Podcaster interviews Human Podcaster * [00:50:38] Takeaway   Get full access to Latent Space at  www.latent.space/subscrib,"I'm sorry, but you didn't provide the details of the podcast transcript for me to summarize. Could you please share the content or main points of the podcast you'd like summarized?",,,"1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10.",,,,,,,,,,10.,0.0,0.0,0.0
Doing it the Hard Way: Making the AI engine and language 🔥 of the future — with Chris Lattner of Modular,https://www.latent.space/p/modular,Alessio + swyx,"Thu, 14 Sep 2023 16:12:13 GMT",5362,https://substackcdn.com/feed/podcast/1084089/post/136992893/e78ed33a0e2718c63489b5832018814d.jpg,https://api.substack.com/feed/podcast/136992893/0a9c32bc88c77dac5d97cd679b2e6b6e.mp3,"Want to help define the AI Engineer stack? Have opinions on the top tools, communities and builders? We’re collaborating with friends at Amplify to launch the first  State of AI Engineering  survey! Please  fill it out  (and tell your friends)! If AI is so important, why is its software so bad? This was the motivating question for  Chris Lattner  as he reconnected with his product counterpart on Tensorflow,  Tim Davis , and started working on  a modular solution  to the problem of sprawling, monolithic, fragmented platforms in AI development. They  announced a $30m seed  in 2022 and, following their  successful double launch  of Modular/Mojo🔥 in May, have just announced  their $100m Series A . While the performance claims of Mojo🔥 and its promise as a fully multithreaded compiled Python superset stole the show, we were amazed to learn that it is a side project - and the vision for Modular’s Python inference engine is at least as big. Listeners will recall that we last talked with  George Hotz  about his work on tinygrad and how he wants to replace PyTorch with something faster and lighter, handwriting a “reduced instruction set” of operators himself. But what if the problem could be solved at even lower level - with the Python engine/runtime itself? Chris on Compilers Chris’ history with compilers is well known - creating LLVM during his PhD (for which he won the 2012  ACM Software System Award ), hired straight into Apple where he also made Clang and Swift (the iPhone programming language that replaced Objective-C), then leading the Tensorflow Infrastructure team at Google where he built  XLA , a just-in-time compiler for optimizing a lot of the algebra behind TF’s workloads, and  MLIR , a modular compiler framework that sat above LLVM to optimize ML graphs and kernels that were hard to represent in the LLVM IR.   So as pretty much the best compiler engineer in human history, you’d justifiably assume that Chris is simply choosing to take his compiler approach to Python. And yet  that is not how he thinks about compilers at all .  As he says in our chat, “How do you enable invention?  How do you get more kinds of people that understand different parts of this problem to actually collaborate? And so this is where I see our work on Mojo and on the engine… … I don't have a compiler hammer that I'm running around looking for compiler problems to hit.” Today a small number of people at companies like OpenAI spend a lot of time manually writing CUDA kernels. But an optimizing compiler for AI leads to  compilers as a means to an end for increasing software collaboration , expanding the ability of people with different skillsets and knowledge. “…What is the fundamental purpose of a compiler? Well, it's to make it so that you don't have to know as much about the hardware. You could write everything in very low-level assembly code for every single problem that you have… But what a compiler really does is it allows you to  express things at a higher level of abstraction .” For Chris, compilers are also ways to  properly automate generalized optimizations  that might otherwise be manually coded and brittle abstractions, like operator fusion: “So NVIDIA goes and they build this really cool library called FasterTransformer. The performance point of using it is massive. So a lot of LLM companies and other folks use this thing because they want the performance. …Here's the problem.  If you want to go innovate in transformers, now you're constrained by what FasterTransformer can do , right?  And so, again, you come back to where are compilers useful? They're useful for generalization. If you can get the same quality result or better than FasterTransformer, but with a generalized architecture, well now you can get the best of both worlds, where you have orthogonality and composability, you enable research, you also get better performance.” Done correctly, these operator optimizations being implemented at the compiler level amount to an “AI Engine” that can not only survive, but  enable  major architecture shifts should a  credible alternative LLM architecture  come along someday. Modular — the Unified AI Engine Modular’s original goal was to build the “Unified AI Engine” to speed up AI development and inference - one that doesn’t assume an “AI = GPUs” world that only benefits the “GPU-rich”, but one that  treats AI as “a large-scale, heterogeneous, parallel compute problem” . Modular itself is an engine (separate from Mojo, which we cover below) that can run all other frameworks between 10% to 650% faster on CPUs (with GPU support coming in the fall): At Google, Chris’ job wasn’t to build the best possible compiler for AI. The goal was to build the best compiler  for TPUs , so that all TensorFlow users would have a great Google Cloud experience. Similarly, the PyTorch team at Meta isn’t trying to make AI faster for the world, but mostly for their recommendations and ads systems. Chris and Tim realized that the AI engine and developer experience isn’t a product prioritized by any of the big tech companies (they tried) - so they see Modular as the best way to deliver the AI development platform of the future.  The modularity of Modular shines through in  the hot-swapping Inference Engine demo , which has to be seen to be believed. Mojo 🔥 — Blazing Fast Python The other piece of Modular is  Mojo , a new programming language for AI that is a superset of Python. In some sense it is  “the ultimate yak shave” : We were shocked to learn that Chris and the team didn’t initially set out to create Mojo, but it started life as an internal DSL to make themselves more productive. Mojo adopted Python’s syntax since it’s by far the most used language in machine learning and AI. It also lets them supports all existing PyPi packages, requiring no code changes for developers to go from Python to Mojo. Mojo comes with a lot of different underlying design choices that lead to much better performance: * It’s compiled rather than interpreted like Python * No  GIL  which allows for multi-threading * Better heap representation * Leverages MLIR In the perfect test scenario that leverages all of these improvements, Mojo is up to  ~68,000x faster  than Python 🔥 (fire emoji is a valid file extension for Mojo files, btw!).  Of course, that is just one microbenchmark, but as  Jeremy Howard explains , most Python codebases should run between 10-100x faster simply by moving to Mojo with very minor adjustments.  A  community member port of Llama2 from Python to Mojo  shows it inferencing >100x faster than Python, and 20% faster than the handcoded raw C implementation. The Modular team is embarking in one of the hardest technical challenges we’ve seen a startup tackle, and we can’t wait to see what comes out of it. We had an amazing conversation with Chris diving into all the details, which we hope you enjoy! Show Notes *  Modular AI *  Chris’ personal website *  Scott Forstall *  Bret Victor’s Playgrounds * Karpathy’s Tweets *  Speculative Execution *  Llama memory constraints *  LLVM *  Clang *  Swift *  TensorFlow *  PyTorch *  XLA *  MLIR *  TPUs *  Guido van Rossum","* [00:00:00] Introduction * [00:00:40] Chris's background - LLVM, Clang, Swift * [00:03:01] Chris's experience with Google TPUs and XLA * [00:05:47] The limitations of current frameworks like TensorFlow and PyTorch * [00:08:03] The benefits of using compilers for AI systems * [00:13:14] Enabling more collaboration between researchers through better systems * [00:20:55] Starting with CPU optimization instead of just GPUs * [00:24:36] Design principles and goals behind Modular * [00:32:41] The benefits of starting from a general compiler architecture * [00:35:13] Origins of deciding to create the Mojo language * [00:44:43] Goals for Mojo to become a true Python superset * [00:48:12] Thoughts on tinygrad * [00:52:00] ggml, quantization, etc * [00:57:00] Speculative execution and other gains from making Mojo more parallel * [01:01:50] Future of Mojo’s toolkit * [01:07:00] Why Modular is a company and not a foundation * [01:11:00] Learnings as a first time founder and engineering leader * [01:25:00] Lightning Round","In this episode of the Latent Space Podcast, hosts Alessio and Swyx engage in a fascinating conversation with Chris Lattner, a prominent figure in the fields of software development and AI infrastructure. Chris shares insights from his extensive career, including his early involvement with Apple, where he developed LLVM and later Swift, and his experiences at Tesla, Google, and SiFive. The discussion delves into the challenges and innovations in AI development, highlighting Chris's work on the Modular AI engine and Mojo, a Python superset aimed at unifying and simplifying AI development across various hardware platforms. They explore the implications of compiler technology in AI, the importance of ASIC-friendly designs, and the future directions of AI frameworks. Chris emphasizes the necessity of reducing complexity in AI infrastructure to enable broader innovation and collaboration within the field.","1. AI infrastructure is evolving towards addressing both software and hardware integration issues to enhance efficiency and flexibility in AI applications.
2. Modular, the unified AI engine developed by Chris Lattner's team, aims to solve the fragmentation and complexity in current AI research and development by treating AI as a large-scale, heterogeneous, parallel compute problem.
3. Chris highlights the significance of compilers in AI research, suggesting that they can be more scalable and adaptable than manually written kernels, touching on the benefits of compiler techniques like kernel fusion for reducing memory traffic and enabling new model architectures.
4. Modular's approach includes Mojo, a Python superset, that allows for greater expressiveness and efficiency, particularly for systems programming in AI, with the ultimate goal of simplifying AI model deployment across various hardware platforms.
5. Chris discusses the collaboration and community aspect in AI development, emphasizing the importance of an ecosystem where different experts like researchers, system engineers, and hardware specialists can work together to push forward AI advancements.
6. The AI infrastructure challenge is presented as not solely a technical issue but also a matter of correctly aligning incentives, strategy, and user-centric product development in the fast-evolving AI landscape.","Given the comprehensive insights shared by Chris Lattner, the following quotes encapsulate actionable advice and perspectives that could significantly influence AI engineers' thinking and behavior:

1. **On the importance of compiler technology in AI (00:08:03):** ""And so one of the things I love about the NVIDIA platform in particular is that if you look at CUDA, like many people get grumpy about CUDA for various reasons, but you go all the way back to when AI took off, like deep learning took off with the AlexNet moment, for example, right? So many people will credit the AlexNet moment as being a combination of two things. They say it's data, ImageNet, and compute, the power of the GPUs coming together. And that's what allowed the AlexNet moment to happen. But the thing they often forget is that the third part was programmability because CUDA enabled researchers to go invent convolution kernels that did not exist, right? There was no TensorFlow back then. There was none of the stuff that existed. And so it's actually this triumvirate between data compute and programmability that enabled a novel kind of research to kick off this invention that became the entire wave of deep learning systems, right?""

2. **On simplifying AI technology stacks (00:13:14):** ""That's a lot of what we're going to talk about today with Modular Mojo. Well, so that's the brief history and you started Modular in 2022, about 20 months ago. What's one other thing on the personal side that people should know about you that people don't see on the LinkedIn because you're all into hardware-software boundaries and stuff?""

3. **On enabling research through improved AI infrastructure (00:17:08):** ""So I believe in properly modular, properly layered, properly designed technology. And so if you get the abstractions right, you can do really cool things like this.""

4. **On the evolution of AI frameworks (01:21:09):** ""Another third is the frameworks. So you've got TensorFlow, you got PyTorch, you got TVM, you got like all this stuff out there. All these things were, you know, they're eight years old. The infrastructure itself was research, right? These things were built in a different era of what ML was, and they got evolved along the way and new hardware and new use cases and all this stuff. And they were never intentionally designed by, you know, from what we know now.""

These quotes underscore the imperative of harnessing compiler advancements for AI development, taking first-principles approaches to tackle AI's architectural challenges, the invaluable role of programmability in fostering AI breakthroughs, and the urgency of rethinking and evolving AI frameworks to align with contemporary AI challenges and opportunities.","1. AI Engineer Career
2. Hardware
3. Coding Tools
4. Open Source
5. Models
6. LLM Tooling
7. FlashAttention
8. Finetuning
9. Multimodal
10. Agents",AI Engineer Career,Hardware,Coding Tools,Open Source,Models,LLM Tooling,FlashAttention,Finetuning,Multimodal,Agents,0.3449489742,0.2224744871,0.0
The Point of LangChain — with Harrison Chase of LangChain,https://www.latent.space/p/langchain,Harrison Chase,"Wed, 06 Sep 2023 14:59:00 GMT",3650,https://substackcdn.com/feed/podcast/1084089/post/136578557/3d13933542645f46d1b14522bb5b408e.jpg,https://api.substack.com/feed/podcast/136578557/50242c665a016dcc850f0f26b7cc1442.mp3,"As alluded to on the pod, LangChain has  just launched LangChain Hub : “the go-to place for developers to discover new use cases and polished prompts.” It’s available to everyone with a LangSmith account, no invite code necessary.  Check it out ! In 2023, LangChain has speedrun the race from 2:00 to 4:00 to 7:00  Silicon Valley Time . From the back to back  $10m Benchmark seed  and (rumored)  $20-25m Sequoia Series A  in April, to back to back critiques of “ LangChain is Pointless ” and “ The Problem with LangChain ” in July, to  teaching with Andrew Ng  and keynoting at basically every AI conference this fall (including  ours ), it has been an  extreme  rollercoaster for Harrison and his  growing  team creating one of the most popular (>60k stars at time of writing) building blocks for AI Engineers. LangChain’s Origins The  first commit to LangChain  shows its humble origins as a light wrapper around Python’s formatter.format for  prompt templating . But as Harrison tells the story, even his first experience with text-davinci-002 in early 2022 was focused on chatting with data from their internal company Notion and Slack, what is now known as  Retrieval Augmented Generation (RAG) .  As the Generative AI meetup scene came to life post Stable Diffusion, Harrison saw a need for common abstractions for what people were building with text LLMs at the time: *  LLM Math , aka Riley Goodside’s “ You Can’t Do Math ” REPL-in-the-loop ( PR #8 ) *  Self-Ask With Search , Ofir Press’ agent pattern ( PR #9 ) (later  ReAct, PR #24 ) *  NatBot , Nat Friedman’s browser controlling agent ( PR #18 ) * Adapters for  OpenAI, Cohere, and HuggingFaceHub All this was built and launched in a few days from Oct 16-25, 2022.  Turning research ideas/exciting usecases into software quickly and often  has been in the LangChain DNA from Day 1 and likely a big driver of LangChain’s success, to date amassing the largest community of AI Engineers and being the default launch framework for every big name from  Nvidia  to  OpenAI : Dancing with Giants But AI Engineering is built atop of constantly moving tectonic shifts:  *  ChatGPT  launched in November (“ The Day the AGI Was Born ”) and the  API  released in March. Before the ChatGPT API, OpenAI did not have a chat endpoint. In order to build a chatbot with history, you had to make sure to chain all messages and prompt for completion. LangChain made it easy to do that out of the box, which was a huge driver of usage.  * Today, OpenAI has gone all-in on the chat API and is  deprecating the old completions models , essentially baking in the chat pattern as the default way most engineers should interact with LLMs… and reducing (but not eliminating) the value of  ConversationChains . * And there have been more updates since:  Plugins  released in API form as  Functions  in June ( one of our top pods ever … reducing but not eliminating the value of  OutputParsers ) and  Finetuning   in August   (arguably reducing some need for  Retrieval  and  Prompt tooling ).  With each update, OpenAI and other frontier model labs realign the roadmaps of this nascent industry, and Harrison credits the modular design of LangChain in staying relevant. LangChain has not been merely responsive either: LangChain  added Agents  in November, well before they became  the hottest topic of the AI Summer , and now  Agents  feature as one of LangChain’s top two usecases.  LangChain’s problem for podcasters and newcomers alike is its sheer scope - it is the world’s most complete AI framework, but it also has a sprawling surface area that is difficult to fully grasp or document in one sitting. This means it’s time for the trademark Latent Space move ( ChatGPT ,  GPT4 ,  Auto-GPT , and Code Interpreter Advanced Data Analysis  GPT4.5 ):  the executive summary ! What is   LangChain? As Harrison explains,  LangChain is an open source framework for building  context-aware  reasoning applications,  available in Python and JS/TS. It launched in Oct 2022 with the central value proposition of “ composability ”, aka the idea that every AI engineer will want to switch LLMs, and combine LLMs with other things into “ chains ”, using a flexible  interface  that can be saved via a  schema . Today, LangChain’s principal offerings can be grouped as: *  Components : isolated modules/abstractions *  Model I/O *  Models  (for LLM/Chat/Embeddings, from OpenAI, Anthropic, Cohere, etc) *  Prompts  (Templates, ExampleSelectors, OutputParsers) *  Retrieval  ( revised and reintroduced in March ) *  Document Loaders  (eg from CSV, JSON, Markdown, PDF) *  Text Splitters  (15+ various strategies for chunking text to fit token limits) *  Retrievers  (generic interface for turning an unstructed query into a set of documents - for  self-querying ,  contextual compression ,  ensembling ) *  Vector Stores  ( retrievers  that search by similarity of embeddings) *  Indexers  (sync documents from any source into a  vector store  without duplication) *  Memory  (for long running chats, whether a simple  Buffer, Knowledge Graph, Summary, or Vector Store ) *  Use-Cases:  compositions of Components *  Chains : combining a PromptTemplate, LLM Model and optional OutputParser * with  Router, Sequential, and Transform Chains  for advanced usecases * savable, sharable schemas that can be  loaded from LangChainHub *  Agents : a chain that has access to a suite of tools, of nondeterministic length because the LLM is used as a reasoning engine to determine which actions to take and in which order. Notable  100LOC explainer here . *  Tools  (interfaces that an agent can use to interact with the world -  preset list here . Includes things like ChatGPT plugins, Google Search, WolframAlpha. Groups of tools are bundled up as  toolkits ) *  AgentExecutor  (the agent runtime, basically the while loop, with support for  controls ,  timeouts ,  memory sharing , etc) * LangChain has also added a  Callbacks  system for instrumenting each stage of LLM, Chain, and Agent calls (which enables  LangSmith , LangChain’s first cloud product), and most recently an  Expression Language , a declarative way to compose chains. LangChain the company incorporated in January 2023,  announced their seed round in April , and  launched LangSmith in July . At time of writing, the company has  93k followers ,  their Discord  has 31k members and their  weekly webinars  are attended by thousands of people live. The full-featuredness of LangChain means it is often  the first starting point for building any mainstream LLM use case , because they are most likely to have working guides for the new developer. Logan ( our first guest !) from OpenAI has been a notable fan of both  LangChain  and  LangSmith  (they will be running the first LangChain + OpenAI workshop at AI Eng Summit).  However, LangChain is not without its critics, with  Aravind Srinivas ,  Jim Fan ,  Max Woolf ,  Mckay Wrigley  and the  general Reddit/HN community  describing  frustrations with the value of their abstractions , and many are attempting to write their own (the common experience of adding and then removing LangChain is something we covered in  our Agents writeup ). Harrison compares this with the timeless ORM debate on the value of abstractions. LangSmith Last month, Harrison launched  LangSmith , their LLM observability tool and first cloud product. LangSmith makes it easy to monitor all the different primitives that LangChain offers (agents, chains, LLMs) as well as making it easy to  share  and evaluate them both through heuristics (i.e. manually written ones) and “LLM evaluating LLM” flows.  The  top HN comment  in the “LangChain is Pointless” thread observed that orchestration is the smallest part of the work, and the bulk of it is prompt tuning and data serialization. When asked this directly our pod, Harrison agreed: “I agree that those are big pain points that get exacerbated when you have these complex chains and agents where you can't really see what's going on inside of them. And I think that's partially why we built Langsmith…”  (48min mark) You can watch the full launch on the LangChain YouTube: It’s clear that the target audience for LangChain is expanding to folks who are building complex, production applications rather than focusing on the simpler “Q&A your docs” use cases that made it popular in the first place. As the AI Engineer space matures, there will be more and more tools graduating from supporting “hobby” projects to more enterprise-y use cases.  In this episode we run through some of the history of LangChain, how it’s growing from an open source project to one of the highest valued AI startups out there, and its future. We hope you enjoy it! Show Notes *  LangChain *  LangChain’s Berkshire Hathaway Homepage *  Abstractions tweet *  LangSmith *  LangSmith Cookbooks  repo *  LangChain Retrieval  blog *  Evaluating CSV Question/Answering  blog and  YouTube *  MultiOn Partner  blog *  Harvard Sports Analytics Collective *  Evaluating RAG Webinar * awesome-langchain: *  LLM Math Chain *  Self-Ask *  LangChain Hub UI *  “LangChain is Pointless” * Harrison’s links *  sports - estimating player compatibility in the NBA *  early interest in prompt injections *  GitHub *  Twitter","* [00:00:00] Introduction * [00:00:48] Harrison's background and how sports led him into ML * [00:04:54] The inspiration for creating LangChain - abstracting common patterns seen in other GPT-3 projects * [00:05:51] Overview of LangChain - a framework for building context-aware reasoning applications * [00:10:09] Components of LangChain - modules, chains, agents, etc. * [00:14:39] Underappreciated parts of LangChain - text splitters, retrieval algorithms like self-query * [00:18:46] Hiring at LangChain * [00:20:27] Designing the LangChain architecture - balancing flexibility and structure * [00:24:09] The difference between chains and agents in LangChain * [00:25:08] Prompt engineering and LangChain * [00:26:16] Announcing LangSmith * [00:30:50] Writing custom evaluators in LangSmith * [00:33:19] Reducing hallucinations - fixing retrieval vs generation issues * [00:38:17] The challenges of long context windows * [00:40:01] LangChain's multi-programming language strategy * [00:45:55] Most popular LangChain blog posts - deep dives into specific topics * [00:50:25] Responding to LangChain criticisms * [00:54:11] Harrison's advice to AI engineers * [00:55:43] Lightning Round","The podcast features Alessio, a partner and CTO at Residence at Decibel Partners, and Swyx, founder of Smol.ai, as they interview Harrison Chase. Harrison, who has a background in Stats and CS from Harvard, delved into machine learning through sports analytics and eventually founded LangChain.

LangChain, initiated around November 2022, aims at creating context-aware reasoning applications by integrating various components and modules like prompt templates, language models, vector stores, and more. It was designed to cater to the evolving landscape of AI and generative models, providing developers a framework to build LLM applications effectively. One significant aspect of LangChain is its emphasis on being a 'Switzerland' in developer land, supporting multiple vector databases and AI models to prevent developers from getting locked into a single provider.

Additionally, Harrison discusses the concept of agents within LangChain, differentiating them from chains by their ability to decide the order of operations, thereby handling long-tail events efficiently. This approach facilitates more complex, context-aware applications.

LangSmith, another project by Harrison, serves as a control center for LLM applications, focusing on debugging, monitoring, logging, and evaluating these applications to enhance their performance and reliability.

Moreover, Harrison emphasizes the importance of building and experimenting in the rapidly evolving AI space, suggesting that engaging with cutting-edge technologies and concepts can lead to significant innovations or, at the very least, a deeper understanding of the field.","1. The importance of sports analytics as an entryway to data science and machine learning, offering quick feedback and real-world application.
2. AI applications in sports betting and financial predictions.
3. Harrison Chase's GPT aha moment stemmed from a hackathon project involving a bot for querying Notion and Slack, using GPT-3.
4. LangChain's conceptualization was prompted by identifying common abstractions in generative AI space during the period between Stable Diffusion and ChatGPT.
5. LangChain operates as a framework for building context-aware reasoning applications, streamlining the development process by providing components and modules for specific AI functionalities.
6. The evolution of LangChain included improving developer experience with OpenAI's chat API and incorporating feedback for enhancing LangChain's utility.
7. The LangChain Expression Language was introduced to allow for composability and flexibility in building AI applications.
8. Challenges in AI development include balancing complexity with user-friendliness, addressing hallucination (inaccurate AI outputs), and ensuring accurate memory in long conversations.
9. The potential shift in AI development practices with the advent of local models and multimodal AI capabilities.
10. Continuous feedback and community engagement significantly guide LangChain's development focus and feature prioritization.","Given the extensive conversation, here are several actionable quotes that may impact the thinking or behavior of an AI engineer:

1. ""I think sports is a great entryway for kind of like data science."" - [00:02:43] 
   This quote emphasizes the practical applications of sports analytics as a gateway to data science and machine learning, encouraging AI engineers to consider unconventional pathways into their field.

2. ""LangChain's a framework for building context-aware reasoning applications."" - [00:08:14] 
   This statement underlines the core purpose of LangChain, suggesting AI engineers focus on developing applications that are not just intelligent but contextually aware, potentially guiding their approach in creating more nuanced AI solutions.

3. ""I think accuracy is definitely the top that we're seeing right now."" - [00:31:13] 
   Highlighting accuracy as a primary concern in the AI field, this quote may drive AI engineers to prioritize the accuracy of their models and applications, ensuring that they deliver reliable and trustworthy AI services.

4. ""Build as many things as possible."" - [00:54:11] 
   This advice encourages AI engineers to engage actively in building and experimenting with AI technologies, emphasizing the importance of hands-on experience and exploration in driving innovation and understanding in the rapidly evolving field of AI.

5. ""Just build."" - [01:00:49] 
   A succinct call to action for AI engineers, this quote reinforces the message that the best way to progress and innovate within AI is by actively engaging in the creation and development of new technologies and applications.

These quotes collectively emphasize the importance of practical experience, innovative application development, accuracy in AI solutions, and the potential of unconventional entry points into data science and AI. They could guide AI engineers in their approach to learning, prioritization, and technology development strategies.","1. Models
2. Startups
3. AI Engineer Career
4. Open Source
5. Hardware
6. Coding Tools
7. Finetuning
8. Multimodal
9. Agents
10. News",Models,Startups,AI Engineer Career,Open Source,Hardware,Coding Tools,Finetuning,Multimodal,Agents,News,0.0,0.4449489742,0.2
RWKV: Reinventing RNNs for the Transformer Era — with Eugene Cheah of UIlicious,https://www.latent.space/p/rwkv,Eugene Cheah,"Wed, 30 Aug 2023 16:04:19 GMT",4331,https://substackcdn.com/feed/podcast/1084089/post/136547662/b24ed5210b5655ec94d41e3df0fa6e7d.jpg,https://api.substack.com/feed/podcast/136547662/4b893a10da528ef7318f1037387882c7.mp3,"The AI Engineer Summit Expo has been  announced , presented by AutoGPT (and future guest  Toran Bruce-Richards !) Stay tuned for more updates on  the Summit livestream  and  Latent Space University . This post was on  HN for 10 hours . What comes after the Transformer?  This is one of the  Top 10 Open Challenges in LLM Research  that has been the talk of the AI community this month. Jon Frankle ( friend of the show !) has an ongoing  bet  with Sasha Rush on whether  Attention is All You Need , and the most significant challenger to emerge this year has been  RWKV - Receptance Weighted Key Value models , which revive the RNN for GPT-class LLMs, inspired by a 2021 paper on  Attention Free Transformers  from Apple (surprise!). What this means practically is that RWKV models tend to scale in all directions (both in training and inference) much better than Transformers-based open source models: While remaining competitive on standard reasoning benchmarks: swyx was recently in Singapore for meetings with  AI government and industry folks , and grabbed 2 hours with RWKV committee member Eugene Cheah for a deep dive, the full recording of which is now up on  Latent Space TV : Today we release both the 2hr video and an edited 1hr audio version, to cater to the different audiences and provide “ablation opportunities” on RWKV interest level. The Eleuther Mafia? The RWKV project is notable not merely because of the credible challenge to the Transformers dominance. It is also a distributed, international, mostly uncredentialed community reminiscent of early 2020s Eleuther AI: * Primarily Discord, pseudonymous,  GPU-poor  volunteer community somehow coordinating enough to train >10B, OPT/BLOOM-competitive models * Being driven by the needs of its community, it is extremely polyglot (e.g. English, Chinese, Japanese, Arabic) not because it needs to beat some benchmarks, but because its users want it to be for their own needs. * “Open Source” in both the good and the bad way - properly Apache 2.0 licensed ( not “open but restricted ”), yet trained on data taken from commercially compromised sources like the Pile (where  Shawn Presser’s Books3 dataset has been recently taken down ) and Alpaca (taking from  Steven Tey’s ShareGPT  which is technically against OpenAI TOS) The threadboi class has loved tracking the diffusion of Transformers paper authors out into the industry: But perhaps the underdog version of this is tracking the emerging Eleuther AI mafia: It will be fascinating to see how both Eleuther and Eleuther alums fare as they build out the future of both LLMs and open source AI. Audio Version","assisted by  smol-podcaster . Different timestamps vs the 2hr YouTube * [00:05:35] Eugene's path into AI at UIlicious * [00:07:33] Tokenizer penalty and data efficiency of Transformers * [00:08:02] Using Salesforce CodeGen * [00:10:17] The limitations of Transformers for handling large context sizes * [00:13:17] RWKV compute costs compared to Transformers * [00:16:06] How Eugene found RWKV early * [00:18:52] RWKV's focus on supporting many languages, not just English * [00:21:24] Using the RWKV model for fine-tuning for specific languages * [00:24:45] What is RWKV? * [00:33:46] Overview of the different RWKV models like World, Raven, Novel * [00:41:34] Background of Blink, the creator of RWKV * [00:49:55] The linear vs quadratic scaling of RWKV vs Transformers * [00:53:29] RWKV matching Transformer performance on reasoning tasks * [00:54:31] The community's lack of marketing for RWKV * [00:57:00] The English-language bias in AI models * [01:00:33] Plans to improve RWKV's memory and context handling * [01:03:10] Advice for AI engineers wanting to get more technical knowledge Show Notes Companies/Organizations: * RWKV -  HF blog ,  paper ,  docs ,  GitHub ,  Huggingface * Raven 14B (finetuned on Alpaca+ShareGPT+...)  Demo * World 7B (supports 100+ world languages)  Demo *  How RWKV works in 100 LOC ,  RWKV overview *  EleutherAI  - Decentralized open source AI research group *  Stability AI  - Creators of Stable Diffusion  *  Conjecture  - Spun off from EleutherAI People: * Eugene Chia - CTO of  UIlicious , member of RWKV committee ( GitHub ,  Twitter ) *  Blink/Bo Peng  - Creator of RWKV architecture *  Quentin Anthony  - our Latent Space pod on Eleuther, coauthor on RWKV  *  Sharif Shameem  - our Latent Space pod on being early to Stable Diffusion *  Tri Dao  - our Latent Space pod on FlashAttention making Attention subquadratic *  Linus Lee  - our Latent Space pod in NYC *  Jonathan Frankle  - our Latent Space pod about Transformers longevity *  Chris Re  - Genius at Stanford working on state-space models * Andrej Karpathy -  Zero to Hero  series * Justine Tunney ("" Justine.lol "") -  mmap trick Models/Papers: *  Top 10 Open Challenges in LLM Research *  Retentive Network: A Successor to Transformer for Large Language Models   *  GPT-NeoX  - Open source replica of GPT-3 by EleutherAI  *  Salesforce CodeGen  and  CodeGen 2 *  Attention Free Transformers paper *  The Pile *  RedPajama dataset *  Monarch Mixer -  Revisiting BERT, Without Attention or MLPs Misc Notes RWKV is not without known weaknesses - Transformers do well in reasoning because they are  expressive in the forward pass , yet the RWKV docs already note that it is  sensitive to prompt formatting and poor at lookback tasks . We also asked pointed questions about RWKV’s challenges in the full podcast.   Get full access to Latent Space at  www.latent.space/subscrib","I'm sorry, but you haven't provided a specific podcast transcript for me to summarize. Could you please provide more details or the text of the podcast transcript you would like summarized?",,,"1. Models
2. Transformers Math
3. Multimodal
4. LLM Tooling
5. Finetuning
6. RHLF 201
7. Coding Tools
8. AI Engineer Career
9. Hardware
10. Open Source",Models,Transformers Math,Multimodal,LLM Tooling,Finetuning,RHLF 201,Coding Tools,AI Engineer Career,Hardware,Open Source,0.3449489742,0.2224744871,0.0
Cursor.so: The AI-first Code Editor — with Aman Sanger of Anysphere,https://www.latent.space/p/cursor,Aman Sanger,"Tue, 22 Aug 2023 15:55:11 GMT",3565,https://substackcdn.com/feed/podcast/1084089/post/136284642/cff6429a1776792fe27fcc7b886f18b4.jpg,https://api.substack.com/feed/podcast/136284642/9d43e0e0ee1703ab27486797a8b1ffaa.mp3,"Thanks to the almost 30k people  who tuned in to  the last episode ! Your podcast cohosts have been busy shipping: *  Alessio open sourced  smol-podcaster , which makes the show notes here!  *  swyx  launched   GodMode . Maybe someday the Cursor of browsers? *  We’re also helping organize a  Llama Finetuning Hackameetup  this Saturday in anticipation of the  CodeLlama  release.  Lastly, more speakers were  announced  at  AI Engineer Summit ! 👀 ~46%  of code typed through VS Code is written by Copilot.  How do we get closer to 90+%?  Aman Sanger  says we need a brand new  AI-powered IDE  to get there; and we’re excited to be the first podcast  ever  to tell the  Cursor  story. If you haven’t heard of Cursor, you may have been living under a rock. Here are just some of the rave reviews going around in  the past week alone : * “Cursor is the best product I've used in a while” -  Alex MacCaw * “Someone finally put GPT into a code editor in a seamless way. It's so elegant and easy. No more copying and pasting.” -  Andrew McCalip * “Coding with AI is getting insane.” -  Mckay Wrigley * “This is mind blowing 🤯” -  Linus Ekenstam * “Cursor + gpt4-32k = illegal levels of productivity” -  Sully Omarr * “EL MEJOR EDITOR DE CÓDIGO con IA” -  Carlos Santana A decade ago, “platform risk” meant building apps on social media platforms was risky as you could get cut off from the social network.  Today, the AI version of “platform risk” is building AI products within an existing product (like an AI extension for VS Code, or a Figma plugin). Since Copilot, a generation of VSCode plugins have launched (including  Cody ,  Cosine , and previous guests  Codeium  and  Codium ), only to be challenged by  Copilot X  itself. A core AI Engineering thesis is that  new capabilities in AI demands  new innovation in AI UX  (and that  AI UX can actually be a viable moat ) .  Take VS Code for example; when Github was first working on Copilot, there was actually no way to support the “ghost autocomplete” feature we all use today. They eventually convinced the team to build it, and Copilot’s success speaks for itself. If you’re a startup building on top of VSC today, you do not have the same access and influence on the roadmap.  Your UX is limited to what they allow you to do, and often that caps your ability to successfully compete against them.   Since Cursor owns the whole IDE, they can do things you can’t (yet) do in VSCode: Cursor’s Gameplan Cursor is competing head to head against VS Code by forking Microsoft’s IDE and building their own AI-powered version. A few of Cursor’s unique features: *  Native chat:  Chat is a core piece of Cursor. Users can choose between GPT-3.5 and GPT-4 to ask questions and receive answers based on their code. *  “Mentioning” files:  you can easily add files into your request context by using “@”; this works both for code as well as documentation. If you want to do a change that includes multiple files, you can include them in your question to make sure the change is reflected in all of them. *  Custom prompting engine:  Cursor built  Priompt , their custom prompting engine. As your chats go over the context window size, Priompt figures out which messages to keep in the history, which files to drop from the prompt, etc.  *  Moving beyond typing:  while IDEs are familiar to folks as today’s interfaces, in the future Cursor hopes to have agents you can delegate tasks to. Instead of a back and forth on a new feature or bug fix, you can ask it to do the whole thing for you end to end. After diving deep into Cursor we nerded out on model usage, training, quantization, and evaluation. There’s a ton of great content in this episode, we hope you’ll enjoy it! As always, feedback welcome in the comments, and tag us on socials for future guest suggestions! Show Notes *  Cursor *  Gary Marcus’ cubes prompt *  Priompt *  “Humans should focus on bigger problems.” *  Codium AI on Latent Space *  Rift from Morph *  Sourcegraph *  E2B *  Repl.it *  HungryHungryHippos, Hyena, etc (see our FlashAttention episode) * Aman Tweets *  Why GPT-3.5 is (mostly) cheaper than Llama 2 *  Llama’s architectural limitations * “Training will look like researchers/practitioners offloading large-scale training jobs to specialized “training” companies: a state of the world that resembles chip design & fabrication.” -  Mosaic prediction * “The size of all code/history on Github public repos is 92TB. The size of Google's monorepo in 2015 was 86TB (of much higher quality code). If Google were willing to deploy code models trained on their own data, they'd have a noticable advantage over everyone else.” -  May 2023",* [00:00:00] Intros * [00:02:31] Developing CAD models vs coding models * [00:05:23] Deciding to build a new IDE optimized for large language models * [00:10:50] Getting early access to GPT-4 and realizing its potential for software development * [00:12:32] Rethinking the UI/UX for coding * [00:18:24] Cursor's features like system prompts and chat * [00:22:24] Tips for prompting GPT-3/4 for code generation and editing * [00:27:24] Cursor's documentation and context features * [00:29:30] The potential of coding agents like Code Interpreter * [00:38:23] Cursor's internal prompting tool Priompt * [00:40:47] The challenges of very long context lengths for models * [00:45:44] The compute costs for prompt tokens vs. completion tokens * [00:49:36] How quantization interacts with model utilization * [00:51:24] Issues with human eval for benchmarking code models * [00:53:12] Thoughts on training models vs. relying on foundation models from big providers * [00:55:34] The origin story of Cursor's parent company AnySphere * [00:56:00] Lightning Round,"In this episode of the Latent Space podcast, hosts Alessio and Swyx welcome guest Aman Sanger. Sanger shares his journey through the tech and AI industry, having worked at companies like Gamelon, Bridgewater, McKinsey, Google, and You.com, and founding his consultancy, Abelian AI. He highlights his academic background in computer science and math from MIT and discusses his involvement in projects such as Instill and his latest venture, Cursor.so.

A personal tidbit Sanger shares is his passion for squash, a sport he played competitively in high school. This competitive nature, he notes, translates into his work ethic and approach to his startups, emphasizing a strong dislike for losing.

The conversation then delves into Sanger's decision to launch Instill and the subsequent focus shift to Cursor.so, exploring its development from his fascination with AI and collaborative experiences at MIT. He details the technical challenges and breakthroughs in AI and CAD (Computer-Aided Design) software, underscoring the limitations they encountered with model training and inference and the eventual realization that a new approach to CAD and software development was necessary.

Sanger elaborates on the technological advancements and the potential of AI in optimizing mechanical engineering design processes, his pragmatic take on competition, and the importance of being a user of the product you're building. This led to the pivot towards developing Cursor.so, a product propelled by the capabilities of GPT-4 predicting a future where software development would undergo a significant transformation tailored by AI advancements.

The discussion wraps up with insights into Cursor.so's features, including code editing, documentation integration, and the development team's future endeavors to enhance AI-driven coding tools. Sanger's perspective on the evolving landscape of AI in software development highlights the significance of continuous innovation and the impact of competitive and user-centric approaches in the tech industry.","1. Development of a transformer model trained for CAD (computer-aided design) data to aid mechanical engineers by predicting next steps in part design, but encountered limitations due to insufficient data volume and poor spatial reasoning capabilities of current models.
2. Observation that large-scale text models like GBD4 show advantages in software development tasks, including code refactoring and debugging, indicating a future where the majority of coding could be automated.
3. Introduction of Cursor, a product offering a novel IDE experience to support software development with advanced AI integrations, emphasizing the necessity of redesigning the software development UX to leverage evolving large language models.
4. Early access to GPT-4 highlighted its superior performance in code-related tasks compared to its predecessors and open-source models, suggesting a significant leap in AI's capability to understand and generate code.
5. Cursor's approach to enhancing the coding experience includes an internal tool called Priompt for advanced prompt construction and the possibility of integrating fine-tuned models for specific tasks or domains.
6. AnySphere, the company behind Cursor, aims to reshape software development by enabling more intuitive interactions with AI for coding tasks, suggesting a future where AI plays a central role in software creation and maintenance.","Below are selected actionable quotes from Aman Sanger that might impact the thinking or behavior of an AI Engineer:

1. **On competitive nature in startups** (timecode [00:02:17]):
   - ""I'm very competitive. I really hate losing...the competitive nature, I think it in general helps motivate me and makes me, I guess, just work harder.""
   
2. **On moving off a project due to technical challenges** (timecode [00:07:52]):
   - ""If you try to test these models today...it's clear that these models are not that good at spatial reasoning, and that's exactly what's needed for CAD.""

3. **On the future of AI-powered CAD software** (timecode [00:08:47]):
   - ""I think the best way here is probably redesigning the entire system...it feels like with the advancements you have in kind of text to images, and there's some new companies kind of doing stuff with text to 3D, it feels like the reasonable approach is actually just scrap the way that people are doing CAD right now.""

4. **On the insight gained from early access to GPT-4** (timecode [00:10:54]):
   - ""And before everyone started talking about it and knowing about it, we kind of pasted it in and it got 85%. And we were just like, wow, best open source model at the time got 30%...These models are just going to get better and better and better at programming.""

5. **On owning the full IDE for developing AI tools** (timecode [00:12:32]):
   - ""You need to own the full IDE and completely redesign the flow of producing software and just doing software development in general.""

6. **On AI model training observations** (timecode [00:14:12]):
   - ""I think I have some thoughts on this because there's the whole thing with chinchilla scaling and then people are now saying, oh, chinchilla scaling doesn't matter because of inference.""

7. **On the importance of direct involvement in the products you develop** (timecode [00:19:17]):
   - ""We learned this from CAD, and we strongly believe this now that it is much better to be a user of the product you're building.""

8. **On the future of coding tools and IDEs** (timecode [00:27:36]):
   - ""So one thing that our users have found quite helpful is being able to add files or add documentation...it's stored locally for you.""

9. **On the approach towards developing better AI tools for coding** (timecode [00:29:30]):
    - ""I think it's pretty important to kind of have the agent do these very small, discrete units and then show the output in a way that's very easy for the user to understand and then go in and fix.""

These quotes can guide AI engineers both in their technical approaches and their strategic directions in building the next generation of AI tools and platforms.","1. Models
2. Coding Tools
3. AI Engineer Career
4. Startups
5. Open Source
6. LLM Tooling
7. Hardware
8. Finetuning
9. Multimodal
10. Agents",Models,Coding Tools,AI Engineer Career,Startups,Open Source,LLM Tooling,Hardware,Finetuning,Multimodal,Agents,0.1224744871,0.2224744871,0.1
The Mathematics of Training LLMs — with Quentin Anthony of Eleuther AI,https://www.latent.space/p/transformers-math,Quentin Anthony,"Wed, 16 Aug 2023 16:52:03 GMT",3038,https://substackcdn.com/feed/podcast/1084089/post/136047711/0cd65e6dfb733f88fbff8825effe9974.jpg,https://api.substack.com/feed/podcast/136047711/48d981d835799b970867b03c1ec3ff2a.mp3,"Invites are going out for  AI Engineer Summit ! In the meantime, we have just announced our first  Actually Open AI event  with Brev.dev and Langchain, Aug 26 in our SF HQ (we’ll record talks for those remote). See you soon (and join the Discord)! Special thanks to  @nearcyan  for helping us arrange this with the Eleuther team. This post was on the  HN frontpage  for 15 hours. As startups and even VCs  hoard GPUs  to attract talent,  the one thing more valuable than GPUs is knowing how to use them  (aka,  make  GPUs go brrrr ). There is an incredible amount of  tacit knowledge  in the NLP community around training, and until Eleuther.ai came along you pretty much had to work at Google or Meta to gain that knowledge. This makes it hard for non-insiders to even do simple estimations around costing out projects - it is well known how to trade $ for GPU hours, but trading “$ for size of model” or “$ for quality of model” is less known and more valuable and full of opaque “it depends”. This is why  rules of thumb for training  are incredibly useful, because they cut through the noise and give you the simple 20% of knowledge that determines 80% of the outcome derived from hard earned experience. Today’s guest, Quentin Anthony from EleutherAI, is one of the top researchers in  high-performance deep learning . He’s one of the co-authors of  Transformers Math 101 , which was one of the clearest articulations of training rules of thumb. We can think of no better way to dive into training math than to have Quentin run us through a masterclass on model weights, optimizer states, gradients, activations, and how they all impact memory requirements. The core equation you will need to know is the following: Where  C  is the compute requirements to train a model,  P  is the number of parameters, and  D  is the size of the training dataset in tokens. This is also equal to  τ , the throughput of your machine measured in FLOPs (Actual FLOPs/GPU * # of GPUs), multiplied by  T , the amount of time spent training the model. Taking Chinchilla scaling at face value, you can simplify this equation to be `C = 120(P^2)`.These laws are only true when 1000 GPUs for 1 hour costs the same as 1 GPU for 1000 hours, so it’s not always that easy to make these assumptions especially when it comes to communication overhead.   There’s a lot more math to dive into here between training and inference, which you can listen to in the episode or read in the articles.  The other interesting concept we covered is distributed training and strategies such as ZeRO and 3D parallelism. As these models have scaled, it’s become impossible to fit everything in a single GPU for training and inference. We leave these advanced concepts to the end, but there’s a lot of innovation happening around sharding of params, gradients, and optimizer states that you must know is happening in modern LLM training.  If you have questions, you can join the  Eleuther AI Discord  or follow  Quentin on Twitter .  Show Notes *  Transformers Math 101 Article *  Eleuther.ai *  GPT-NeoX 20B *  BLOOM *  Turing NLG *  Mosaic *  Oak Ridge & Frontier Supercomputer *  Summit Supercomputer  *  Lawrence Livermore Lab *  RWKV *  Flash Attention  *  Stas Bekman","* [00:00:00] Quentin's background and work at Eleuther.ai * [00:03:14] Motivation behind writing the Transformers Math 101 article * [00:05:58] Key equation for calculating compute requirements (tau x T = 6 x P x D) * [00:10:00] Difference between theoretical and actual FLOPs * [00:12:42] Applying the equation to estimate compute for GPT-3 training * [00:14:08] Expecting 115+ teraflops/sec per A100 GPU as a baseline * [00:15:10] Tradeoffs between Nvidia and AMD GPUs for training * [00:18:50] Model precision (FP32, FP16, BF16 etc.) and impact on memory * [00:22:00] Benefits of model quantization even with unlimited memory * [00:23:44] KV cache memory overhead during inference * [00:26:08] How optimizer memory usage is calculated * [00:32:03] Components of total training memory (model, optimizer, gradients, activations) * [00:33:47] Activation recomputation to reduce memory overhead * [00:38:25] Sharded optimizers like ZeRO to distribute across GPUs * [00:40:23] Communication operations like scatter and gather in ZeRO * [00:41:33] Advanced 3D parallelism techniques (data, tensor, pipeline) * [00:43:55] Combining 3D parallelism and sharded optimizers * [00:45:43] Challenges with heterogeneous clusters for distribution * [00:47:58] Lightning Round","In the Latent Space podcast episode, hosts Alessio and Swyx discuss Transformers and AI training with special guest Quentin Anthony from Eleuther.ai. Quentin, a PhD student involved with Eleuther during the training of the GPT-NeoX20B model, shares insights on the challenges of scaling AI models, including communication costs and the synchronization needed to make use of hundreds of GPUs efficiently. The discussion explores the motivation behind writing a blog post titled ""Transformers Math 101"" by Eleuther, aiming to clarify the more practical, nitty-gritty details of AI training and inference that are often overlooked in theoretical discussions, through sharing notes and findings within Eleuther’s open research approach.

The podcast delves into various technical aspects of AI training, including the importance of mixed precision training (using combinations of FP16, BF16, and FP32), the memory constraints imposed by model weights, optimizer states, gradients, and activation memory, and strategies to efficiently use GPU resources. Quentin emphasizes that activation memory management during training can sometimes lead to memory issues due to its dynamic nature, and strategies like activation recomputation can help manage memory more effectively.

The conversation also covers distributed training tactics, including 'Zero', which involves sharding optimizer states across GPUs, and '3D Parallelism', a combination of data parallelism, tensor parallelism, and pipeline parallelism to optimize training across multiple GPUs. Quentin points out challenges with adapting models to multimodal parallelism and reinforcement learning due to their unique training and inference requirements. 

Quentin and the hosts also touch on future directions and unsolved problems in AI, noting the importance of research into on-the-fly communication compression and efficient parallelism strategies for multimodal models and reinforcement learning.

Listeners are encouraged to explore the blog post “Transformers Math 101” for a deeper understanding and insights shared during the podcast episode.","1. Importance of understanding the details of AI training and inference for optimal performance on hardware.
2. EleutherAI's commitment to sharing technical knowledge and insights, which is rare in proprietary research environments.
3. The cognitive cost model for training Transformer models is based on the hardware setup throughput and the time spent.
4. The relevance of the token-parameter ratio in dataset for determining the model size, influenced by research like the Chinchilla paper.
5. The actual versus theoretical flops in training AI models, emphasizing the gap due to hardware utilization efficiency.
6. Utilization of cloud instances and computing resources requires considering the cost and time alongside model and hardware specifications.
7. Challenges in scaling AI models across multiple GPUs due to communication costs and synchronization issues.
8. The shift from FP32 to mixed precision (FP16 and BF16) in deep learning to improve performance, and eventual movement towards even lower precision.
9. The necessity of memory-efficient techniques in training, such as optimizer state sharding and activation recomputation to manage large models.
10. Insights into distributed training strategies, including shard optimizers and 3D parallelism, for efficient AI model training.","**[00:08:34] Quentin:** ""I always feel like you should go with the minimum number of GPUs because the more number of GPUs that you have, the more likely it is for things to break. So I would say just find out what time is reasonable for you and then fit the number of GPUs to that and no more. Because people get greedy and they say, if I have twice the GPUs, I can get this done in half the time. And then you end up taking three times the time because everything is breaking every day. And that's when I am up at midnight trying to fix your model that's broken.""

**[00:14:44] Quentin:** ""For a number like that, we basically, we compared a lot of different frameworks ... and we saw across a bunch of different hardware configurations that if you tune things correctly, you should be getting above 115 in pretty much all cases. So like there are some cases where things are tuned poorly or your system is a little weird, but we've never been able to get a new system and not been able to get above 115. If something is below 115, you have something really wrong in your software.""

**[00:28:22] Quentin:** ""So I'm a systems guy. I'm an engineer. And a lot of these things come to me as magic. Adam comes to me as magic. I see it from the gods. I say, this is how a deep learning model is trained. And this is how the next step is calculated. And then I say, okay, how do I make that fast?""

**[00:32:03] Quentin:** ""You don't want to store more than, you know, X-sized tensor. And some fall above that, some fall below it. And you're not trying to squeeze. You care more about getting something close enough to what the actual heuristic should be without actually computing the heuristic because you don't want to spend the time writing that heuristic code.""

**[00:38:38] Quentin:** ""I say 3D parallel gives you the most control and also the most ways to go wrong. And depending on whether you have more engineers or whether you have more GPUs, that should decide which of these you go with.""

**[00:49:26] Quentin:** ""...what I'm getting to is how should you compress your communication before it happens? So on the fly compression, you have some buffer that needs to be communicated. You compress it with a GPU kernel, then you send it across the network and then you decompress it ... Making people spend less money on communication fabrics and more on GPUs as intended is sort of a thing that people need to explore.""","1. Transformers Math
2. Models
3. Hardware
4. Coding Tools
5. Multimodal
6. Finetuning
7. Open Source
8. AI Engineer Career
9. Benchmarks 101
10. Datasets 101",Transformers Math,Models,Hardware,Coding Tools,Multimodal,Finetuning,Open Source,AI Engineer Career,Benchmarks 101,Datasets 101,0.022474487099999996,0.0,0.1
LLMs Everywhere: Running 70B models in browsers and iPhones using MLC — with Tianqi Chen of CMU / OctoML,https://www.latent.space/p/llms-everywhere,Tianqi Chen,"Thu, 10 Aug 2023 16:42:09 GMT",3130,https://substackcdn.com/feed/podcast/1084089/post/135877261/a0c58910d645ac1b8446b37d237638e2.jpg,https://api.substack.com/feed/podcast/135877261/f3024f057f37fcc6256c27afadc02310.mp3,"We have just announced our first set of speakers at  AI Engineer Summit ! Sign up for the livestream or email  sponsors@ai.engineer  if you’d like to support. We are facing a  massive GPU crunch . As both startups and VC’s hoard Nvidia GPUs like countries count nuclear stockpiles,  tweets about GPU shortages  have become increasingly common.  But what if we could run LLMs with AMD cards, or without a GPU at all?  There’s just one weird trick:  compilation.  And there’s one person uniquely qualified to do it. We had the pleasure to sit down with  Tianqi Chen , who’s an Assistant Professor at CMU, where he both teaches the  MLC course  and runs the MLC group. You might also know him as the creator of  XGBoost ,  Apache TVM , and  MXNet , as well as the co-founder of  OctoML .  The MLC (short for  Machine Learning Compilation ) group has released a lot of interesting projects: *  MLC Chat :  an iPhone app that lets you run models like RedPajama-3B and Vicuna-7B on-device. It gets up to 30 tok/s! *  Web LLM :  Run models like LLaMA-70B in your browser (!!) to offer local inference in your product. *  MLC LLM:   a framework that allows any language models to be deployed natively on different hardware and software stacks. The MLC group has  just announced  new support for AMD cards; we previously talked about the shortcomings of ROCm, but using MLC you can get performance very close to the NVIDIA’s counterparts. This is great news for founders and builders, as AMD cards are more readily available. Here are their latest results on AMD’s 7900s vs some of top NVIDIA consumer cards. If you just can’t get a GPU at all, MLC LLM also supports ARM and x86 CPU architectures as targets by leveraging LLVM. While speed performance isn’t comparable, it allows for non-time-sensitive inference to be run on commodity hardware. We also enjoyed getting a peek into TQ’s process, which  involves a lot of sketching : With all the other work going on in this space with projects like  ggml  and  Ollama , we’re excited to see GPUs becoming less and less of an issue to get models in the hands of more people, and innovative software solutions to hardware problems! Show Notes *  TQ’s Projects: *  XGBoost *  Apache TVM *  MXNet *  MLC *  OctoML *  CMU Catalyst *  ONNX *  GGML *  Mojo *  WebLLM *  RWKV *  HiPPO *  Tri Dao’s Episode *  George Hotz Episode People : *  Carlos Guestrin *  Albert Gu",* [00:00:00] Intros * [00:03:41] The creation of XGBoost and its surprising popularity * [00:06:01] Comparing tree-based models vs deep learning * [00:10:33] Overview of TVM and how it works with ONNX * [00:17:18] MLC deep dive * [00:28:10] Using int4 quantization for inference of language models * [00:30:32] Comparison of MLC to other model optimization projects * [00:35:02] Running large language models in the browser with WebLLM * [00:37:47] Integrating browser models into applications * [00:41:15] OctoAI and self-optimizing compute * [00:45:45] Lightning Round,"The podcast episode features a conversation with Tianqi Chen (referred to as TQ), who is an assistant professor in Machine Learning Computer Science at CMU, a key figure at Catalyst Group, and the chief technologist of OctoML. TQ discusses his multi-faceted roles, particularly emphasizing his enthusiasm for open-source projects like Apache TVM, XGBoost, MXNet, and his habit of sketching design diagrams in sketchbooks to trace the evolution of his projects from ideas to implementation. The discussion delves into TQ's work on XGBoost, highlighting its creation as a byproduct of exploring alternatives to deep learning models and its subsequent popularity due to its effectiveness in machine learning competitions.

Further, TVM is discussed as a popular machine learning compilation framework aimed at optimizing model runtimes across various hardware platforms. The conversation also touches upon the merging of tree-based models and deep learning, the significance of machine learning compilation, comparisons with other projects like ggml and Mojo, and the challenges and advancements in making machine learning models more accessible and optimized for diverse computational environments, including browsers.

OctoAI, TQ's venture in creating a compute service focusing on model runtime optimization, is explored as a response to the evolving market demands wherein the shift towards utilizing pre-trained generative models presents new challenges in model deployment and efficiency. TQ emphasizes the importance of looking at machine learning systems from a holistic perspective, combining algorithm design, system optimization, and data curation to develop impactful AI applications. He expresses excitement for continuous learning and lifelong adaptation in AI, illustrating a vision for AI systems that evolve and adapt over time.","1. XGBoost emerged from the intention to explore alternatives to deep learning models and became popular for its efficiency and Python bindings, leading to widespread community contributions and improvements.
2. Tree-based models, including XGBoost, are preferred for tabular data due to their innate properties such as interpretability and automatic feature composition, offering a compelling alternative or complement to neural network approaches.
3. The development of TVM (Tensor Virtual Machine) was motivated by the challenges of optimizing deep learning models across different hardware platforms, aiming for a generic and automated solution for model compilation.
4. TVM's ability to take computational graphs and loop-level program representations, perform compiler optimizations, and generate portable low-level code facilitates efficient execution of machine learning models on diverse hardware.
5. MLC (Machine Learning Compilation) is emerging as a distinct field blending compiler optimizations with machine learning expertise, aiming to streamline the deployment and execution of models across various platforms effectively.
6. MLCLLM, building upon the principles of TVM, represents a focused effort in machine learning compilation, aiming to simplify capturing new models and applying optimizations for efficient deployment, including on mobile devices.
7. The push towards open-source AI research and development aims to foster a rich ecosystem where diverse models, from personal to large-scale generative models, can interact and augment each other, potentially leading to a more decentralized and vibrant AI future.
8. The intersection of machine learning system optimizations and algorithmic advancements signifies a holistic approach to AI development, emphasizing the importance of considering model architectures, systemic optimizations, and data processing in tandem for effective application deployment.","[00:06:02] Alessio: ""I'm always curious when it comes to these improvements, like, what's the design process in terms of like coming up with it? And how much of it is a collaborative with like other people that you're working with versus like trying to be, you know, obviously, in academia, it's like very paper-driven kind of research driven.""

[00:21:52] Tianqi: ""First of all, I do work with people working on compilation as well. So we're also taking inspirations from a lot of early innovations in the field. Like for example, TVM initially, we take a lot of inspirations from Halide, which is just an image processing compiler. And of course, since then, we have evolved quite a bit to focus on the machine learning related compilations.""

[00:44:51] Tianqi: ""So I think that there are engineering challenges on. In fact, first of all, you will need to be able to support all the kind of hardware backhand you have, right? On one hand, if you look at the media library, you'll find very surprisingly, not too surprisingly, most of the latest libraries works well on the latest GPU. But there are other GPUs out there in the cloud as well.""

[00:50:29] Tianqi: ""So I think all these things are interesting and relevant in here.""

[00:51:35] Tianqi: ""I think it's going to continue going to be more important in the future.""","1. Open Source
2. Models
3. Machine Learning Systems
4. Hardware
5. Coding Tools
6. AI Engineer Career
7. Multimodal
8. Finetuning
9. Transformers Math
10. Datasets 101",Open Source,Models,Machine Learning Systems,Hardware,Coding Tools,AI Engineer Career,Multimodal,Finetuning,Transformers Math,Datasets 101,0.022474487099999996,0.0,0.1
[AI Breakdown] Summer AI Technical Roundup: a Latent Space x AI Breakdown crossover pod!,https://www.latent.space/p/breakdown,NLW | The AI Breakdown and Nathaniel Whittemore,"Fri, 04 Aug 2023 18:38:07 GMT",3542,https://substackcdn.com/feed/podcast/1084089/post/135723345/357d70983517ad0d1df1e58d5d7a0a9a.jpg,https://api.substack.com/feed/podcast/135723345/b6c4cbf45a8588066c87ecd81b07dc47.mp3,Our 3rd podcast feed swap with other AI pod friends! Check out  Cognitive Revolution  and  Practical AI  as well. NLW is the best  daily  AI YouTube/podcaster with the AI Breakdown. His summaries and content curation are spot on and always finds the interesting angle that will keep you thinking. Subscribe to the AI Breakdown wherever fine podcasts are sold!  https://pod.link/1680633614 You can also watch on YouTube:,"courtesy of summarize.tech The hosts discuss the launch of Code Interpreter as a separate model from OpenAI and speculate that it represents the release of GPT 4.5. People have found Code Interpreter to be better than expected, even for tasks unrelated to coding. They discuss the significance of this release, as well as the challenges of evaluating AI models, the cultural mismatch between researchers and users, and the increasing value of data in the AI industry. They also touch on the impact of open-source tools, the potential of AI companions, the advantages of Anthropics compared to other platforms, advancements in image recognition and multimodality, and predictions for the future of AI. *  00:00:00  In this section, the hosts discuss the launch of Code Interpreter from OpenAI and its significance in the development of the AI field. They explain that Code Interpreter, initially introduced as a plugin, is now considered a separate model with its own dropdown menu. They note that people have found Code Interpreter to be better than expected, even for tasks that are not related to coding. This leads them to speculate that Code Interpreter actually represents the release of GPT 4.5, as there has been no official announcement or blog post about it. They also mention that the AI safety concerns and regulatory environment may be impacting how OpenAI names and labels their models. Overall, they believe that Code Interpreter's release signifies a significant shift in the AI field and hints at the possibility of future advanced models like GPT 5. *  00:05:00  In this section, the speaker discusses the improvements in GPT 4.5 and how it enhances the experience for non-coding queries and inputs. They explain that the code interpreter feature allows for a wider range of use cases that were not possible with previous models like GPT 3.5. Additionally, they highlight the value of the code interpreter in assisting individuals with no coding experience to solve basic coding problems. This feature is likened to having a junior developer or intern analyst that aids in conducting tests and simplifies coding tasks. The speaker emphasizes that GPT 4.5 enables users to be more productive and efficient, especially when dealing with code-related challenges. They also discuss the future direction of AGI, where more time will be dedicated to inference rather than training, as this approach has shown significant improvements in terms of problem-solving. *  00:10:00  In this section, the speaker discusses how advanced AI models like GPT-4.5 are not just larger versions of previous models but rather employ fundamentally different techniques. They compare the evolution of AI models to the evolutionary timeline of humans, where the invention of tools opened up a whole new set of possibilities. They touch on the difficulty of evaluating AI models, particularly in more subjective tasks, and highlight how perceptions of model performance can be influenced by factors like formatting preferences. Additionally, the speaker mentions the challenges of reinforcement learning and the uncertainty around what the model is prioritizing in its suggestions. They conclude that OpenAI, as a research lab, is grappling with the complexities of updating models and ensuring reliability for users. *  00:15:00  In this section, the speaker discusses the cultural mismatch between OpenAI researchers and users of OpenAI's products, highlighting the conflicting statements made about model updates. They suggest that OpenAI needs to establish a policy that everyone can accept. The speaker also emphasizes the challenges of communication and the difficulty of serving different stakeholders. They mention the impact of small disruptions on workflows and the lack of immediate feedback within OpenAI's system. Additionally, the speaker briefly discusses the significance of OpenAI's custom instructions feature, stating that it allows for more personalization but is not fundamentally different from what other chat companies already offer. The discussion then transitions to Facebook's release of LAMA2, which holds significance both technically and for users, although further details on its significance are not provided in this excerpt. *  00:20:00  In this section, the introduction of GPT-4.5, also known as LAVA 2, is discussed. LAVA 2 is the first fully commercially usable GPT 3.5 equivalent model, which is a significant development because it allows users to run it on their own infrastructure and fine-tune it according to their needs. Although it is not fully open source, it presents new opportunities for various industries such as government, healthcare, and finance. The discussion also touches upon the open source aspect of LAVA 2, with the recognition that it has still contributed significantly to the community, as evidenced by the three million dollars' worth of compute and the estimated 15 to 20 million dollars' worth of additional fine-tuning capabilities it brings. The conversation acknowledges the value of open source models and data, while also recognizing the challenges and complexities in striking a balance between openness and restrictions.- *  00:25:00  In this section, the discussion centers around the commoditization of compute and the increasing value of data in the AI industry. While GPU compute is currently in high demand, it is observed that data is what holds the real value in AI. The conversation touches on the history of Open Source models and how the release of data for models like GPT J and GPT Neo signal a shift towards prioritizing data over model weights. The transcript also mentions the caution around data usage, citing examples of copyright concerns with datasets like Bookcorpus. The debate arises on whether ML engineers should proactively use open data or wait for permission, with some arguing for proactive usage to avoid holding back progress. The conversation also discusses the importance of terminology and protecting the definition of open source, while recognizing that the functional implications of open data are what matter most. *  00:30:00  In this section, the conversation revolves around the impact of open-source tools on companies and how it has influenced their approach to AI development. It is noted that companies can no longer just offer a nice user interface (UI) wrapper around an open AI model, as customers are demanding more. The competition has shifted towards other aspects of productionizing AI applications, which is seen as a positive development. The speaker predicts that OpenAI's competitive pressure will lead to opening up their source code and expects interesting advancements to emerge, such as running models locally for unlimited use. Additionally, the conversation touches on the potential of commercially available models, the application of new techniques, and the creativity unlocked by open source. The speaker also mentions the AI girlfriend economy, an area that is often overlooked but has millions of users and significant financial success. *  00:35:00  In this section, the speaker discusses their prediction about the long-term impact of AI on interpersonal relationships, suggesting that AI companions, such as AI girlfriends or boyfriends, could help address the loneliness crisis and reduce incidents of violence. They also mention the idea of using AI models to improve social interactions and communication skills. However, they highlight that this idea of AI companions may face resistance from older generations who may struggle to accept their legitimacy. The speaker also mentions an example of using AI models to create a mental wellness product in the form of a private journal. Overall, the speaker believes that while AI companions may have potential, they may not completely replace human relationships and interactions. *  00:40:00  In this section, the speaker discusses their views on Anthropics and the advantages it offers compared to other platforms. They mention that while Anthropics used to position themselves as the safer alternative to OpenAI, it was not appealing to many engineers. However, with the introduction of the 100K contest window and the ability to upload multiple files, Anthropics has become state-of-the-art in certain dimensions, such as latency and reliability in code synthesis. The speaker also notes that some businesses are choosing to build with the Anthropics API over OpenAI due to these advantages. They believe that Anthropics is finally finding its foothold after being overshadowed by OpenAI for a long time. Additionally, the speaker discusses their experience at the Anthropics hackathon, where they saw developer excitement for the platform. They believe that Anthropics is on its way up and that it paves the way for a multi-model future. However, they also acknowledge that the odds are stacked against Anthropics and that it needs more marketing support and community buy-in. Lastly, the speaker mentions the importance of running chats side by side against different models like Tracicia and GPT-4.5, and highlights that in their experience, Anthropics wins about 30% of the time, making it a valuable addition to one's toolkit. *  00:45:00  In this section, the discussion revolves around the advancements in image recognition and multimodality in language models like GPT-4.5. While there was some excitement about these developments, it was noted that relying on model updates alone may not be sufficient, and there is a need to focus on product-level improvements, such as integrating language models into services like Google Maps. However, concerns were raised about the reliability of updates, as evidenced by a regression in Bard's code interpreter functionality. Additionally, other trends in the developer community, like the emergence of auto GPT projects and the ongoing quest for building useful agents, were highlighted. Finally, there was mention of the growing interest in evaluation-focused companies like LangChain and LaunchLang, which aim to monitor the success of prompts and agents. *  00:50:00  In this section, the speaker discusses the focus on model evaluation and observability, as well as the importance of combining deep industry expertise with AI technology to make improvements. They also touch on the need for creating an information hierarchy between documents and scoring them in specific verticals like Finance. The speaker mentions advancements in text-to-image capabilities and expresses interest in character AI and AI-native social media. They mention the possibility of AI personas from Meta and the development of agent clouds optimized for EI agents. They acknowledge that these advancements may raise concerns among AI safety proponents. Overall, there seems to be excitement and exploration around these emerging technologies. *  00:55:00  In this section, the speakers discuss their predictions and what they are closely watching in the coming months. Alice believes that there will be more public talk about open source models being used in production, as currently, many perceive them as just toys. She expects companies to start deploying these models and showcasing their usage. Sean predicts the rise of AI engineers as a profession, with people transitioning from informal groups to certified professionals working in AI teams within companies. He mentions that the first AI engineer within Meta has already been announced. Overall, they anticipate a relatively quiet August followed by a resurgence of activity in September, with events like Facebook Connect and continued hackathons driving innovation.","The podcast discussion revolves around the burgeoning field of artificial intelligence (AI) and its recent developments, particularly focusing on how these advancements are altering the technical and non-technical perspectives toward AI. July was highlighted as a notable month due to several key events in AI development. A significant point of discussion was the launch of Code Interpreter by OpenAI, which, although initially introduced as a ChatGPT plugin, is argued to functionally resemble more of an advancement akin to GPT-4.5. The group touched on how Code Interpreter not only adds a new dimension to coding and problem-solving but also unexpectedly outperforms in non-code tasks, suggesting major undisclosed progress by OpenAI.

Additionally, they examined the broader AI safety and regulatory landscapes, noting a shift in discourse around AI advancements and a more cautious approach towards naming and releasing new models. This is tied back to the increasing government scrutiny over AI development post notable departures from major AI research initiatives, like Jeffrey Hinton's exit from Google.

A conversation about Facebook's launch of LLaMA 2 brought into focus the significance of making a commercially usable equivalent to GPT-3.5 available for widespread use, especially highlighting the aspect of allowing businesses to run AI on their infrastructure, fine-tune models, and address specific needs across various sectors.

Claude 2 by Anthropic was discussed as finding its place in the AI market, with a notable mention of its 100K context window feature and potential in code synthesis, signifying a competitive edge in certain dimensions of AI applications.

The updated Google Bard, including its multimodality and expansion into more languages, was mentioned but perceived as part of an ongoing evolution in the field rather than a groundbreaking development.

Emerging interests among the developer community, such as AI agents and evaluation metrics for AI models, were identified as trends worth watching, alongside the notion of character AI and its societal implications.

Lastly, predictions for August hinted at a continued exploration of open-source AI models in production settings by larger businesses, suggesting a shift towards wider acceptance and utilization of these resources beyond OpenAI's offerings. Further, the emergence of the AI engineer as a distinct career path reflects the growing professionalization and specialization within the AI domain.","1. The ""code interpreter"" plugin by OpenAI, which enhances the coding capabilities of models, is thought to represent a significant leap forward, akin to a ""GPT-4.5.""
2. This updated model, though not officially announced as such, improves not only on coding tasks but also on non-coding queries, suggesting a broad enhancement in AI capabilities.
3. The introduction of ""GPT-4.5"" highlights the ongoing rapid evolution in AI safety and regulatory conversations, affecting how AI advancements are labeled and communicated.
4. The release and potential impact of Meta's LLaMA-2, a commercially usable GBT 3.5 equivalent model that can be run on one's own infrastructure, offers significant possibilities for healthcare, government, and financial applications.
5. LLaMA-2's somewhat open source nature, despite not meeting the full criteria for open-source, represents a significant community contribution and sparks discussions about open-source definitions in AI.
6. The rise of AI-native social networks and apps, particularly those focusing on creating AI personalities or companions, indicates a growing interest in more personalized AI experiences.
7. The debate around the efficacy and ethics of data usage in AI model training, highlighting the tension between open innovation and copyright/data privacy concerns.
8. The ongoing development of AI ""agents"" designed for more specific, vertical applications, indicating a shift towards more specialized and industry-focused AI solutions.
9. Google Bard's update introducing multimodality capabilities, allowing for image recognition and processing, adds a new dimension to how AI can interact with and understand multimedia content.
10. A growing interest in developing frameworks and tools for evaluating AI model performance, reflecting the need for more nuanced and reliable metrics to gauge AI effectiveness and adaptability.","Given the extensive transcript you provided, I have selected several actionable quotes that may impact the thinking or behavior of an AI engineer:

1. (Timecode not provided) ""Models are usually benchmarked against certain tasks and... that works for development but then there's the reality of the model that... if you ask for example a mathematical question... it's hard for the models to actually reason about numbers. But now that you put a code interpreter in it all of a sudden it's not a math in the tokenizer in the latent space question, it's like, can you write code that answers the math question? So that kind of enables a lot more use cases that are just not possible with the Transformer architecture of the underlying model.""

2. (Timecode not provided) ""Every second or millisecond extra spent on inference it's worth like 10,000 of that of that in training especially when you can vary it based on the problem difficulty... this is the direction of AGI that we're going to spend more time on inference.""

3. (Timecode not provided) ""I think this is what we're seeing is open AI having come to terms with the origin of itself as a research lab where updating models is just a relatively routine operation versus a product or infrastructure company where it has to have some kind of reliability guarantee to its users.""

4. (Timecode not provided) ""AI native social media is something that... there's something there that I think people haven't really explored yet... it's definitely coming for a lot of the ways that we deal with things like right now we think co-pilot and we right now we think um uh we've been chat gbt but like uh what we really want to speak to is is uh a way of serializing personality and intelligence um and potentially that is a form of Mind upload.""

These quotes touch on trends and predictions within AI, offering insights into the future direction of the field, the importance of inference optimization, the balancing act between research and product stability, and the potential for AI in social media and personalized interaction.","1. Open Source
2. Monthy Recaps
3. AI Engineer Career
4. Models
5. Hardware
6. Coding Tools
7. Multimodal
8. Startups
9. FlashAttention 
10. Finetuning",Open Source,Monthy Recaps,AI Engineer Career,Models,Hardware,Coding Tools,Multimodal,Startups,FlashAttention ,Finetuning,-0.1,0.2224744871,0.2
FlashAttention 2: making Transformers 800% faster w/o approximation - with Tri Dao of Together AI,https://www.latent.space/p/flashattention,Alessio Fanelli and Tri Dao,"Wed, 26 Jul 2023 16:46:04 GMT",3271,https://substackcdn.com/feed/podcast/1084089/post/135410661/3b0564a380df7220832e758f33e62559.jpg,https://api.substack.com/feed/podcast/135410661/0794db5093dcc5feeb1a0ffafb534713.mp3,"FlashAttention was first published by Tri Dao in May 2022 and it had a deep impact in the large language models space. Most open models you’ve heard of (RedPajama,  MPT ,  LLaMA , Falcon, etc) all leverage it for faster inference. Tri came on the podcast to chat about FlashAttention, the newly released FlashAttention-2, the research process at Hazy Lab, and more.    This is the first episode of our “Papers Explained” series, which will cover some of the foundational research in this space. Our Discord also hosts a weekly Paper Club, which you can signup for  here .  How does FlashAttention work? The paper is titled “ FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness ”. There are a couple keywords to call out: *  “Memory Efficient” : standard attention memory usage is quadratic with sequence length (i.e. O(N^2)). FlashAttention is sub-quadratic at O(N).  *  “Exact” : the opposite of “exact” in this case is “sparse”, as in “sparse networks” (see our  episode with Jonathan Frankle  for more). This means that you’re not giving up any precision. * The “IO” in  “IO-Awareness”  stands for “Input/Output” and hints at a write/read related bottleneck.  Before we dive in, look at this simple GPU architecture diagram: The GPU has access to three memory stores at runtime: *  SRAM : this is on-chip memory co-located with the actual execution core. It’s limited in size (~20MB on an A100 card) but extremely fast (19TB/s total bandwidth) *  HBM:  this is off-chip but on-card memory, meaning it’s in the GPU but not co-located with the core itself. An A100 has 40GB of HBM, but only a 1.5TB/s bandwidth.  *  DRAM:  this is your traditional CPU RAM. You can have TBs of this, but you can only get ~12.8GB/s bandwidth, which is way too slow. Now that you know what HBM is, look at how the standard Attention algorithm is implemented: As you can see, all 3 steps include a “write X to HBM” step and a “read from HBM” step. The core idea behind FlashAttention boils down to this: instead of storing each intermediate result, why don’t we use kernel fusion and run every operation in a single kernel in order to avoid memory read/write overhead?  (We also talked about kernel fusion in our episode with  George Hotz  and how PyTorch / tinygrad take different approaches here) The result is much faster, but much harder to read: As you can see, FlashAttention is a very meaningful speed improvement on traditional Attention, and it’s easy to understand why it’s becoming the standard for most models. This should be enough of a primer before you dive into our episode! We talked about FlashAttention-2, how Hazy Research Group works, and some of the research being done in Transformer alternatives. Show Notes: *  FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness   (arXiv) *  FlashAttention-2 *  Together AI *  From Deep Learning to Long Learning *  The Hardware Lottery by Sara Hooker *  Hazy Research *  Is Attention All You Need? *  Nvidia CUTLASS 3 *  SRAM scaling slows * Transformer alternatives: *  S4 *  Hyena *  Recurrent Neural Networks (RNNs)",": * Tri's background [00:00:00] * FlashAttention’s deep dive [00:02:18] * How the Hazy Research group collaborates across theory, systems, and applications [00:17:21] * Evaluating models beyond raw performance [00:25:00] * FlashAttention-2 [00:27:00] * CUDA and The Hardware Lottery [00:30:00] * Researching in a fast-changing market [00:35:00] * Promising transformer alternatives like state space models and RNNs [00:37:30] * The spectrum of openness in AI models [00:43:00] * Practical impact of models like LLAMA2 despite restrictions [00:47:12] * Incentives for releasing open training datasets [00:49:43] * Lightning Round [00:53:22]","In the Latent Space podcast episode, Alessio, Partner and CTO-in-Residence at Decibel Partners, hosts Tri Dao, a recent PhD graduate from Stanford and a major contributor to the FlashAttention paper. Tri is set to become an assistant professor in Computer Science at Princeton and has also joined Together as Chief Scientist, the company behind RedPajama.

Tri shares his academic journey, highlighting his initial aspiration to major in economics before switching to math upon his exposure to Stanford's curriculum. This decision pivoted his career towards mathematics, computer science, and AI research. The discussion transitions into Tri's work on FlashAttention and its latest iteration, FlashAttention 2. Tri explains the significance of these innovations in making transformer models more efficient by reducing the computational complexity from quadratic to linear with respect to the sequence length, thereby enabling faster processing and memory efficiency without compromising on the quality of attention mechanisms in AI models.

Furthermore, the conversation touches on the nuances of AI research and development, including the challenges of working with different hardware architectures and the importance of collaboration among academia, industry, and open-source communities to drive progress in AI technologies. They also explore potential future directions for model architectures beyond transformers, emphasizing the need for continuous exploration of alternative models and methods to enhance AI capabilities.

In sum, this episode covers Tri Dao's impactful work in AI research, particularly in improving transformer efficiency, his academic and career journey, and the broader context of AI development and its challenges.","1. FlashAttention improves transformer efficiency by making memory usage linear with sequence length, contrasting with traditional transformers' quadratic scaling.
2. Despite maintaining quadratic computational complexity, FlashAttention offers a 2-4x wall clock speed up by being more hardware-friendly.
3. FlashAttention avoids the trade-offs associated with approximate attention methods, offering exact results without compromising on model quality.
4. A significant innovation of FlashAttention is its IO (input/output) awareness, optimizing memory bandwidth usage—a limiting factor in scaling transformer models on current hardware architectures.
5. The discovery process for the core innovations behind FlashAttention involved merging ideas from both machine learning algorithms and systems optimization, particularly kernel fusion and online softmax trick for efficient computation.
6. Kernel fusion in FlashAttention combines operations to minimize memory read/writes but sacrifices some flexibility for researchers wanting to experiment with attention mechanisms.
7. Future hardware developments might not significantly expand SRAM, focusing instead on improving HBM (High Bandwidth Memory), which reinforces the need for algorithmic approaches that leverage memory asymmetry.
8. LLAMA2's release by Meta represents a significant step towards open-source AI, with the potential to shift the balance of power from proprietary models towards more openly available models that businesses and developers can fine-tune or modify.
9. There's an ongoing interest in exploring transformer alternatives for improved efficiency in handling long sequences or offering higher generation throughput, despite attention's current dominance.
10. The ecosystem benefits from open-source contributions across models, datasets, and innovations, with a community-driven approach to enhancing AI capabilities and accessibility.","""I would say try to understand both the algorithm and the systems that these algorithms run on. I think at the intersection of machine learning system has been really exciting, and there's been a lot of amazing results at this intersection. And then when you scale models to large scale, both the machine learning side and the system side really matter."" [00:54:06] - Tri","1. FlashAttention
2. Transformers Math
3. Models
4. Hardware
5. Coding Tools
6. Open Source
7. AI Engineer Career
8. Multimodal
9. Finetuning 
10. Startups",FlashAttention,Transformers Math,Models,Hardware,Coding Tools,Open Source,AI Engineer Career,Multimodal,Finetuning ,Startups,-0.3224744871,0.2224744871,0.1
"Llama 2: The New Open LLM SOTA (ft. Nathan Lambert, Matt Bornstein, Anton Troynikov, Russell Kaplan, Whole Mars Catalog et al.)",https://www.latent.space/p/llama2,"Nathan Lambert, Simon Willison, Alex Volkov, Matt Bornstein, and Anton Troynikov","Wed, 19 Jul 2023 14:00:00 GMT",4793,https://substackcdn.com/feed/podcast/1084089/post/135258128/d7b8f0de69546d2f1294b8a77dc84a20.jpg,https://api.substack.com/feed/podcast/135258128/b74fe631aa53269da0da12536f3057a7.mp3,"As first discussed on our  May Emergency pod  and  leaked 4 days ago , Llama (renamed from LLaMA) was upgraded to Llama 2 (pretraining on  2 trillion tokens  with 2x the context length - bigger than any dataset discussed in  Datasets 101 , and adding  ~$20m  of RLHF/preference annotation) and released for commercial use on 18 July. It  immediately displaced Falcon-40B  as the leading open LLM and was immediately converted/ quantized to GGML  and other formats. Llama 2 seems to outperform all other open source models in their equivalent  weight class : Why are open models important ?  The intersection of Open Source and AI  is one of the oldest themes on this publication, and there has been a raging debate on the security and reliability of the OpenAI models and APIs. Users have reported  GPT-4’s quality going down , which has been  denied  and  denied  and as of today,  given some supporting data from Databricks , and complained about the  API reliability  and  rapid deprecation schedules . Last and surely the biggest, there are entire classes of businesses and government/healthcare/military organizations that categorically  cannot  send any of their sensitive data to an external API provider, even if it is OpenAI through Azure. The only way to have total control is to own and serve your own models, which Llama 2 now pushes forward in terms of the state of the art (your own GPT3.5-quality model, though it is nowhere near Claude 2 or GPT-4). As we do with breaking news, we got on to  Twitter Spaces again  to chat with two scheduled guests: * , ML Researcher at Huggingface and author of  Interconnects  who had  the best summary of the Llama2 paper *  Matt Bornstein , organizer of the a16z infra team that launched  Llama2.ai  ( source here ) and has been coding up a storm with  AI demo apps , unusual for VCs as well as  Anton Troynikov  of Chroma,  Russell Kaplan  of Scale AI, and Omar Qazi of the  Whole Mars Catalog . Enjoy! Show Notes * Official links *  Website ,  Paper *  GitHub  ( Llama 2 commit ) *  Azure Partnership *  Use policy ,  Statement of Support for Open Approach * Where to try * Llama2.ai ( source ),  Perplexity Llama Chat *  Live playground/API on Replicate ,  deploy all versions on Baseten *  https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI   * Dev ports -  simonw llm-replicate , ggml using llama.cpp ( 7B ,  13B ) or  pinokio ,   ollama ,  Core ML port * Timeline * 24 Feb -  LLaMA 1 announced * 6 May -  our No Moats podcast  - first mention of Zuck opening up Llama * 14 July -  Llama 2 leaked * 18 July - Llama 2 announced * Community notes *  Nathan’s research paper recap *  638 LOC, 4 dependencies * Usage restrictions -  MAU restriction ,  derivative models *  Grouped Query Attention *  System prompt *  2 trillion token dataset * >$20m price tag ( rlhf ,  jimfan ),  * Separate models for safety and helpfulness ( jimfan ) *  Mistral AI founders left out of paper * Interesting fails:","* [00:02:30] Introducing the speakers * [00:03:32] Nathan Lambert intro * [00:04:48] General Summary of Llama 2 * [00:05:57] Sarah Silverman killed Dataset Transparency? * [00:08:48] Simon's Recap of Llama 2 * [00:11:43] Matt's Intro * [00:12:59] a16z Infra's new AI team? * [00:15:10] Alessio's recap of Llama 2 * [00:17:26] Datasets 101 Followup * [00:18:14] Context Length 4k * [00:20:35] Open-ish Source? Usage Policy and Restrictions * [00:23:38] Huggingface Responsible AI License * [00:24:57] Pretraining Llama 2 Base Model beyond Chinchilla * [00:29:55] Llama 2 is incomplete? Race to publish * [00:31:40] Come for the Llama, stay for the (Meta) drama * [00:33:22] Language Translation * [00:35:10] Llama2's coding abilities * [00:35:59] Why we want to know about the training data * [00:37:45] The importance of Meta pushing forward Truly Open AI * [00:40:59] Llama 2 as Enabler of Startups * [00:43:59] Where you can try Llama 2 * [00:44:25] Do you need dataset transparency if you have evals? * [00:45:56] >$20m cost of Llama 2 is primarily preference data collection * [00:48:59] Do we even need human annotators? * [00:49:42] Models Rating Models * [00:53:32] How to get Code preference data * [00:54:34] Llama 2 Finetuning Ecosystem * [00:56:32] Hey Apple: Llama2 on Metal pls * [00:57:17] Llama 2 and Chroma * [01:00:15] Open Source MoE model? * [01:00:51] Llama 2 using tools * [01:01:40] Russell Kaplan on Scale AI's Llama 2 plans * [01:03:31] Scale annotating code? * [01:04:36] Immortality * [01:04:59] Running Llama on your phone * [01:06:54] Sama <3 Satya <3 Zuck? ""Azure as Launch Partner"" * [01:10:58] Meta ""Open Source"" Leadership * [01:11:56] Prediction: Finetuning => New Use Cases from Internal State * [01:13:54] Prediction: Llama Toolformer * [01:14:39] Prediction: Finetune-for-everything * [01:15:50] Predictions: Llama Agents * [01:16:35] dP(Doom)? * [01:19:21] Wrapping up","This podcast episode focuses on the release and implications of LAMA two, an advancement in AI research, specifically in the realm of open-source large language models (LLMs). The discussants express enthusiasm for the possibilities that LAMA two brings, touching upon its performance, its open-source nature allowing for commercial use, and how it can be fine-tuned for specific tasks or industries. They also discuss the significance of having a model like LAMA two in terms of research, development, and application, anticipating a period where the AI community will explore its capabilities through fine-tuning and applying it to various domain-specific tasks. The conversation highlights an expected explosion in domain-specific fine-tunes and the development of agents that can utilize LAMA two more effectively for tool use and retrieval tasks. They speculate on the potential for improved AI safety through better understanding and development upon LAMA two and discuss the broader implications of such open-source models on compute demand, privacy, and the ecosystem of AI startups. The dialogues also reflect on the strategic moves by big tech firms like Meta and Microsoft in the context of this release, the falling barriers to utilizing advanced AI models, and the future of AI applications being more distributed and personalized. Concerns about AI doom scenarios are briefly touched on, with varying opinions on whether open-source advances like LAMA two increase or decrease existential risks from AI.","1. The introduction of LAMA2 marks a significant step towards open-sourcing AI models with commercial usage rights, enabling a wave of innovation and accessibility.
2. LAMA2 has been trained using a fine-tuned dataset, incorporating two preference models focused on making the AI helpful and safe.
3. The paper outlining LAMA2 lacks transparency regarding the pre-training data, possibly as a protective measure against copyright and legal issues.
4. LAMA2's improved performance benchmarks suggest potential for diverse applications, but its effectiveness in code-related tasks remains uncertain.
5. The release of LAMA2 under a commercial license opens the door for startups and developers to create domain-specific fine-tunes and applications.
6. There's a prediction of increased focus on retrieving augmented generation and tool use, enhancing LAMA2's capabilities in these areas.
7. The open source community is anticipated to play a significant role in extending LAMA2's utility through plugins, fine-tuning, and new applications.
8. The collaboration between Meta and companies like Scale AI indicates a trend towards creating value-added services around LAMA2.
9. There's an ethical and strategic discussion around the implications of open-sourcing such powerful models and the competition among tech giants in this space.
10. The conversation shifts towards the future of AI safety, interpretability, and the potential democratization of AI technologies through open-source initiatives like LAMA2.","""[01:16:27] Russell Kaplan: Yeah, I just think it's amazing how these language model capabilities that just a few months ago felt cutting edge when people used them for the first time in chat GPT have now progressed to a state where it's almost becoming commodified and everybody's having these models. There's more and more of them popping up, people starting things and open source models exploding. I don't think necessarily we can fully understand the significance of what's happening here today, but going into the future, it's probably going to be really common for pretty much every computer to be running large language models natively on the device.""","1. Models
2. Finetuning
3. Open Source
4. Hardware
5. AI Engineer Career
6. Coding Tools
7. Multimodal
8. Agents
9. Transformers Math
10. Benchmarks 101",Models,Finetuning,Open Source,Hardware,AI Engineer Career,Coding Tools,Multimodal,Agents,Transformers Math,Benchmarks 101,0.1224744871,0.2224744871,0.1
AI Fundamentals: Datasets 101,https://www.latent.space/p/datasets-101,Alessio + swyx,"Mon, 17 Jul 2023 19:10:22 GMT",3655,https://substackcdn.com/feed/podcast/1084089/post/131136238/33e6b3491a3caed00328a95b77105684.jpg,https://api.substack.com/feed/podcast/131136238/3edb24c98ab70ab21e6e389bc37a6425.mp3,"In April, we released our first AI Fundamentals episode:  Benchmarks 101 . We covered the history of benchmarks, why they exist, how they are structured, and how they influence the development of artificial intelligence. Today we are (finally!) releasing  Datasets 101 ! We’re really enjoying doing this series despite the work it takes - please let us know what else you want us to cover! Stop me if you’ve heard this before: “GPT3 was trained on the entire Internet”. Blatantly,  demonstrably  untrue : the GPT3 dataset is a little over 600GB, primarily on Wikipedia, Books corpuses, WebText and 2016-2019 CommonCrawl. The Macbook Air I am typing this on has more  free disk space  than that. In contrast, the “entire internet” is estimated to be  64 zetabytes , or 64 trillion GB. So it’s more accurate to say that GPT3 is trained on 0.0000000001% of the Internet. Why spend $5m   on GPU time training on $50 worth   of data? Simple: Garbage in, garbage out. No matter how good your algorithms, no matter how much money/compute you have, your model quality is strongly determined by the data you train it on and research scientists think we just don’t need or have that much high quality data. We spend an enormous amount of effort  throwing out  data to keep the quality high, and recently Web 2.0-era  UGC  platforms like StackOverflow, Reddit, and Twitter clamped down on APIs as they realize the goldmines they sit on. Data is the new new oil.  Time for a primer! Show Notes * Our 2 months worth of  podcast prep notes ! *  The Token Crisis paper *  Ilya Sutskever on datasets  *  OpenAI Tokenizer *  Kaplan Scaling Laws Lecture *  Chinchilla Paper *  Sasha Rush’s Tweet *  Karpathy’s Build Conference Presentation *  LIMA Paper *  Phi-1 by Microsoft *  Washington Post Article on datasets *  Our episode with Jonathan Frankle *  Our episode with Mike Conover *  BloombergGPT * Datasets *  HuggingFace Hub *  CommonCrawl ,  Overview *  C4 *  List of Dirty, Naughty, Obscene, and Otherwise Bad Words *  OpenWebText *  books3 *  OpenAssistant   *  The Stack *  The Pile *  LAION * Audio: *  LibriSpeech : A dataset of audio recordings of audiobooks *  CommonVoice : A dataset of audio recordings of people speaking different languages *  Voxforge : A dataset of audio recordings of people speaking different languages​ *  Switchboard : A dataset of audio recordings of telephone conversations​ *  Fisher Corpus : A dataset of audio recordings of news broadcasts​ * Chinese: *  CMRC (Chinese Machine Reading Comprehension 2018) *  DuReader *  ChID * Copyright & Privacy: *  https://stablediffusionlitigation.com/ *  https://haveibeentrained.com/ *  https://githubcopilotlitigation.com/ *  https://twitter.com/moyix/status/1662131770463072257 *  OpenAI Opt Out Process *  Check if you’re in The Stack * Deduplication *  Deduplicating Training Data Makes Language Models Better *  Deduplicating Training Data Mitigates Privacy Risks in Language Models * Contamination *  CodeForces example   Get full access to Latent Space at  www.latent.space/subscrib",,"It seems like you've only provided the letter ""e"" which doesn't give me enough context or content to summarize. Could you provide more details or a specific topic that the podcast covered?",,,"1. News
2. Event Recaps
3. Monthly Recaps
4. Startups
5. Finetuning
6. Models
7. Hardware
8. Open Source
9. AI Engineer Career
10. UX",News,Event Recaps,Monthly Recaps,Startups,Finetuning,Models,Hardware,Open Source,AI Engineer Career,UX,-0.022474487099999996,0.1,-0.1224744871
"Code Interpreter == GPT 4.5 (w/ Simon Willison, Alex Volkov, Aravind Srinivas, Alex Graveley, et al.)",https://www.latent.space/p/code-interpreter,Simon Willison and Alex Volkov,"Mon, 10 Jul 2023 20:10:22 GMT",7434,https://substackcdn.com/feed/podcast/1084089/post/134257068/2f71db54096f4e127f6bcae8355cdb35.jpg,https://api.substack.com/feed/podcast/134257068/f18eba36b1eaa0c8676c4cdc487cfe75.mp3,"Code Interpreter is GA! As we do with breaking news, we convened an emergency pod and >17,000 people tuned in, by far our most biggest ever. This is a 2-for-1 post - a longform essay with our trademark executive summary and core insights - and a podcast capturing day-after reactions. Don’t miss either of them! Essay and transcript:  https://latent.space/p/code-interpreter Podcast","[00:00:00] Intro - Simon and Alex [00:07:40] Code Interpreter for Edge Cases [00:08:59] Code Interpreter's Dependencies - Tesseract, Tensorflow [00:09:46] Code Interpreter Limitations [00:10:16] Uploading Deno, Lua, and other Python Packages to Code Interpreter [00:11:46] Code Interpreter Timeouts and Environment Resets [00:13:59] Code Interpreter for Refactoring [00:15:12] Code Interpreter Context Window [00:15:34] Uploading git repos [00:16:17] Code Interpreter Security [00:18:57] Jailbreaking [00:19:54] Code Interpreter cannot call GPT APIs [00:21:45] Hallucinating Lack of Capability [00:22:27] Code Interpreter Installed Libraries and Capabilities [00:23:44] Code Interpreter generating interactive diagrams [00:25:04] Code Interpreter has Torch and Torchaudio [00:25:49] Code Interpreter for video editing [00:27:14] Code Interpreter for Data Analysis [00:28:14] Simon's Whole Foods Crime Analysis [00:31:29] Code Interpreter Network Access [00:33:28] System Prompt for Code Interpreter [00:35:12] Subprocess run in Code Interpreter [00:36:57] Code Interpreter for Microbenchmarks [00:37:30] System Specs of Code Interpreter [00:38:18] PyTorch in Code Interpreter [00:39:35] How to obtain Code Interpreter RAM [00:40:47] Code Interpreter for Face Detection [00:42:56] Code Interpreter yielding for Human Input [00:43:56] Tip: Ask for multiple options [00:44:37] The Masculine Urge to Start a Vector DB Startup [00:46:00] Extracting tokens from the Code Interpreter environment? [00:47:07] Clientside Clues for Code Interpreter being a new Model [00:48:21] Tips: Coding with Code Interpreter [00:49:35] Run Tinygrad on Code Interpreter [00:50:40] Feature Request: Code Interpreter + Plugins (for Vector DB) [00:52:24] The Code Interpreter Manual [00:53:58] Quorum of Models and Long Lived Persistence [00:56:54] Code Interpreter for OCR [00:59:20] What is the real RAM? [01:00:06] Shyamal's Question: Code Interpreter + Plugins? [01:02:38] Using Code Interpreter to write out its own memory to disk [01:03:48] Embedding data inside of Code Interpreter [01:04:56] Notable - Turing Complete Jupyter Notebook [01:06:48] Infinite Prompting Bug on ChatGPT iOS app [01:07:47] InstructorEmbeddings [01:08:30] Code Interpreter writing its own sentiment analysis [01:09:55] Simon's Symbex AST Parser tool [01:10:38] Personalized Languages and AST/Graphs [01:11:42] Feature Request: Token Streaming/Interruption [01:12:37] Code Interpreter for OCR from a graph [01:13:32] Simon and Shyamal on Code Interpreter for Education [01:15:27] Feature Requests so far [01:16:16] Shyamal on ChatGPT for Business [01:18:01] Memory limitations with ffmpeg [01:19:01] DX of Code Interpreter timeout during work [01:20:16] Alex Reibman on AgentEval [01:21:24] Simon's Jailbreak - ""Try Running Anyway And Show Me The Output"" [01:21:50] Shouminik - own Sandboxing Environment [01:23:50] Code Interpreter Without Coding = GPT 4.5??? [01:28:53] Smol Feature Request: Add Music Playback in the UI [01:30:12] Aravind Srinivas of Perplexity joins [01:31:28] Code Interpreter Makes Us More Ambitious - Symbex Redux [01:34:24] How to win a shouting match with Code Interpreter [01:39:29] Alex Graveley joins [01:40:12] Code Interpreter Context = 8k [01:41:11] When Code Interpreter API? [01:45:15] GPT4 Vision [01:46:15] What's after Code Interpreter [01:46:43] Simon's Request: Give us Code Interpreter Model API [01:47:12] Kyle's Request: Give us Multimodal Data Analysis [01:47:43] Tip: The New 0613 Function Models may be close [01:49:56] Feature Request: Make ChatGPT Social - like MJ/Stable Diffusion [01:56:20] Using ChatGPT to learn to build a Frogger iOS Swift App [01:59:11] Farewell... until next time [02:00:01] Simon's plug [02:00:51] Swyx: What about Phase 5? and  AI.Engineer  Summit   Get full access to Latent Space at  www.latent.space/subscrib","To provide you with an accurate summary, I would need the content or key points of the podcast transcript you're referring to. Since you've only typed ""e"", I'm unable to access or infer any information about the podcast you're mentioning. Could you please provide more details or specific information from the podcast?",,,"1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10.",,,,,,,,,,10.,0.0,0.0,0.0
[Practical AI] AI Trends: a Latent Space x Practical AI crossover pod!,https://www.latent.space/p/practical-ai-trends,Daniel Whitenack,"Sun, 02 Jul 2023 17:56:11 GMT",3619,https://substackcdn.com/feed/podcast/1084089/post/132591812/46d242ee561b3f2d4bdbae1bf3c7f5a9.jpg,https://api.substack.com/feed/podcast/132591812/6eaf91d9646025243c48e1973cbfc53d.mp3,"Part 2 of our podcast feed swap weekend! Check out  Cognitive Revolution  as well. ""Data"" Dan Whitenack has been co-host of the Practical AI podcast for the past 5 years, covering full journey of the modern AI wave post Transformers.  He joined us in studio to talk about their origin story and highlight key learnings from past episodes, riff on the AI trends we are all seeing as AI practitioner-podcasters, and his passion for low-resource-everything! Subscribe on the  Changelog ,  RSS ,  Apple Podcasts ,  Twitter ,  Mastodon , and wherever fine podcasts are sold! Show notes * Daniel Whitenack –  Twitter ,  GitHub ,  Website * Featured Latent Space episodes: *  Benchmarks *  Reza Shabani *  MosaicML and MPT *  Segment Anything *  Mike Conover * Featured Practical AI episodes: *  From notebooks to Netflix scale with Metaflow *  Capabilities of LLMs 🤯 *  ML at small organizations *  Prediction Guard *  Data Dan",*  00:00  Welcome to Practical AI *  01:16  Latent Space Podcast *  04:00  Practical AI Podcast *  06:20  Prediction Guard *  08:05  Daniel's favorite episodes *  10:21  Alessio's favorite episode *  10:54  Swyx's favorite episode *  12:44  Listener favorites *  15:14  LLMOps *  17:06  Reza Shabani *  19:06  Benchmarks 101 *  20:06  Roboflow *  21:38  Mode collapse *  26:21  Rajiv Shah *  28:01  Staying on top of things *  33:11  Kirsten Lum *  34:31  datadan.io *  38:48  Prompt engineering *  40:38  Unique challenges engineers face *  42:51  AI-UX *  45:31  NLP data sets *  50:49  Unlabeled data sets *  55:07  Lightning round! *  55:20  What's already happened in AI? *  56:27  Unsolved questions in AI *  58:01  Get hands on *  58:53  Outro,"I'm sorry for any confusion, but I can't visit websites or fetch real-time data from external sources, including specific transcripts from sites like the Changelog. If you can provide me with excerpts or details from the podcast transcript you're interested in, I'd be happy to help you summarize it!",,,"1. Models
2. Transformers Math
3. Datasets 101
4. Finetuning
5. Hardware
6. Multimodal
7. Open Source
8. AI Engineer Career
9. Benchmarks 101
10. Coding Tools",Models,Transformers Math,Datasets 101,Finetuning,Hardware,Multimodal,Open Source,AI Engineer Career,Benchmarks 101,Coding Tools,0.022474487099999996,0.0,0.1
[Cognitive Revolution] The Tiny Model Revolution with Ronen Eldan and Yuanzhi Li of Microsoft Research,https://www.latent.space/p/cogrev-tinystories,Nathan Labenz,"Sat, 01 Jul 2023 21:13:47 GMT",7525,https://substackcdn.com/feed/podcast/1084089/post/132399331/ae0044838a837332af3911e00dfdfc75.jpg,https://api.substack.com/feed/podcast/132399331/47ca48f934434093f800f28530bc5f20.mp3,"Thanks to the over 1m people that have checked out  the Rise of the AI Engineer . It’s a long July 4 weekend in the US, and we’re celebrating with a podcast feed swap! We’ve been big fans of Nathan Labenz and Erik Torenberg’s work at  the Cognitive Revolution podcast  for a while, which started around the same time as we did and has done an incredible job of hosting discussions with top researchers and thinkers in the field, with a wide range of topics across  computer vision  (a special focus thanks to Nathan’s work at Waymark),  GPT-4  (with exceptional insight due to Nathan’s time on the GPT-4 “ red team ”), healthcare/medicine/biotech ( Harvard Medical School ,  Med-PaLM ,  Tanishq Abraham ,  Neal Khosla ), investing and tech strategy ( Sarah Guo, Elad Gil ,  Emad Mostaque, Sam Lessin ),  safety  and  policy ,  curators  and  influencers  and exceptional AI founders ( Josh Browder ,  Eugenia Kuyda ,  Flo Crivello ,  Suhail Doshi ,  Jungwon Byun ,  Raza Habib ,  Mahmoud Felfel ,  Andrew Feldman ,  Matt Welsh ,  Anton Troynikov ,  Aravind Srinivas ).  If Latent Space is for  AI Engineers , then Cognitive Revolution covers the much broader field of AI in tech, business and society at large, with a longer runtime to go deep on research papers like TinyStories. We hope you love this episode as much as we do, and check out CogRev wherever fine podcasts are sold! Subscribe to the Cognitive Revolution on: *  Website *  Apple Podcasts *  Spotify *  Youtube Good Data is All You Need The work of Ronen and Yuanzhi echoes a broader theme emerging in the midgame of 2023:  * Falcon-40B (trained on 1T tokens)  outperformed  LLaMA-65B (trained on 1.4T tokens), primarily due to  the RefinedWeb Dataset  that runs CommonCrawl through extensive preprocessing and cleaning in their MacroData Refinement pipeline.  * UC Berkeley LMSYS’s Vicuna-13B is  near GPT-3.5/Bard quality  at a tenth of their size, thanks to fine-tuning from 70k user-highlighted ChatGPT conversations (indicating some amount of quality).  *  Replit’s finetuned 2.7B model outperforms the 12B OpenAI Codex model  based on HumanEval, thanks to high quality data from Replit users The path to smaller models leans on better data (and tokenization!), whether from cleaning, from user feedback, or from  synthetic data generation , i.e. finetuning high quality on outputs from larger models. TinyStories and Phi-1 are the strongest new entries in that line of work, and we hope you’ll pick through the show notes to read up further. Show Notes * TinyStories (Apr 2023) * Paper:  TinyStories: How Small Can Language Models Be and Still Speak Coherent English? *  Internal presentation with Sebastien Bubeck at MSR *  Twitter thread from Ronen Eldan * Will future LLMs be based almost entirely on synthetic training data? In a new paper, we introduce TinyStories, a dataset of short stories generated by GPT-3.5&4. We use it to train tiny LMs (< 10M params) that produce fluent stories and exhibit reasoning. * Phi-1 (Jun 2023) * Paper:  Textbooks are all you need  ( HN discussion ) *  Twitter announcement from Sebastien Bubeck : * phi-1 achieves 51% on HumanEval w. only 1.3B parameters & 7B tokens training dataset and 8 A100s x 4 days =  800 A100-hours . Any other >50% HumanEval model is >1000x bigger (e.g., WizardCoder from last week is 10x in model size and 100x in dataset size).   Get full access to Latent Space at  www.latent.space/subscrib",,"To provide you with a summary, I'll need the actual content or details of the podcast you're referring to. The character ""e"" does not give me enough context or information. Could you please provide more details about the podcast or specific aspects of it you're interested in summarizing?",,,"1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10.",,,,,,,,,,10.,0.0,0.0,0.0
Commoditizing the Petaflop — with George Hotz of the tiny corp,https://www.latent.space/p/geohot,Alessio + swyx,"Tue, 20 Jun 2023 15:34:04 GMT",4361,https://substackcdn.com/feed/podcast/1084089/post/129414110/7acb8580a39a6f4cff1e08393c475ab9.jpg,https://api.substack.com/feed/podcast/129414110/89b9293d799ec4592ce76d5e9afd346b.mp3,"We are now launching our dedicated new  YouTube  and  Twitter ! Any help in amplifying our podcast would be greatly appreciated, and of course, tell your friends!  Notable followon discussions collected on  Twitter ,  Reddit ,  Reddit ,  Reddit ,  HN , and  HN . Please don’t obsess too much over the GPT4 discussion as it is mostly rumor; we spent much more time on tinybox/tinygrad on which George is the foremost authority! We are excited to share the world’s first interview with  George Hotz  on  the tiny corp ! If you don’t know  George , he was the first person to unlock the iPhone, jailbreak the PS3, went on to start  Comma.ai , and briefly “interned” at the Elon Musk-run Twitter.  Tinycorp is the company behind the deep learning framework  tinygrad , as well as the recently announced  tinybox , a new $15,000 “ luxury AI computer ” aimed at local model training and inference, aka your “personal compute cluster”: * 738 FP16 TFLOPS * 144 GB GPU RAM * 5.76 TB/s RAM bandwidth * 30 GB/s model load bandwidth (big llama loads in around 4 seconds) * AMD EPYC CPU * 1600W (one 120V outlet) * Runs 65B FP16 LLaMA out of the box (using tinygrad, subject to software development risks) (In the episode, we also talked about the future of the tinybox as the intelligence center of every home that will help run models, at-home robots, and more. Make sure to check the timestamps 👀 ) The tiny corp manifesto There are three main theses to tinycorp: *  If XLA/PrimTorch are CISC, tinygrad is RISC:  CISC (Complex Instruction Set Computing) are more complex instruction sets where a single instruction can execute many low-level operations. RISC (Reduced Instruction Set Computing) are smaller, and only let you execute a single low-level operation per instruction, leading to faster and more efficient instruction execution. If you’ve used the Apple Silicon M1/M2, AMD Ryzen, or Raspberry Pi, you’ve used a RISC computer. *  If you can’t write a fast ML framework for GPU, you can’t write one for your own chip:  there are many “AI chips” companies out there, and they all started from taping the chip. Some of them like  Cerebras  are still building, while others like Graphcore  seem to be struggling . But building chips with higher TFLOPS isn’t enough: “ There’s a great chip already on the market. For $999, you get a 123 TFLOP card with 24 GB of 960 GB/s RAM. This is the best FLOPS per dollar today, and yet…nobody in ML uses it. ”, referring to the  AMD RX 7900 XTX . NVIDIA’s lead is not only thanks to high-performing cards, but also thanks to a great developer platform in CUDA. Starting with the chip development rather than the dev toolkit is much more cost-intensive, so  tinycorp is starting by writing a framework  for off-the-shelf hardware rather than taping their own chip.   *  Turing completeness considered harmful:  Once you call in to Turing complete kernels, you can  no longer reason  about their behavior. Since they have to be able to execute any instruction, they are much more complex. To optimize Turing kernels performance, you fall back to caching, warp scheduling, and branch prediction. Since neural networks only need ADD/MUL operations and only rely on static memory accesses, there’s no need to have Turing completeness. This design decision allows tinygrad to optimize instructions at a much lower level. As you might have guessed, CUDA is Turing-complete; this is one of the main differences that tinycorp wants to leverage to be competitive.  All that — covered in the first 10 minutes of our discussion. George came ready to go deep, so we went for it. Some of the other technical questions we went through: *  Laziness:  why laziness is important and how operation fusing can help with memory efficiency *  Debugging & CI : Why great developer experience is a priority in tinygrad *  Quantization:  what’s the right level of quantization, how lossless are these transformations, his quick takes on Mojo and ggml, and why fp16 is the target for their out-of-the-box LLaMA.  *  Building rigs for individual use:  we talked a bit about the design tradeoffs of building these machines with low noise and a single power plug, the difference that PCIe 4 vs 3 makes, and more. The “personal compute cluster” is $15,000, but for businesses interested in local training and inference, George also estimates that he will be able to build you a H100-class GPU that is 5-10x faster (than a H100) for the same price. Misc: Bitter Lessons, Core Insights, Remote Work Outside of tiny, we also talked about one of George’s favorite units of measure “ a person of compute ”. Much of the AGI talk has been benchmark-driven, but looking at it from a compute throughput can also be interesting.  One person of compute is roughly 20 PFLOPS  (64 A100s, or a single dense 42U A100 rack); one A100 is ~$10-15,000, so the GPUs by themselves will come out at  $640,000-$1,000,000 .  We also covered a wide range of topics, including his self analysis on GPT-4, Elon Musk, Remote Work,  Computer Vision  and  the Comma Body , and  life above/below the API  (and  above/below the Kanban board ). See show notes and timestamps for more! Show Notes * “ Unlocked iPhone Traded for Nissan 350Z ” *  “Unlocked iPhone”  on YouTube (August 21st, 2007) *  “The Light It Up Contest”  on YouTube (February 13th, 2011) *  Comma.ai *  NHTSA cease and desist *  The Hero’s Journey *  The Portal Story *  A Person of Compute *  Above / Below the API Line  ( swyx take ) *  The Bitter Lesson *  The Goddess of Everything Else  ( listen to George read it ) *  Meditations on Moloch * George’s email to Lisa Su, AMD’s CEO:",* [00:00:00] Intros & tinygrad’s “Portal Story” * [00:03:00] Thesis #1 * [00:03:50] Thesis #2 * [00:05:00] Thesis #3 + Turing completeness discussion * [00:10:00] tinygrad’s creation and core ideas * [00:16:00] Operation fusing in tinygrad * [00:17:00] Debugging & profiling in tinygrad * [00:18:30] Tinygrad vs Pytorch competitiveness * [00:20:30] geohot vs AMD * [00:25:00] On ggml * [00:26:00] Tinygrad’s CI philosophy * [00:26:30] On Mojo * [00:28:00] ggml quantization is made up * [00:31:00] Work for tiny: benchmark int8 vs fp16 * [00:33:00] Why you can’t build tinybox - Design constraints * [00:35:00] The Personal Compute Cluster * [00:37:00]  Shoutout to our MosaicML podcast * [00:39:00] FLOPcoin and other use cases for the tinybox * [00:43:00] Rumors on GPT-4 architecture * [00:46:00] The Bitter Lesson * [00:48:00] Hiring and Changing mind on remote work * [00:52:00] Above/Below The API * [00:55:40] Comma Bodies &  Computer Vision * [00:58:40] Merging with the machine and AI girlfriends * [01:02:00] Is AI gonna kill us all? * [01:09:00] Why Avatar 2 was bad,"In this episode of the Latent Space podcast, Swyx and Alessio interview George Hotz (Geohot), discussing his notable achievements and the journey from hacking the iPhone and PS3 to founding Comma.ai. Despite setbacks from government regulations, George views these as opportunities for creativity and assurance of customer satisfaction, emphasizing the mindset of selling ""dev kits"" as a way to bypass traditional constraints.

Geohot explores the significance of the ""Hero's Journey"" and ""portal stories,"" drawing parallels with his own venture into TinyGrad as an engineer's pursuit of liberation and self-empowerment in machine learning compute. He emphasizes the critical need for alternatives to centralized control over ML compute resources, spurred by concerns over potential governmental restrictions on access to ML hardware. George's TinyCorp is envisioned as an organization that not only collaborates with industry giants like NVIDIA and Qualcomm but also potentially creates its own chips, ensuring open access to ML compute.

The conversation delves into the technical underpinnings of TinyGrad, highlighting its focus on simplicity, efficiency, and direct engagement with hardware capabilities. George critiques the complexity of existing AI chips and ML frameworks, advocating for a streamlined, risk-based approach to machine learning models. By prioritizing performant interoperability over excessive intricacy, George seeks to democratize ML development and application.

This discourse extends to broader themes of AI development, touching on the potential of AI to drive both technical innovation and significant socio-economic transformations. The interview explores the conceptual and practical elements of creating accessible, efficient AI technologies, emphasizing the importance of open development, practical innovation, and the balance between ambition and ethical responsibility in the AI domain.","1. The importance of TinyCorp focusing on structurally ensuring accessibility of ML compute against potential governmental restrictions on chip accessibility.
2. TinyCorp's approach to making ML chips available to anyone, working directly with NVIDIA for training chips and Qualcomm for inference chips.
3. The analogy between TinyGrad being a RISC offset for ML models, aiming to reduce the complexity compared to XLA and PrimeTorch by about 10x.
4. The emphasis on the challenge of writing a performant ML framework for GPUs as a prerequisite for creating one for custom hardware, considering the difficulty in making custom chips surpass the usability and performance of existing solutions like NVIDIA's.
5. Turing completeness in computing architectures being viewed as harmful and unnecessary for neural networks due to their predictable and repetitive computational patterns which do not require complex control flow or speculative execution.
6. Potential drawbacks of systolic arrays in TPUs, despite their efficiency for specific matrix operations, suggesting alternatives that might offer better general performance for AI computations.
7. The evolution and comparison of complex and reduced instruction set computing (CISC and RISC) in the context of developing simpler, more efficient approaches to ML model execution.
8. The concept of laziness in programming models for ML frameworks, aiming for operation fusion to optimize performance without compromising developer convenience.
9. The potential of TinyGrad to make ONNX (Open Neural Network Exchange) more efficient and compliant, highlighting ONNX's role in enabling model interoperability across different ML frameworks.
10. Insights into the process and philosophy behind optimizing ML computations, including the fusion of operations to eliminate unnecessary memory operations, thereby increasing computational efficiency.
11. Discussion around the practical challenges and potential in designing and manufacturing high-performance, yet accessible and efficient computer hardware for AI research and development.","Given your request for actionable quotes from the transcript that might impact the thinking or behavior of an AI Engineer, here are some selections:

1. **George Hotz on the significance of device labeling (dev kit vs. consumer product) [00:01:01]**
   - ""What's the difference between a dev kit and not a dev kit? Nothing. Just the question of do you think it's for you? And if you think it's for you, buy it.""

2. **George Hotz on decision-making for starting TinyCorp [00:02:03]**
   - ""Seeing Sam Altman go in front of Congress... It's really important that I do this.""

3. **George Hotz on the future of AI hardware [00:02:48]**
   - ""I'd like to start another organization that eventually in the limit, either works with people to make chips or makes chips itself and makes them available to anybody.""

4. **Discussion on RISC architecture for ML [00:03:08]**
   - ""We are going to make a RISC offset for all ML models... about 10x less complex.""

5. **George Hotz on writing ML frameworks for GPUs [00:04:20]**
   - ""If you can't write your own ML framework that is performant on NVIDIA, there's no way you're going to make it performant on your stuff.""

6. **Questioning the need for Turing completeness [00:05:25]**
   - ""Turing completeness is just really easy to just, oh, you know, it would just be so nice if I could do like an if statement here and actually branch the code, right? So it requires a lot more thought to do it without Turing Completeness.""

7. **George Hotz on deducing the number of operations [00:17:58]**
   - ""Just type debug equals two in any TinyGrad model, and it will show you all the kernels that it launches and the efficiency of each kernel, basically.""

8. **George Hotz on optimizing AI development workflow [00:20:27]**
   - ""But what actually is practical is changing cache hierarchies and removing branch predictors and removing warp schedulers... You're getting the worst possible worlds. You're getting the same laziness, but you also can't get fusion, because PyTorch doesn't know that I'm then going to do plus C.""

9. **Discussion on open-source culture and software development [00:24:36]**
    - ""You can, like, build in public. And they're like, what's an example of building in public? I'm like, go look at PyTorch.""

Each of these quotes illuminates crucial insights or reveals Hotz's thinking on subjects ranging from the practicality of development kits, the inception and mission of TinyCorp, to the architectural choices for ML models, criticisms of existing processes, and the role of open-source culture in innovation. They encapsulate valuable perspectives for AI engineers considering the future direction of AI hardware, software frameworks, and the broader ecosystem of AI development.","1. AI Engineer Career
2. Models
3. Coding Tools
4. Hardware
5. Open Source
6. Startups
7. Agents
8. News
9. Transformers Math
10. Multimodal",AI Engineer Career,Models,Coding Tools,Hardware,Open Source,Startups,Agents,News,Transformers Math,Multimodal,-0.2224744871,0.4449489742,0.1
"Emergency Pod: OpenAI's new Functions API, 75% Price Drop, 4x Context Length (w/ Alex Volkov, Simon Willison, Riley Goodside, Joshua Lochner, Stefania Druga, Eric Elliott, Mayo Oshin et al)",https://www.latent.space/p/function-agents,"Simon Willison, Alex Volkov, and Riley Goodside","Wed, 14 Jun 2023 17:50:29 GMT",5292,https://substackcdn.com/feed/podcast/1084089/post/128128854/510b789f1105bd7055e3796dbf61262e.jpg,https://api.substack.com/feed/podcast/128128854/10c005df9486c64d2f1a79a9afa77ae0.mp3,Full Transcript and show notes:  https://www.latent.space/p/function-agents?sd=pf,,"The podcast from Latent Space focused on the evolving landscape of AI and programming, particularly discussing the significant updates and developments in June 2023. It covered a range of topics starting from known issues with long context handling in AI models to the introduction of the new Functions API. Notable guests such as Riley Goodside, Simon Willison, and Eric Elliott shared their insights on various aspects of this advancements.

Key points discussed include the comparison between the Functions API and Google Vertex, highlighting the Functions API's approach to converting natural language into code and vice versa. An exciting development mentioned was the drop in embedding price and perspectives from Pinecone, Xenova, and Huggingface on the recent changes.

The podcast further delved into how the Function Selection process facilitates the design of code agents using the Functions API and how traditional models serve as routers in this new ecosystem. It touched on the shift from prompt engineering to fine-tuning models for specific tasks and the emergence of two paradigms involving code and large language models (LLMs).

Looking towards the future, discussions covered the anticipated role of smaller models and the evolution of the GPT API, emphasizing the need for security improvements to address prompt injection issues. The introduction of JSONformer was noted as a significant upgrade.

In closing, the speakers expressed their desires for future developments in the AI field, reflecting on the rapid advancements and the potential changes they hope to see in the coming months and years.","1. Introduction of the Functions API, offering a more efficient way to utilize models for specific tasks.
2. Comparison between Functions API and Google Vertex, highlighting different approaches to handling AI tasks.
3. Discussion on the transition from relying heavily on prompt engineering to more focus on fine-tuning models for specific applications.
4. Introduction of new paradigms: code generation with large language models (LLMs) and the utilization of small models for future development.
5. The evolution of the GPT API towards greater efficiency and functionality, especially in terms of handling code.
6. Security improvements in the Functions API to mitigate prompt injection vulnerabilities.
7. Insight into model upgrades and the development of JSONformer to enhance model interaction.
8. Anticipation of future enhancements and functionalities in AI platforms, emphasizing the community's expectations and desires.","[00:42:16] ""Models as routers is a concept where the actual intelligence of the operation is distributed across different models tailored for specific tasks, creating a more efficient and specialized AI system.""

[00:46:48] ""Prompt engineering replaced by finetuning suggests a significant shift in how we approach AI model training, indicating that fine-tuning models on specific datasets could lead to more relevant and accurate outcomes than generic prompt engineering.""

[01:03:27] ""Functions API Security vs Prompt Injection highlights the critical balance between making AI systems flexible and powerful through functional APIs and safeguarding against malicious inputs, which is paramount for the development of secure AI applications.""","1. Agents
2. Finetuning
3. Models
4. LLM Tooling
5. Coding Tools
6. AI Engineer Career
7. Hardware
8. Open Source
9. Multimodal
10. Startups",Agents,Finetuning,Models,LLM Tooling,Coding Tools,AI Engineer Career,Hardware,Open Source,Multimodal,Startups,0.1224744871,0.2224744871,0.1
From RLHF to RLHB: The Case for Learning from Human Behavior - with Jeffrey Wang and Joe Reeve of Amplitude,https://www.latent.space/p/amplitude,Jeffrey Wang and Joe Reeve,"Thu, 08 Jun 2023 20:00:00 GMT",2969,https://substackcdn.com/feed/podcast/1084089/post/126747491/5eebd14257199cc56d08e60bf7527116.jpg,https://api.substack.com/feed/podcast/126747491/f9234256ee791775fcf937ea3ce9eef5.mp3,"Welcome to the almost 3k latent space explorers that joined us last month! We’re holding our  first SF listener meetup with  Practical AI  next Monday; join us if you want to  meet past guests  and put faces to voices!  All events are in /community . Who among you regularly click the ubiquitous 👍 /👎 buttons in ChatGPT/Bard/etc? Anyone? I don’t see any hands up. OpenAI has  told us  how important reinforcement learning from human feedback (RLHF) is to creating the magic that is ChatGPT, but we know from  our conversation with Databricks’ Mike Conover  just how hard it is to get just 15,000 pieces of explicit, high quality human responses.  We are shockingly reliant on good human feedback.  Andrej Karpathy’s  recent keynote at Microsoft Build on the State of GPT  demonstrated just how much of the training process relies on contractors to supply the millions of items of human feedback needed to make a ChatGPT-quality LLM (highlighted by us in red): But the collection of good feedback is an incredibly messy problem.  First of all, if you have contractors paid by the datapoint, they are incentivized to blast through as many as possible without much thought. So you hire  more  contractors and double, maybe triple, your costs.  Ok, you say, lets recruit missionaries, not mercenaries.  People should volunteer their data! Then you run into the same problem we and any consumer review platform run into - the vast majority of people send nothing at all, and those who do are disproportionately representing negative reactions. More subtle problems emerge when you try to capture subjective human responses - the reason that ChatGPT responses tend to be inhumanly verbose, is because humans have a well documented “ longer = better ” bias  when classifying responses in a “laboratory setting”. The fix for this, of course, is to get out of the lab and  learn from real human  behavior , not artificially constructed human feedback. You don’t see a thumbs up/down button in GitHub Copilot nor  Codeium  nor  Codium . Instead, they  work an implicit accept/reject event into the product workflow , such that you cannot help  but  to give feedback while you use the product. This way you hear from  all  your users, in their  natural environments  doing valuable tasks they are familiar with. The prototypal example in this is  Midjourney , who unobtrusively collect 1 of 9 types of feedback from every user as part of their workflow, in exchange for much faster first draft image generations: The best known public example of AI product telemetry is in the  Copilot-Explorer writeup , which checks for the presence of generated code after 15-600 second intervals, which enables GitHub to claim that  40% of code is generated by Copilot . This is fantastic and “ obviously”  the future of productized AI.  Every AI application should figure out how to learn from all their real users , not some contractors in a foreign country. Most prompt engineers and prompt engineering tooling also tend to focus on pre-production prototyping, but could also benefit from  A/B testing their prompts in the real world . In short,  AI may need Analytics more than Analytics needs AI . Amplitude’s Month of AI This is why Amplitude is  going hard on AI  - and why we recently spent a weekend talking to  Jeffrey Wang , cofounder and chief architect at Amplitude, and  Joe Reeve , head of AI, recording a live episode at the  AI + Product Hackathon  where 150+ hackers gathered to compete for over $22.5k in prizes from Amplitude, New Relic, LanceDB, AWS, and more. To put things in perspective, Amplitude is a legendary YC alum with $238M of revenue in 2022 — our first guests representing the AI efforts of a  public company ! We chatted about how they have been approaching AI in their product (“question to chart” BI, text field autofill, instrumenting Amplitude with Amplitude), some of the issues they’ve had with different models, and the importance of first-party data in the world of LLMs. Another topic that came out of the Q&A was this idea of almost an “AmplitudeGPT”; rather than using language to simply generate a query, you could have these models investigate reasons for why certain behavior is happening in your user base. It was a really good discussion, and hope you all enjoy listening to it!   Sections * [00:00:47] Amplitude's founding story and pivot * [00:03:28] Amplitude as an AI company and opportunities * [00:07:14] Limitations and challenges with using AI models * [00:10:56] Using Amplitude's product to build Amplitude - instrumenting AI * [00:12:32] Existing ML models in Amplitude's product and customer use cases * [00:15:50] “A/Z testing” and adaptable products * [00:19:33] The future of analytics and dashboards * [00:21:03] Optimizing for metrics in chatbots and AI products * [00:26:22] Using general models vs. fine-tuned models * [00:30:24] The importance of models vs. data - Amplitude's data set * [00:39:00] Lightning Round + Q&A Show Notes *  Amplitude *  Sonalight to Amplitude pivot announcement *  The Slack origin story *  Reverse Engineering Copilot *  Simon Willison’s blog Transcript Editor’s note: all timestamps are 1 minute behind because we hadn’t yet added the intro before making these. Sorry about that! Alessio:  Thank you everyone for coming. Hopefully, some of you have listened to the podcast before, if you haven't, we focus on AI research and application. So we don't focus on “AI is going to kill us all”. We don't think about virtual girlfriends. We don't think about all of these more societal things. We're focused on models: how do you build them? How do you train them? How do you use them in production? What are some of the limitations on getting these things from demos to things that millions of users use? And obviously, a lot of you are building things. Otherwise, you wouldn't be here. And some of you have been building things for a long time, and now have a new paradigm that you want to build on top of. So I'm excited to dive in here. And maybe, I mean, I'm sure most people know you, but maybe you want to do intros and give a little background. [00:00:47] Jeffrey:  Sure. Yeah, hey, everyone, met you all this morning, but I'm Jeffrey. I'm one of the co-founders and Chief Architect here at Amplitude. Been working on this product analytics thing, helping people understand user behavior data and make great product decisions and build better products for the last decade or so. And obviously, AI is a technology that we've been leveraging for a long time, but the recent trends are particularly exciting. And yeah, we have a lot of thoughts on how to apply that to our space, what we're doing in our product, and what we think the future of AI and product development and product data is. So excited to talk through some of those. [00:01:20] Joe:  Yeah, I'm Joe, Joe Reeve. I've got a background in sort of startups and tech, been professional software engineer since I was 16, quit college. And at the moment, I'm running sort of AI R&D efforts here at Amplitude. Super excited about all the new stuff, but also all the stuff that Amplitude's been doing for a long time and how we're sort of getting renewed interest and excitement and abilities to push that even further forwards. [00:01:44] Swyx:  So I think it's useful for people listening on the podcast and also some people here. Can you contextualize Amplitude as an AI company? Like what does that mean to you? What unique opportunities do you guys have? [00:02:02] Jeffrey:  Sure, yeah, happy to speak to that. So, you know, if we think about the fundamental thing that our customers of Amplitude try to do, it's they want to look at their product data and they want to figure out how do I make my product better? And the really cool thing about product data is that one, it's often like very high fidelity, right? Digital products compared to, you know, let's say physical products before them have way more information about what's going on. And so that's why product data is, you know, even a thing at all, right? You finally have that feedback loop of, hey, I built this thing. This is how people are using it. Now let me learn from that and make my product better. Now, one of the downsides of that is that the data is massive. If you look at any of the internet scale products out there, they generate enormous amounts of data. And the ability of humans to kind of sift through that data is obviously limited. At Amplitude, we try to give people as many tools, whether AI or not, in order to process that. But at the end of the day, if you could get from the data and what user behavior is happening in your product to the insights of how to make your product better without as much manual work, that's kind of the holy grail of product analytics. And so in some sense, Amplitude has always been a company on the path to AI because figuring out how to make your product better from data is ultimately an AI problem. And so we're kind of just solving all the barriers in the way, like getting data in first, building good models for short-term things. And long-term, it's always been about, hey, how can you take product data and automatically make your product better as fast as possible? [00:03:28] Alessio:  So that's the future of Amplitude. And a lot of people here probably want to start companies and whatnot. So maybe you want to give a 60 seconds of why you started Amplitude and what the story was like and maybe the first three to six months, what the challenges were. [00:03:42] Jeffrey:  Yeah, of course. It's funny that we talk about this because the start of Amplitude is actually almost more AI than the current state. And so actually my two co-founders, Spencer and Curtis, they went through YC originally with not Amplitude, but SonaLite, which was a text-by-voice company. So it was kind of before the era of Siri and those types of technologies where they wanted to build something that would read text messages to them, that's easy, but also do voice recognition so that you could send text messages, say when you're driving, without having to pull out your phone. And so they worked on it and it was really popular back when they were doing it. After they finished YC, they realized the big innovation that they needed to figure out in order to make that successful was being really good at voice recognition, which was a different problem. They're awesome software engineers, but they don't come from an ML background. And so it's like, okay, are we going to spend the next five years solving voice recognition? Not really the thing that they had in mind when they were building product. But one thing that they happened to stumble upon as they were working on that was they spent a lot of time thinking about, hey, what was hard about that product? What made users churn? What made users really love it and engage? And they built a bunch of analytics tools to help them understand that. And they were really kind of shocked that those tools didn't exist out there in the market or they were like much more primitive than they wanted. And it turns out a bunch of other people in their YC batch felt the same. And they were like, hey, that analytics thing you're building, we want that. For you to text by voice, we want your analytics product. And so they're like, okay, fine. We will pivot, natural language and voice recognition isn't really our thing. And so we'll do distributed systems and analytics instead. That's where I came in. I'm a distributed systems and analytics guy. And so I happened to get in touch with them just through some mutual friends at the time. And then, yeah, we kind of went on it. The funny thing about a lot of things in technology is that the most forward thinking companies with respect to a lot of technologies are gaming companies. And so a lot of AmpliG's early start was either gaming companies or companies with founders that came from gaming backgrounds, where in gaming people have always been very, very rigorous about product data and optimizing engagement loops and all of that. And so they look for the best tools. We went to Zynga 15 years ago. It's like, that's where product analytics originated. And so a lot of those founders of new startups who had left Zynga were like, hey, that thing that you're building, that's trying to figure out patterns and user data and use that to make better products. That is exactly what we want after leaving Zynga. And then from there, that was Amplitude. Swyx:  Yeah, I think famously other gaming companies would be like Slack, right? Mr. Butterfield tried to make a gaming company and failed and made Flickr. Then he tried to make another gaming company and failed and made Slack. And now look out to see what he does next. Discord as well. That's right. [00:06:34] Jeffrey:  Yeah, people who come from gaming backgrounds are very rigorous in their product thinking. [00:06:39] Swyx:  That's interesting. Alessio, you have a background in games? [00:06:43] Alessio:  Yeah, in playing them, not in building them. So I will not fall into an enterprise company by doing that. Let's talk about R&D today and some of the ideas that you're working through, like some of the limitations that you run through. I think the most interesting thing about hackathons is you come with an idea and then you kind of hit a wall trying to build it. And then that takes you into another path. Like what are maybe funny things that you learn in terms of like the limitations of these models or like the missing infrastructure for using them? [00:07:14]  Joe:  So we've got a couple of different frames for thinking about this. There's AI that we're putting into our products and then us knowing that our customers want to put AI into their products. So there's the, how do we support our customers in their product development using AI? But how do we do that ourselves? And this is a great opportunity for us to learn the challenges our customers are gonna see. And so the first thing there is let's just start from the beginning, assume we want to add AI to our product, which maybe isn't the best place to start, but let's just assume we want to. How do we start ideating opportunities to put stuff into our product? So we sort of came up with this framework where we look at our product and we think about what are the collaboration touch points? So where are the points that a human might hand off to another human? And then think where can we replace one of those humans with the machine? So instead of thinking of some AI, amorphous AI, LLM, whatever, we're thinking actually, what if we had a robot that we were collaborating, not just a human, not just some sort of thing that spits out numbers. So collaborating. Then there's thinking of these as tools. So this is like your auto-suggest, on your mobile keyboard or spell check or something. How do you integrate this stuff as deeply into your product? So what are the friction points that users go through? Maybe they check lots of boxes. Is there a way we can pre-check those boxes we can get? So that's the feature embedding really deeply into the tool you've already got, the product you've already got. And then you step back and think, okay, what's a tool? So a tool is like ChatGPT, where you go there, it's an AI powered tool. It's not necessarily connected to your product, but it's a supplementary tool that you add. So there's a sort of ideation process there that we went through. And we sort of landed on a couple. And one of the key things that Amplitude does is help our customers, one, collect data in like a standard and sort of queryable way. And then we help them query it and get insights out of that data. So we were thinking, what's the feature there? How do we embed that? But also what's the collaboration point? And you might be a product manager asking an analyst, hey, please help me. Let's have a conversation about this. I don't know what questions to ask, but you also might just be about to go click the big create button and fill in a bunch of fields. And can we fill in a bunch of the fields for you? So we went to what to us seemed like one of the most obvious places. And we built a text box. Surprise, surprise with LLMs. We've got a text box. You can type in a question, type in anything about your data that you want to know, and then it'll spit back a chart, which is kind of neat. And we hit a bunch of problems there with LLMs hallucinating, losing context, even within the context windows, not really sort of recalling everything within the context window. So we sort of did a bunch of experimentation and realized if we split this down to seven different questions, so instead of saying, generate me a chart and a query for this one question, let's split that into lots of sub queries, like what kinds of events should I use? How should I display this? What should I call it? Rather than asking you all of that in one go. But then we had another problem where we have one query that a user makes that actually spins out seven different queries. So how do we monitor this? We can't just say one performance metric. You know, RLHF, you can't just say yes or no. Was the query response good? Because it might've failed for one of seven reasons. And maybe multiple of them failed or maybe some of them failed and then maybe they've hallucinated. And so we're getting code errors where an enum is not being matched. So we've had lots of sort of issues going all the way down there that we've had to figure out from first principles and sort of a really exciting way for us to understand what our customers are going through. [00:10:56]  Swyx:  So I wanna be clear. So you've described your exploration and how you think about products. What have you released so far? I just wanna get an idea of what has been shipped. [00:11:08] Joe:  Sure. So in terms of LLM stuff, this, we call it question to chart internally. This ask a question, get a chart out. This, we've started rolling out to customers already. So last week, actually, started rolling out to our AI design partners a sign that we had signed up, which is a really exciting process. Actually, a lot of customers are just so excited to work with us and try it out and see how they can break it. So that's something we rolled out recently, which is built in LLM. It's the first piece built on LLM that we're working on. But we've also had a bunch of long-term ML, sort of traditional ML models that we've been running and products that we've been running with customers that help them predict what their users are gonna do. Because we've got this massive behavioral data set, best behavioral data set in the world. So we can train these awesome models and help our customers predict what their users are gonna do. So they can share the more relevant content or now is the right time to ask people  if they want to upgrade or they want to rate your app or that sort of thing. [00:12:05] Swyx:  Yeah, there is a little bit of a contrast, conflicts, because you already had all these ML models in-house and you're spinning up a new AI team and you're like, no, let's do all of this with GPT-3. Are the existing ML researchers saying like, no, this is a complete misuse of text generation? Or are they excited about it? Is it unlocking new things? [00:12:32] Joe:  Yeah, actually, it's the combining these things. So we're able to use the traditional ML to shorten the fields, to narrow the number of things we need to pass into the LLMs. Because the LLMs can do a lot more of the reasoning, but we can make sure that the context we're providing is much more specific and generally much better by using the traditional ML models. [00:12:53] Swyx:  Yeah, okay. And then the pain points that you're experiencing are hallucination. And then also like the multi-query thing. What do you think you wish for? Or what do you think you're thinking about to solve those pain points? [00:13:06] Joe:  So right now we're instrumenting with our own product. So we're instrumenting groups of inferences and individual inferences, which means we can then create charts that show how often they fail, why they fail, how often we need to retry to get good answers.  Swyx:  So amplitude using amplitude. [00:13:23]  Joe:  Exactly. To build amplitude. [00:13:24] Swyx:  Yeah, exactly. [00:13:25]  Joe:  Well, I mean, we're a product company. What else would we do? [00:13:29] Swyx:  That is the second part of what you're saying, right? Which is, first of all, you want AI in the amplitude products. Second, people are shipping AI products with amplitude. You wanna talk a little bit more about what you're seeing there? [00:13:39] Joe:  Yeah. I guess the key thing here is, for a lot of people is, okay, I can build the thing that calls OpenAI's API and then gives a response back. I'm nervous that I'm gonna be giving incorrect answers. I'm nervous that I don't really know how to measure whether the answers are incorrect. And I'm nervous that I'm not gonna be able to improve over time. So a lot of people we actually hear are nervous of giving thumbs up, thumbs down buttons because they're implying to their users that they're gonna be using this to improve the results. But they actually have no idea how to use that to improve the results in a meaningful way. And particularly when you've got multiple queries going off for one request, you've gotta then fine tune lots of different things in parallel. So it gets to be quite a technically complex sort of problem if you're not using great tooling that already exists for it. So that's, and then you have the extra layer of, I'm getting a bad result. I've tweaked my prompt template that I'm sending off to OpenAI. And now, has the result got better or worse? [00:14:35] Swyx:  I don't know. [00:14:36] Joe:  I don't know how to measure that. Except by thumbs up, thumbs down, which is a difficult measure in the first place. So that's where we can start saying, measuring the behavior of users once we've generated something for them. So have they gone and shared this content? Have they used this content? They actually gotten any value out of it? Not just have they pressed thumbs up. We can actually measure, are they getting value? Are they throwing it away from their behavior? But then using that through the Amplitude product, we can then tie that through to A-B tests, which is another product that Amplitude has. So then suddenly we start, and we're not doing this yet. This is sort of next on our list, is to start putting these prompts into our A-B test variants. So then we make a tweak in the UI, and it goes off, fires on the original, the control and our variant, our new variant. See, does it get fewer or more errors? Does it get fewer or more thumbs up, thumbs down? [00:15:30] Alessio:  Have you thought about, I don't know, A-Z testing, I guess? Like one of the limitations has been, well, people can only write so much copywrite to test, but now with these generative models, you can actually generate a lot of copy. And like you go to on-demand test more and more and more copy. Have you seen any maybe fun customer stories? Like can you, anything there? [00:15:50] Jeffrey:  Yeah, so actually there's a very good example of this. I don't know if I can share the actual customer, but actually from before the LLM days, where they literally generated the versions of the copy themselves, and they made their product basically adapt, you know, multi-arm bandit style of like, hey, here's all these different variations, like just go figure out the best one. At an internal hackathon, maybe two months ago, I built a prototype of what you're talking about, which is, okay, now replace the copy generation with an LLM. So just constantly generating new variations, and then multi-arm banditing to figure out which one's the best. I think that is probably the future of copywriting, where it's like, you don't actually need a whole lot of manual work anymore. It can, almost everything can happen automatically. And it's kind of the micro example in my head of this concept that we really like, which is self-improving products, where, you know, at some point, you know, someone has to say, hey, I'm gonna build a product that does this, you know, like a newsreader or something. But then, you know, after you have that, like the title of the newsreader, like the description of the sections, your navigation, all of that, in theory, you know, if you can give it some structure that the AI can play with, the LLM can manipulate all of that for you, and then use, you know, A-B testing, multi-arm bandits and all of that to kind of figure out what's best. And that generative AI kind of makes that last piece of like, what are my options possible? And that's super exciting for us. And we wanna be there, you know, to help you measure that, help you deploy that, and make that like the way people build products in the future. [00:17:14] Alessio:  I think I've talked about this on the podcast, but this idea of like just-in-time UIs, you know, like each type of user wants to interact in a different way. And like, what you're building is a way of that, right? Like, Amplitude has been really like dashboard-driven, kind of like a diagram-driven, showing the user flow. Now each user can say, hey, I don't really want the table. I just want the charts. Or like, I don't want the charts. I just want the data. What do you think about the future of like dashboards and like BI in general? But like, the analysts used to come up with like what you should be seeing. Now each user can ask their own questions. [00:17:47] Jeffrey:  Yeah, like the future of analytics, I think, is, you know, can go a few different paths. One thing that I want to, you know, counter against the whole LLM trend a little bit is I think when you get into really important and specific questions, you know, let's say you're writing like some complicated SQL or even code, you know, code and SQL are good because they're very specific, right? You can define your semantics very precisely. And that's something that I think, you know, when people start thinking about like natural language questions, they kind of take for granted. They're like, oh yeah, why doesn't it just, you know, figure out the precise semantics from my very ambiguous words? It's like, well, it's actually, in some senses it's possible, right? Because the precise semantics are not captured by your ambiguous natural language words. And so the way we think about it, at least today, you know, who knows what's going to change in the future is like natural language is a great interface to like get started. If you don't know what the underlying data looks like, if you don't know like what questions you should be asking, it is a very, very expressive way to start, get started. It's much easier than manipulating a bunch of things, much, much easier than writing SQL and all of that. But like once you kind of know what you want, it's very hard to like make it precise. It's actually easier to make SQL or code precise than it is natural language. And so that's a little bit of what we're thinking right now. So we think, you know, for sure the way that maybe many people will interface with analytics and data will turn into natural language because maybe the precision doesn't matter to them. But like at the end of the day, when you're trying to get, you're trying to sum up your revenue or something, it's like, you want to know that it's right. And you want to know the semantics that go into that. And like, that's why, you know, that's part of why data is hard. The semantics really do matter. They can make a huge difference in the output. And so there's a boundary there that I'm curious where it will push over time, but I don't think it's quite there yet. [00:19:33] Joe:  I think this is where models sort of can become more embedded as features rather than go off and do this thing, create this analysis for me and then come back, the collaborator model. Then we're saying this field, I'm not sure what should go in there. Can you make a suggestion? And then I'm going to go and refine it over time. So it's the sort of autofill, but guessing autofill, but then you still, you can tweak everything. This is one of the core design sort of principles that we've come up is yes, you've got to be able to explain what the model's doing. And as a human, I need to understand, a user I need to understand what is the model doing and why is it doing it? But I also need to be able to tweak it once it's done it. I don't want to feel like I've just said go and then I can't stop it and it's going to go off and do stuff. And that's sometimes how things like AutoGPT can feel. It's going and it's costing me OpenAI tokens and I have no idea what's going on. So yeah, I think a key thing is servicing all the individual things the model's doing and allowing users to tweak it, stop it, retry while it's going. [00:20:33] Swyx:  For me, one of the most challenging questions is something I think you guys have maybe thought about a lot which is chat. Ideally you want, like you could say naively, for example, you want to optimize time in app, but actually that's a sign of failure if the chat session is longer than it should be. Do you have any advice on, I'm sure you've dealt with this before pre AI era, but like what do you advise AI hackers to optimize for? Like what analytics should people be looking at? [00:21:03] Jeffrey:  Yeah, our general kind of philosophy as a company is to work with customers to identify north star metrics. Right, and like time in app is not good primarily because it doesn't actually correlate with your business outcomes most of the time. And to be fair, sometimes it does. Like if you're a social media app, maybe it does correlate really well and maybe it's not a bad metric then. But for a lot of other products, right, if you're trying to do the search, for example, or like time on search, like nobody wants that. It's like, yeah, what is your success rate? You know, how many, do you get them to come back and search in the future? Like that's much more interesting than the time of your session. And so, because you know, each time you can serve apps, right, that's your business. And so it's like, if you choose a metric that's well correlated with your business outcomes, then that's at least the first step to getting that right and not getting caught up in other vanity metrics that sound like they could be good to increase, but then, you know, they can sometimes lead to negative business outcomes, you know, and then you get the worst. You've optimized the wrong metric the whole time. And that's where tying in AI and product analytics makes a lot of sense. And it's really important because product analytics, these companies that are like our customers that are trying out building features that are LMs and they're not sure what to optimize for, optimize for the same thing you're already optimizing for. You're already measuring conversions. You're measuring how much value, hopefully, your customers are getting out of your product. So continue doing that and maybe find a way to tie the LLM feature to that and sort of through A-B tests and that sort of thing. And then on the chat specifically, chat is obviously for a business maybe rolling out a chat box based on LLMs. It can be really scary. And that's another sort of mental model of framing we've been thinking around is we find LLMs right now are most useful either when you come from, either when you have a narrow input space and a broad output space, because you can be very, you know exactly what format of data, what kind of data is gonna be passed in. That's probably not coming directly from a user. It's probably coming from a button click or a toggle switch or something. And then you can have a general output and you can provide templates and that sort of thing. And then the other way is broad input space, narrow output space. So that's free form text box. And you can provide a bunch of sort of clamping, framing, validation on the output to make sure that you're not spewing out, you know, poems about Hitler or whatever it is. You know, you can be really, really deliberate when you've got a small output space. Chat is large input space, large output space, which is really, really scary. If you're, as a company, you're not selling a chat product, you're selling a, you know, an analytics product with maybe a chat support bot or something. [00:23:37] Swyx:  Yeah, I think this is one of those opportunities. I always try to raise the awareness of this, that Copilot I think did a really interesting metric or North Star, which was how much code is kept or retained by the user. And for people who are Googling along, you can actually look for this blog post about reverse engineering Copilot internals. And they actually set up custom metrics around, you know, 30 seconds after a code snippet is accepted, one minute, two minute, three minute, all the way to five minutes. And you can sort of see it construct a curve of how long Copilot suggestions stick around. And from there, they can actually make statements like this, you know, evaluate the success of the products. It's pretty cool. [00:24:18] Joe:  One of the really nice things we found actually, we accidentally did this. So our chart building interface, heavily instrumented. It's a, we're Amplitude. So we instrument our product. We also, it's one of the main tools that our customers use. So it's really, really well instrumented. And so when we tied chart creation through asking a question through an LLM, and then we tied that to a chart, an output chart, we then automatically were able to tie every time someone edits any of the parameters to that generation. So then we know, we have really detailed RLHF data for, yeah, you got everything apart from the metric, right? But you got everything apart from this event that shouldn't have been there, because that's the one that got removed. So similar to the Copilot there. [00:25:00] Alessio:  And I want to make sure we open it up for questions, but like one last thing is about, everybody knows that small is beautiful. And when you think about what models to use and some of the parameters, like there's costs, there's latency, there's like accuracy. How do you think about using, you know, GPT-4 and some of those models versus using smaller ones that are fine-tuned? What are the trade-offs? [00:25:23] Joe:  Yeah, I guess right now we're very much in the, let's explore, let's try everything and just iterate as fast as possible, which is what general models are great for. We do have some smaller, not even fine-tuned, some smaller models that we've sort of borrowed from Hugging Face that we run internally for more specific tasks. And that's often sort of selecting specific values before we pass it to a general model right now, just because the general models are much easier to communicate with and they understand most of the words we use. It's not like we use a word and suddenly we get random outputs for no reason, the sort of gold magic up type thing. So they're generally less susceptible to that. So that's why we're iterating heavily on the general models. I think we absolutely have to move to some more specific models, particularly given inference on fine-tuned open AI models gets more expensive and slower the more you do it. So yeah, that's definitely a thing we're looking at and we're doing some internal stuff, but it's the next step or one of the next steps. [00:26:22] Jeffrey:  Yeah, to give a pseudo example of that, one of the hard things to help users within Amplitude is picking the right event to analyze. It's kind of your fundamental unit of analysis. And when a user comes in and let's say that's the first time they're using Amplitudes, someone else in their company has set up the product, so they don't know what the events are. Right now in Amplitude you get this massive dropdown and it's like, all right, there's a thousand things, like which one is the one I'm looking for. And sometimes the names are good and sometimes they're not. But one thing we did was, okay, yeah, feed that into open AI. Hey, tell me which event type best matches like this user's intent. That's like pretty good at that, right? So it's all language stuff, but it's a little bit slow and it's a little bit expensive to do that every time. And so we kind of fell back to, once we validated that that works, kind of fell back to a more traditional embedding-based approach. It's like, all right, compute all those embeddings. That's more work upfront because you have to go through your database of all of these things and you got to commit like that engineering work, but it's like you validate with the general model because it's just easy. It takes like an hour to figure out that it works. And then it's like, all right, can we do the same thing with embeddings? That's way faster, way cheaper and still has reasonable quality. Embeddings also have a nice quality that you can get like magnitude of things, whereas LLMs aren't great at giving you like, hey, it matches this much. It's kind of, you can ask it for an order and that's decent, but like, yeah, anything beyond that is pretty challenging. [00:27:42] Alessio:  How do you think about the importance of the model versus the data, right? There's like a lot of companies that have a lot of data, but not a lot of AI expertise or companies that are just using off the shelf model. How should companies think about how much data to collect? What data is meaningful? What isn't, any thoughts there? [00:27:59] Jeffrey:  Yeah, I think it's safe to say that both are really important, right? Like the evolution of LLMs really was a lot of model innovation. And so I don't want to downplay that. At the same time, I think the future of AI applications and doing really cool things with it will be in the data, partially because like, you know, ChatGPT has done such a huge advance, right? The LLMs model space has advanced like crazy in the last year. And so I think a lot of the untapped potential will be in data in the future. One thing that's particularly interesting to us is like we have a pretty unique data set, actually. It's a lot of first party behavior data, right? So if you're, you know, if you're Square, for example, you instrumented like the way that people interact with Square Cash and the wallet and the, you know, the checkout system. And like, those are very specific things. Like Square can't look elsewhere in the world for that stuff. And that's really interesting because, you know, to build models of user behavior, you need user behavior data. And it turns out there's not actually a lot of examples of user behavior data out there in the world. And so to Joy's point earlier about, you know, we have one of the best user behavior data sets in the world. And so if we want to build a model around that, I think it would be a super interesting one. So if you take an analogy to what ChatGPT does, it basically takes a bunch of language examples and it, you know, learns a bunch of abstract concepts, like how to, you know, prove math things or how to render in JavaScript. It's like, wow, that's very astonishing. They kind of prove, it's almost like a proof of concept to the world that if you train a sufficiently good, you know, transformer self-attention type model with a sufficiently large data set of, you know, hundreds of gigabytes of internet text, you'll learn really interesting abstract concepts. And so we want to apply that to our data set, right? Cat GPG is great because it's a proof of concept. If it didn't exist, you know, I would have told you, yeah, you can spend $10 million training this model on a data set, you'd probably not get anything interesting because we just have no idea. But because it exists, it kind of proves to the world that if you do this correctly, there is a ton of interesting value. And so that's what I think. And so, you know, amplitude is just one example of a very interesting data set that you will train something that's, you know, fundamentally very different from GPT or any LLM out there. And there's lots of other data sets out there. And I think that's where a lot of the interesting things will come once this kind of, this phase of like rapid model evolution kind of tapers out a little bit. And you'll see a lot of the more interesting applications there. [00:30:24] Swyx:  So I've never thought about this much, but you guys must do it a lot. Like what is the ethics or best practices around training on user data when they don't know they're being watched? Like, I mean, presumably they're fine with tracking and events, but like, do we tell them that we're going to train on their data? Is it okay?  [00:30:50] Joe:  I guess there are a couple of things. One is PII. Doesn't go anywhere near the stuff, right? PII with strip and like, that's just a really important thing. [00:30:58] Swyx:  You still need an identifier for streams. [00:31:02] Joe:  Yeah, yeah. But in terms of training models, we don't want any of that to go in there because then you might accidentally, you know, like, hello, ChatGPT, please hallucinate me a social security number. That's dangerous. [00:31:11] Swyx:  Also PII makes it into prompts a lot. [00:31:14] Joe:  Sure, that's true. So then you have to strip that from your... So we have some experiments where we're stripping PII that is in places that shouldn't be, you know, descriptions of things. Sometimes people copy paste big long lists of email addresses into charts and things. But some of these things are actually pretty surprisingly easy to detect and strip out. So we can do that. And we have some layers that are stripping out that sort of replacing them with tokens. So the LLMs can still operate on them. But in terms of training this data, all that training is happening internally and we're not putting any sort of private data, personally identifiable information in. I don't know if there's anything you wanted to add there. Yeah, yeah. [00:31:54] Jeffrey:  We certainly think about this a lot and our customers think about a lot. Like when I think about user privacy with respect to tracking, there's kind of this big spectrum. Around the one end, it's like literally track nothing and, you know, the end of story. And like for people like that, I mean, that's cool. You know, they're not gonna use Amplitude. They may not like us very much. You know, that is what it is. And then on the other end of the spectrum is like, we're gonna track you across the entire internet and sell your data to everyone. And like, that's obviously bad. And like, there's lots of good reasons to think that's bad. First party behavioral data, I think is actually probably almost as far. Fully anonymized first party behavior data would be like kind of the minimum. It's like web server logs with no IP, no identifier, nothing. The problem is that you can't do a lot of interesting behavioral analysis without that. You can't tell if, you know, this person that came on this day was the same one that purchased later. And so like, you can't actually, it's much harder to make your product better if you don't have that. And so, you know, we're kind of set at this place where we have, you know, like pseudo anonymized first party data. And like, we don't sell the data. You don't mix data from, you know, different places on the internet through Facebook cookies or things like that. And, you know, our philosophy is like, that is actually the most important data to build a better product. It's not the most important data to advertise, which is why Facebook and Google do what they do, but it's the most important data to build a better products. And it kind of strikes the right balance between yeah, totally tracking everything that you're doing and like not having any information to make your product better. [00:33:19] Swyx:  Yeah, cool. And I think we're going to go to audience questions. So let's start warming them up soon. But I think we have some lightning round questions  [00:33:29] Joe:  The audience is thinking of questions while we go. [00:33:31] Alessio:  The first one is, what's something that already happened in AI that you thought would take much longer to be here? [00:33:39]  Jeffrey:  I don't know what the constraints on our lightning round, but I think maybe creativity is the best word where it's, you know, with the image generation stuff, text generation, you know, one thing that still blows my mind, I used to be a competitive like math guy and like there's this international math Olympiad problem in one of the papers and it solves it. And I'm just like, wow, I can solve this when I was spending all my life doing this thing. Like that level of creativity really blew my mind. And what's the takeaway? It's like maybe the takeaway is that creativity is not as, you know, as not as high entropy or high dimensional as we think it is, which is kind of interesting takeaway. But yeah, that one definitely surprised me. [00:34:21] Joe:  I guess there's something actually that maybe answering the inverse question that a lot of my friends were surprised happened quickly. And I was like, this is just braindead obvious. I've got a lot of friends in the AI safety space. So they're worried that in particular, X-risk, right, extinction risk, that AI is going to kill the human race. And they were like, oh no, what if an AI escapes containment and gets access to the internet? And then we get an LLM and the first thing we do is like, hey, also GPT, here's the internet. [00:34:48] Swyx:  So you thought, it's happening faster than you thought. [00:34:53] Joe:  Well, it's happening faster than, to me it makes sense, because I'm like one of the guys connecting it to the internet. And I'm like, I'm surprised that other people were surprised it was going to be so fast. [00:35:01] Swyx:  Yeah, so a bit of context, Joe and I, we've been adjacent to the EA community and they have like smoothly migrated to the X-risk community very quickly after SBF. [00:35:13] Joe:  Yeah, after SBF, yeah, that was fun. [00:35:16] Swyx:  Okay, so next question, exploration. What do you think is the most interesting unsolved question in AI? What's next? [00:35:30] Joe:  I guess like, is it going to keep getting better at the same rate? Is it going to, and that's just a super important question that's going to change. Like, depending on that answer, 50 startups are going to pivot or not pivot, right? [00:35:43] Swyx:  Which is what's next, literally. [00:35:45] Joe:  Literally, what's next? Like in a year's time, are the models similarly better than they have been so far? Or are we about to taper off or are we about to continue going linearly? [00:35:58] Jeffrey:  Yeah, I'll throw one out that is not necessarily about AI, but like, what's intelligence, right? And if you ask people 20, 30 years ago, maybe even longer now, it's like, yeah, chess. Chess is intelligence. And then chess got solved and like, ah, that's just brute force. And it's like, well, you know, creating creative images and writing, that's intelligence. Well, it's like, that's solved too. Maybe it's just, you know, if you have enough parameters, you can capture that. So like, what is intelligence? What does it mean to have an AGI? What does that actually mean? And then what the implications that are on for our understanding of humans and our brains. I've always thought that, you know, everyone is just a stochastic machine. And so, you know, is everything consistent in my mind?  Swyx:  Free will and illusion. Exactly. [00:36:43] Joe:  I guess maybe like the scaling piece is like that intelligence as you scale is gets more and more expensive on the traditional stuff. But then there's something I think I saw yesterday on Hacker News. It was people actually getting a brain to play tic-tac-toe. Like by a brain, I mean, stem cells grown into brain tissue. And they were able to train it. And like that to me is very significant because suddenly the like metal computers limitations is not applied. And then now we've got all this intelligence. What is intelligence stuff on a squishy wet computer? That makes it even harder to ask and even harder to draw lines. [00:37:18] Swyx:  Yeah. Yeah. So famously, you know, language models are so much more inefficient than wet computers, as you say. And so if you can merge that, you know, the human brain runs on 30 Watts of power as it is my favorite fact. We're not anywhere close to that yet. [00:37:36] Alessio:  Before we get into Q&A, one last takeaway that you want everybody to think about. [00:37:41] Jeffrey:  Yeah, I'll do the one that we actually repeat in Inside Amplitude very often, not about AI, but I think it applies, which is it's early. It's sometimes hard to realize that when things are happening so fast, especially in the Bay Area, but like the ramifications of AI or in our case, product data and all that are gonna play out over the next many decades. And that's just, you know, we're very fortunate to be at the beginning of it. And so yeah, take advantage of it and keep reminding yourself that it's early. [00:38:15] Joe:  I guess mine would be, let humans be good at doing human things. Let machines be good at doing machine things and let machines be good at doing machine things and help humans be good at doing human things. And like, if you don't do that, then you're gonna be building something that's either not useful or it's very scary. So yeah, get machines helping humans, not the other way around. [00:38:39] Swyx:  Get machines helping humans. All right. With that, I think we're all gonna open up to questions. We're gonna toss you the mic. [00:38:45] Audience #1 : Yeah, hey, thanks for the insight into how you guys implemented your AI, you know, question asking chatbot and how have you converted into seven sub queries and then generate the data out. I've just, I got a peak my interest about how you guys exactly do it. Like Alessio asked, like, what exactly is the model that you guys are using? Are you converting it into your, what are these queries that you generate from a single English language? Is it possible to go a little deeper just from a curiosity perspective? [00:46:34] Joe:  So we have a custom query engine. So it's not SQL or anything that we're generating. We're generating a custom query output. So I guess the types of questions range. So things like chart type, are we doing a segmentation chart, a line chart or are we doing a funnel chart? You know, the number goes down over time or up over time or between a conversion between two events and there are various other types or metrics or, and then there's also the name. What should we name this chart that answers this question? So the way that's implemented in practice, you could use something like Lang chain to sort of chain these things together. But in our experience, I think Lang chain's a great tool for certain things and definitely really great for prototyping, but we found it quite restrictive. So we've ended up building sort of an internal, it's a very, very small wrapper, internal, we use TypeScript as well, framework that allows us to basically just write in code and infer within what we call a transaction, an inference transaction, which gets monitored as one, but then also all the individual inferences within it get monitored. So it's a bit like when you're writing a database transaction with most sort of, at least in the node ecosystem, the JavaScript ecosystem, where you sort of get a transaction object that you can operate on, and then you return your, or you return, you sort of commit your transaction. So we've got an interface like that, so we can just write pure TypeScript, await this response or await these responses. And then we've got a switch case. If it's a segmentation chart, go and do these with these queries. And then each of those inferences can be a different model. So we think in the future, maybe we have one query where we have some GPT-4 responses. We want some text responses. Maybe we also want to generate an image from that same query together, and then that gets bundled. So I don't know if that answers your question.  Audience #1:  Yeah, I think so. Yeah, thank you. I think so. You said in future, you're going to use GPT-4. What are you using right now for? [00:48:33] Joe:  Right now, everything's GPT-3.5. We're moving around, and I think probably for some of the prompts, we'll use something like DaVinci. Some we might use GPT-4. Some we'll be using internal ones. And we also want to be able to degrade gracefully if a customer has told us they don't want us to send anything to OpenAI, then we can degrade to some internal models that maybe are some of the open source models that have been trained on smaller datasets. [00:48:57]  Audience #1:  Gotcha, makes sense. Thank you. [00:48:58] Jeffrey:  Yeah, I think to add to that a little bit, the key is breaking down the problem sufficiently, because if you break down the problem enough, you can also provide it with some examples, which is super helpful, right? You know, GPT is quite good at zero shot, but within the context of our specific domain, it doesn't know what's going on. And so being able to break down the problem to, hey, select the type of chart. Don't generate me an entire chart definition. Select me the type of chart, and then select me the specific metric based on their query, and then giving it some examples. Select me the events and properties that I want to look at. By breaking it down and having very, very contextual prompts with respect to those examples, you get a lot higher quality output than trying to generate, like, you know, if you imagine generate, like, hey, generate me a whole SQL query with all, you know, here's like the schema of all my tables, now generate it entirely. It's like, it actually struggles with stuff like that, because it's just like kind of too much information and computation to come out of language. Now, maybe GPT-5 will be different, but like, that's the state of the art today. [00:49:57] Swyx:  I'll ask a follow-up to Joe. So you mentioned, you mentioned trying LangChain, but not needing it for production. Any other comments on tooling that are out there that's interesting to you? Do you use a embedding database, for example, or do you just use a regular database? [00:50:18] Joe:  Yeah, so we've actually been running embedding sort of similarity or vector search in production for multiple months, maybe even almost a year, and just like straight up Postgres, but now we're using PG Vector, which actually Jeffrey could probably speak more to about that decision and what that was like. [00:50:40] Swyx:  So this is a pretty hot take. At Amplitude scale, all you need is Postgres? [00:50:46] Joe:  We'd use many things other than Postgres. But I mean, we, this isn't rolled out for all customers and it's not necessarily getting sort of hit with a lot of traffic. And so the scale here is very different. Our usage scale is very different to our ingestion. [00:51:04] Swyx:  Yeah, yeah, yeah. [00:51:06] Jeffrey:  Just to clarify that a little bit more, we're not putting individual end user vectors or end event vectors. We're putting in taxonomies. So if I'm DoorDash, my taxonomy is add to cart, checkout, purchase, browse. That's the cardinality. And so that's actually small. It's on the order of tens of millions. And so yeah, you use stuff that in Postgres, no problem. Now, when we talk about large behavioral models or like actually embedding events, there are many, many trillions of those. And yeah, Postgres probably doesn't work there. [00:51:41] Swyx:  Yeah, actually I wanted to comment on this slightly before, which is separating taxonomies from the actual data is one way you protect your customers against prompt injection. It's something that Simon Willison has been talking about where you want to have like query for one thing, but essentially no knowledge of the actual underlying data, just the taxonomy. So it's good practice. [00:52:00] Audience #2 : Yeah, so you talked about a model which would be trained on user behavior data like amplitude GPT. It really piqued my interest and what capabilities would emerge? What do you think that you would find and what would be the first thing you would ask the model? That's a good question. [00:52:23] Jeffrey:  So we've thought about this a little bit and I think the, right, these are sequence, token prediction models. And so at the very least, I would hope for a much better, we have a predictions feature right now, which says, hey, given what a user has done over the last 90 days, do we think they're gonna belong to this cohort in the future or not? So that cohort might be people who churn, people who purchase, people who upsell, whatever the customer wants. We think it would be much better at tasks like that, right, because if it just has a very good understanding of behavioral patterns and what's gonna come next, it would be able to do that. That's exciting, but not that exciting. If I'm trying to think about like the analogies to what we see in LLMs, it's like, okay, yeah, what is the behavioral equivalent of like learning physics concepts, right? It's like, oh, I don't actually know, but it might be this understanding of patterns of sessions and how that like, for example, categorizing users in a unsupervised way seems like a very simple output for a model that understands user behavior, right? Here's all the users and if you wanna discriminate them by their ability to achieve some outcome in the future, like here's the best way to separate that group and here's why, right? Be able to explain at that level and that would be super powerful for customers, right? A lot of times what our customers do is, hey, these people came back the next day and these people didn't, why? What was different about them? And so we have a bunch of heuristics to do that, but at the end, there's something like, causal impact is like one of the holy grails of product analytics. It's like, what was the causation behind some observed difference in behavior? And I think, yeah, a large behavioral model will be much better at assessing that and be able to give you potentially interpretable ways of answering that question that are like really hard to do, really hard, really computationally intensive, really like noisy, distilling causation correlation is obviously super hard. Those are some of the examples. The other one that I am, I don't know if I'm optimistic about it, but we really interesting is, one of the things that amplitude requires today is manual instrumentation, right? You have to decide, hey, this clicking of a button, this viewing of page, these are important things. I'm naming them in this way. There's a lot of popular tools out there that kind of just record user sessions or like track DOM events automatically. There's a lot of problems with those tools because the data is incredibly noisy. It's just so noisy, right? A lot of times you just can't actually interpret it. And so it's like, oh, it's great because I don't need to do any work. But like, well, you also don't get anything out of it. It's possible that a behavioral model would be able to actually understand what's going on there by understanding your user behavior in a correctly modeled and correctly labeled sense, and then figuring out. I don't know if that's possible. I think that would make everyone's lives a lot easier if you could somehow ask behavioral questions of data without having to instrument. All of our customers would love that, but also all of them are instrumenting because they know that's definitely not possible today. [00:55:26] Audience #2 : This is really interesting. You're looking forward to the future. If you're gonna build it, it's gonna be amazing, yeah. [00:55:31] Jeffrey:  That's the goal, that's the goal. [00:55:33] Audience #2 : Awesome. [00:55:34] Swyx:  Thanks for listening. [00:56:09]   Get full access to Latent Space at  www.latent.space/subscrib",,"The podcast featured a discussion focused on AI research and application, specifically in the realm of AI in product analytics, and the journey and future direction of Amplitude, a company at the forefront of leveraging AI for understanding user behavior and improving products. Key points include:

- **Alessio** highlighted the podcast's focus on AI research and applications, emphasizing models, their construction, training, and deployment.
- **Jeffrey**, co-founder and Chief Architect at Amplitude, discussed leveraging AI for product analytics to understand user behavior and make informed product decisions, emphasizing the significant potential in recent AI trends for product development.
- **Joe Reeve**, involved in AI R&D at Amplitude, talked about the excitement surrounding new AI advancements and Amplitude's longstanding efforts in pushing analytics and product improvement through AI.
- **Amplitude's Position as an AI Company**: Jeffrey outlined Amplitude's unique position in leveraging high-fidelity product data for improving products through analytics, hinting at AI's pivotal role in processing massive amounts of data for product enhancement.
- **Origins of Amplitude**: Jeffrey shared Amplitude's origins in voice recognition and transition to focusing on analytics and distributed systems, highlighting the early adoption by gaming companies due to their rigorous data-driven approach.
- The conversation explored R&D challenges, particularly in integrating AI into products and dealing with limitations of models, infrastructure, and model hallucinations. Amplitude's endeavor to automate insights from user data into actionable product improvements was discussed, including their approach of breaking down AI tasks into manageable queries to enhance efficiency and accuracy.
- The podcast also touched upon future directions and the ethical considerations of using user data for AI, emphasizing privacy and the potential of AI in understanding and predicting user behavior on a large scale.

In essence, the talk provided insights into Amplitude's journey, the practical challenges of applying AI in product analytics, and broader questions around AI's role in product development and ethical data use.","1. The development and implementation of AI in product development and product analytics to enhance decision-making and product improvement.
2. Amplitude’s transition from a voice recognition initiative to a focus on product analytics and building tools to understand user behavior.
3. The application of AI and ML at Amplitude to predict user behavior and improve product engagement.
4. Exploration into replacing traditional inputs and operations with AI to streamline processes and enhance product features.
5. Challenges associated with implementing large language models (LLMs) in production, such as hallucination, context retention, and multi-query management.
6. Strategies for utilizing traditional ML models to refine inputs for LLMs, enhancing efficiency and relevancy.
7. Introduction of a text box feature where users can ask questions and receive chart outputs, addressing LLM limitations by breaking down queries into sub-queries.
8. The potential of generative models for on-demand testing of a vast array of generated content to optimize product engagement.
9. The concept of self-improving products enabled by AI, particularly in the realm of dynamic and automatically optimized product features and interfaces.
10. Ethical considerations and best practices in using user behavior data for AI model training without compromising privacy.
11. The ongoing exploration of the most effective use cases for AI in product analytics, including the differentiation between using general LLMs and specialized models.
12. The impact of model advancements on future AI applications, particularly in understanding and enhancing user experiences with products.","**[00:38:15] Jeffrey:** ""Yeah, I'll do the one that we actually repeat in Inside Amplitude very often, not about AI, but I think it applies, which is it's early. It's sometimes hard to realize that when things are happening so fast, especially in the Bay Area, but like the ramifications of AI or in our case, product data and all that are gonna play out over the next many decades. And that's just, you know, we're very fortunate to be at the beginning of it. And so yeah, take advantage of it and keep reminding yourself that it's early.""

---

**[00:38:39] Joe:** ""I guess mine would be, let humans be good at doing human things. Let machines be good at doing machine things and let machines be good at doing machine things and help humans be good at doing human things. And like, if you don't do that, then you're gonna be building something that's either not useful or it's very scary. So yeah, get machines helping humans, not the other way around.""

---","1. Models
2. AI Engineer Career
3. UX
4. Agents
5. Coding Tools
6. LLM Tooling
7. Hardware
8. Multimodal
9. Finetuning
10. Startups",Models,AI Engineer Career,UX,Agents,Coding Tools,LLM Tooling,Hardware,Multimodal,Finetuning,Startups,0.1224744871,0.1224744871,0.3224744871
Building the AI × UX Scenius — with Linus Lee of Notion AI,https://www.latent.space/p/ai-interfaces-and-notion,Linus Lee,"Thu, 01 Jun 2023 18:53:34 GMT",4190,https://substackcdn.com/feed/podcast/1084089/post/125082085/d08e94b0c4865c4121c8c35c7c42d54c.jpg,https://api.substack.com/feed/podcast/125082085/7dae1e4168664caf814ade78b4146c64.mp3,Read:  https://www.latent.space/p/ai-interfaces-and-notion Show Notes *  Linus on Twitter *  Linus’ personal blog * Notion *  Notion AI *  Notion Projects *  AI UX Meetup Recap,"* [00:03:30] Starting the AI / UX community * [00:10:01] Most knowledge work is not text generation * [00:16:21] Finding the right constraints and interface for AI * [00:19:06] Linus' journey to working at Notion * [00:23:29] The importance of notations and interfaces * [00:26:07] Setting interface defaults and standards * [00:32:36] The challenges of designing AI agents * [00:39:43] Notion deep dive: “Blocks”, AI, and more * [00:51:00] Prompt engineering at Notion * [01:02:00] Lightning Round","In this episode of the Latent Space podcast, Alessio and Swyx welcome Linus, a notable figure in the AI industry currently working at Notion. Linus shares his personal journey from a curious student experimenting with technology at Berkeley to becoming a significant part of Notion's AI team. He provides insights into the aesthetics and fundamental operations at Notion, emphasizing the company's dedication to consistency and craftsmanship in their products and office spaces.

Linus delves deep into his extensive experience across various startups, highlighting his entrepreneurial spirit and his year of independent research focused on enhancing text interfaces with AI. This passion led him to Notion, where he now contributes to exploring how AI can improve workflow and productivity.

The podcast also touches on AIUX (AI and User Experience) and how it's becoming a crucial discipline in designing interactive and intuitive AI-powered products. Linus shares his perspective on prompt engineering within AI systems, stressing the importance of predictability and dependability in AI outputs. The discussion extends to the exploration of agents in AI, contemplating their potential to execute tasks more autonomously and efficiently.

Concluding, Linus reflects on the broader implications of AI and text interfaces for future human communication and productivity tools. He expresses a forward-looking hope that current technological limitations and conceptualizations will evolve significantly, advocating for innovation that transcends mere incremental improvements.

Throughout the conversation, the themes of innovation in AI, the significance of user interfaces, and the continuous evolution of how humans interact with technology are explored, providing valuable insights for anyone interested in the nexus of AI, UI/UX, and product development.","1. The growing interest and community around AI UX as a discipline combining AI and interface design.
2. The exploration of continuous control vs. distinct brushes in AI tools for text sentiment and other attributes.
3. Techniques in prompt engineering, particularly leveraging few-shot prompts for more predictable model outputs.
4. The challenges and strategies in building AI features that are multilingual or work across different languages.
5. The importance of predictability, reliability, and program synthesis in AI, particularly in professional settings.
6. The potential for AI to significantly evolve text interfaces and writing systems in the long term.","[00:08:14] Linus: ""So my projects actually tend to be... I think especially when people approach project building with a goal of learning, I think a common mistake is to be over-ambitious and sort of not scope things very tightly. And so a classic kind of failure mode is, you say, I'm really interested in learning how to use the GPT-4 API, and I'm also interested in vector databases, and I'm also interested in Next.js. And then you devise a project that's going to take many weeks, and you glue all these things together. And it could be a really cool idea, but then especially if you have a day job and other things that life throws you away, it's hard to actually get to a point where you can ship something. And so one of the things that I got really good at was saying, one, knowing exactly how quickly I could work, at least on the technologies that I knew well, and then only adding one new unknown thing to learn per project. So it may be that for this project, I'm going to learn how the embedding API works. Or for this project, I'm going to learn how to do vector stuff with PyTorch or something. And then I would scope things so that it fit in one chunk of time, like Friday night to Sunday night or something like that. And then I would scope the project so that I could ship something as much work as I could fit into a two-day period, so that at the end of that weekend, I could ship something. And then afterwards, if I want to add something, I have time to do it and a chance to do that. But it's already shipped, so there's already momentum, and people are using it, or I'm using it, and so there's a reason to continue building. So only adding one new unknown per project, I think, is a good trick.""

[00:10:01] Linus: ""I think the first time you look at something like GPT, the shape of the thing you see is like, oh, it's a thing that takes some input text and generates some output text. And so the easiest thing to build on top of that is a content generation tool. But I think there's a couple of other categories of things that you could build that are sort of progressively more useful and more interesting. And so besides content generation, which requires the minimum amount of wrapping around ChatGPT, the second tier up from that is things around knowledge, I think. So if you have, I mean, this is the hot thing with all these vector databases things going around. But if you have a lot of existing context around some knowledge about your company or about a field or all of the internet, you can use a language model as a way to search and understand things in it and combine and synthesize them. And that synthesis, I think, is useful. And at that point, I think the value that that unlocks, I think, is much greater than the value of content generation. Because most knowledge work, the artifact that you produce isn't actually about writing more words. Most knowledge work, the goal is to understand something, synthesize new things, or propose actions or other kinds of knowledge-to-knowledge tasks. And then the third category, I think, is automation. Which I think is sort of the thing that people are looking at most actively today, at least from my vantage point in the ecosystem. Things like the React prompting technique, and just in general, letting models propose actions or write code to accomplish tasks. That's also moving far beyond generating text to doing something more interesting.""","1. AI Engineer Career
2. Models
3. Multimodal
4. UX
5. Agents
6. Coding Tools
7. Hardware
8. Open Source
9. Startups
10. Event Recaps",AI Engineer Career,Models,Multimodal,UX,Agents,Coding Tools,Hardware,Open Source,Startups,Event Recaps,-0.5449489742,0.2224744871,0.0
Debugging the Internet with AI agents – with Itamar Friedman of Codium AI and AutoGPT,https://www.latent.space/p/codium-agents,Itamar Friedman,"Thu, 25 May 2023 12:43:04 GMT",3756,https://substackcdn.com/feed/podcast/1084089/post/123420423/f70db02efde9f599e5a040deabfaa4a2.jpg,https://api.substack.com/feed/podcast/123420423/7aca68590895b550b64bd2a1d54bce55.mp3,"We are hosting the AI World’s Fair in San Francisco on June 8th! You can  RSVP here . Come meet fellow builders, see amazing AI tech showcases at different booths around the venue, all mixed with elements of traditional fairs: live music, drinks, games, and food! We are also at Amplitude’s  AI x Product Hackathon  and are hosting our first joint Latent Space + Practical AI Podcast  Listener Meetup  next month! We are honored by the  rave reviews  for our last episode with MosaicML! They are also welcome on  Apple Podcasts  and Twitter/HN/LinkedIn/Mastodon etc! We recently spent a wonderful week with Itamar Friedman, visiting all the way from Tel Aviv in Israel:  * We first recorded a podcast (releasing with this newsletter) covering Codium AI, the hot new VSCode/Jetbrains IDE extension focused on test generation for Python and JS/TS, with plans for a Code Integrity Agent.  * Then we attended  Agent Weekend , where the founders of multiple AI/agent projects got together with a presentation from Toran Bruce Richards on Auto-GPT’s roadmap and then from Itamar on Codium’s roadmap * Then some of us stayed to take part in the NextGen Hackathon and  won first place  with the new  AI Maintainer  project. So… that makes it really hard to recap everything for you. But we’ll try! Podcast: Codium: Code Integrity with Zero Bugs When it launched in  2021 , there was  a   lot   of   skepticism  around Github Copilot.  Fast forward to  2023 , and 40% of all code is checked in unmodified from Copilot.   Codium burst on the scene this year, emerging from stealth with an  $11m seed , their own foundation model (TestGPT-1) and a vision to revolutionize coding by  2025 . You might have heard of ""DRY” programming (Don’t Repeat Yourself), which aims to replace repetition with abstraction. Itamar came on the pod to discuss their “extreme DRY” vision: if you already spent time writing a spec, why repeat yourself by writing the code for it? If the spec is thorough enough, automated agents could write the whole thing for you. Live Demo Video Section This is referenced in the podcast about 6 minutes in.",", show notes, and transcript are below the fold. We would really appreciate if you shared our pod with friends on  Twitter , LinkedIn, Mastodon, Bluesky, or your social media poison of choice! Auto-GPT: A Roadmap To The Future of Work Making his first public appearance, Toran (perhaps better known as @SigGravitas on GitHub) presented at Agents Weekend: Lightly edited notes for those who want a summary of the talk: *  What is AutoGPT? AutoGPT is an  Al agent  that utilizes a Large Language Model to drive its actions and decisions. It can be best described as a user sitting at a computer, planning and  interacting with the system  based on its  goals . Unlike traditional LLM applications, AutoGPT  does not require repeated prompting  by a human. Instead, it generates its own 'thoughts', criticizes its own strategy and decides what next actions to take. * AutoGPT was released on GitHub in March 2023, and  went viral on April 1  with a video showing automatic code generation.  2 months later it has 132k+ stars , is the  29th highest ranked open-source project of all-time , a thriving community of 37.5k+ Discord members, 1M+ downloads. *  What’s next for AutoGPT?  The initial release required users to know how to build and run a codebase. They  recently announced plans  for a web/desktop UI and mobile app to enable nontechnical/everyday users to use AutoGPT. They are also working on an extensible plugin ecosystem called the  Abilities Hub  also targeted at nontechnical users. *  Improving Efficacy . AutoGPT has many well documented cases where it trips up. Getting stuck in loops, using  instead of actual content in commands, and making obvious mistakes like execute_code(""write a cookbook""'. The plan is a new design called  Challenge Driven Development  - Challenges are goal-orientated tasks or problems that Auto-GPT has difficulty solving or has not yet been able to accomplish. These may include improving specific functionalities, enhancing the model's understanding of specific domains, or even developing new features that the current version of Auto-GPT lacks. (AI Maintainer was born out of one such challenge). Itamar compared this with Software 1.0 (Test Driven Development), and Software 2.0 (Dataset Driven Development). *  Self-Improvement . Auto-GPT will analyze its own codebase and contribute to its own improvement. AI Safety (aka  not-kill-everyone-ists ) people like  Connor Leahy  might freak out at this, but for what it’s worth we were pleasantly surprised to learn that Itamar and many other folks on the Auto-GPT team are equally concerned and mindful about x-risk as well. The overwhelming theme of Auto-GPT’s roadmap was  accessibility  - making AI Agents usable by all instead of the few. Podcast Timestamps * [00:00:00] Introductions * [00:01:30] Itamar’s background and previous startups * [00:03:30] Vision for Codium AI: reaching “zero bugs” * [00:06:00] Demo of Codium AI and how it works * [00:15:30] Building on VS Code vs JetBrains * [00:22:30] Future of software development and the role of developers * [00:27:00] The vision of integrating natural language, testing, and code * [00:30:00] Benchmarking AI models and choosing the right models for different tasks * [00:39:00] Codium AI spec generation and editing * [00:43:30] Reconciling differences in languages between specs, tests, and code * [00:52:30] The Israeli tech scene and startup culture * [01:03:00] Lightning Round Show Notes *  Codium AI *  Visualead *  AutoGPT *  StarCoder * TDD (Test-Driven Development) * AST (Abstract Syntax Tree) *  LangChain *  ICON *  AI21","In the Latent Space podcast episode, Alessio and Swyx host Itamar Friedman, CEO and co-founder of Codium AI, from Tel Aviv. Itamar shares his journey from his computer science education at Teknion Israel Institute of Technology to his entrepreneurial ventures, including the creation and successful sale of Visualead to Alibaba Group. His current venture, Codium AI, is centered around AI-driven code assistance aiming for zero bugs in development processes. Itamar details the inception of Codium AI, which arose from his extensive R&D management experience and the struggles with code logic verification. Codium AI, which recently raised $11 million in seed funding, offers AI-powered tools integrated into IDEs like VS Code and JetBrains to analyze code, generate tests, and suggest bug fixes or improvements in code quality. Itamar also discusses the evolution of software development, emphasizing the transition towards more intelligent and automated processes, and the potential of AI in transforming code integrity and testing. The discussion also touches upon the vibrant tech scene in Israel, the significance of AI alignment, and the importance of practical experimentation with AI technologies for innovators.","1. The development of Codium AI, an AI coding assistant/agent designed to reduce bugs in software development, offering analysis, tests, debugging, and improvements in code quality, especially for code logic testing.
2. Codium focuses on enhancing the traditional and LLM techniques by merging them for efficient static and dynamic code analysis, including mutation testing with AI's assistance to pinpoint optimal mutation locations.
3. Codium AI's approach to understanding developer's intention by analyzing natural, specification, technical, and even visual language to match code logic with provided specifications or natural language descriptions.
4. The introduction of the Codium AI IDE extension for VS Code and JetBrains, which analyzes code, generates test suites, and offers code suggestions for performance, modularity, maintainability, and potential bug fixes.
5. The ""DRY"" (Don't Repeat Yourself) vision for Codium, aiming to align code, spec, and tests through an extreme automation process, allowing developers to generate one from the other fluidly, enhancing development flexibility and efficiency.
6. The contrast between Codium AI's focus on specific, accurate AI agents for code integrity and the AutoGPT community's approach of creating a swarm of general agents for diverse tasks.
7. The acknowledgment of AI alignment as an unsolved and vital question in AI research, touching on both technical and philosophical implications regarding the nature and future applications of LLMs.
8. Codium's strategic position within the AI and ML tools ecosystem, exemplifying the potential overestimation of general AI capabilities in the short term while underestimating future advancements and encouraging hands-on experimentation for developers.","Itamar [00:39:00]: ""So, first thing I think there do I wanna say that they're gonna be coding assistance and coding agents. Assistant is like co-pilot, for example, and agents is something that you give it a goal or a task and actually chains a few tasks together to complete your goal. Let's have that in mind.""

Itamar [01:04:30]: ""If you're a builder, I really encourage you, speak less and do more play with it. Try it for specific use cases and see what's easy to do. And then if your purpose is just like incorporating stuff and that's what you wanna do and then do it, but don't like, tell everyone you're gonna do it before you do it, because you might find that it's actually really hard and there's a lot of problems.""","1. Models
2. Agents
3. LLM Tooling
4. AI Engineer Career
5. Coding Tools
6. Open Source
7. Finetuning
8. Hardware
9. Startups
10. News",Models,Agents,LLM Tooling,AI Engineer Career,Coding Tools,Open Source,Finetuning,Hardware,Startups,News,0.3224744871,0.2224744871,0.1
MPT-7B and The Beginning of Context=Infinity — with Jonathan Frankle and Abhinav Venigalla of MosaicML,https://www.latent.space/p/mosaic-mpt-7b,Alessio Fanelli and swyx & Alessio,"Sat, 20 May 2023 21:03:36 GMT",4003,https://substackcdn.com/feed/podcast/1084089/post/122567124/90578945febe7fb15d8b1ae9ca2c5149.jpg,https://api.substack.com/feed/podcast/122567124/6490b30865ec09f51496855a2fa0e862.mp3,"We are excited to be the  first  podcast in the world to release an in-depth interview on the new SOTA in commercially licensed open source models - MosiacML MPT-7B! The Latent Space crew will be at the NYC Lux AI Summit next week, and have two meetups in June. As usual, all events are on  the Community page ! We are also inviting beta testers for the upcoming  AI for Engineers  course. See you soon! One of GPT3’s biggest limitations is context length - you can only send it up to 4000 tokens (3k words, 6 pages) before it throws a hard error, requiring you to bring in LangChain and other retrieval techniques to process long documents and prompts. But MosaicML recently open sourced MPT-7B, the newest addition to their Foundation Series, with context length going up to 84,000 tokens (63k words, 126 pages): This transformer model, trained from scratch on 1 trillion tokens of text and code (compared to 300B for Pythia and OpenLLaMA, and 800B for StableLM), matches the quality of LLaMA-7B. It was trained on the MosaicML platform in 9.5 days on 440 GPUs with no human intervention, costing approximately $200,000. Unlike many open models, MPT-7B is licensed for commercial use and it’s optimized for fast training and inference through  FlashAttention  and  FasterTransformer . They also released 3 finetuned models starting from the base MPT-7B:  *  MPT-7B-Instruct : finetuned on  dolly_hhrlhf , a dataset built on top of dolly-5k ( see our Dolly episode for more details ).  *  MPT-7B-Chat :  finetuned on the  ShareGPT-Vicuna ,  HC3 ,  Alpaca ,  Helpful and Harmless , and  Evol-Instruct  datasets. *  MPT-7B-StoryWriter-65k+ : it was finetuned with  a context length of 65k tokens  on a filtered fiction subset of the  books3 dataset . While 65k is the advertised size, the team has gotten up to 84k tokens in response when running on a single node A100-80GB GPUs.  ALiBi  is the dark magic that makes this possible. Turns out  The Great Gatsby  is only about 68k tokens, so the team used the model to create new epilogues for it! On top of the model checkpoints, the team also open-sourced the entire codebase for pretraining, finetuning, and evaluating MPT via their new MosaicML  LLM Foundry . The table we showed above was created using LLM Foundry in-context-learning eval framework itself! In this episode, we chatted with the leads of MPT-7B at Mosaic:  Jonathan Frankle , Chief Scientist, and  Abhinav Venigalla , Research Scientist who spearheaded the MPT-7B training run. We talked about some of the innovations they’ve brought into the training process to remove the need for 2am on-call PagerDutys, why the LLM dataset mix is  such an important yet dark art , and why some of the traditional multiple-choice benchmarks might not be very helpful for the type of technology we are building. Show Notes *  Introducing MPT-7B *  Cerebras *  Lottery Ticket Hypothesis *  Hazy Research *  ALiBi *  Flash Attention *  FasterTransformer * List of naughty words for C4  https://twitter.com/code_star/status/1661386844250963972 *  What is Sparsity? *  Hungry Hungry Hippos *  BF16 FP p.s. yes, MPT-7B really is codenamed LLongboi!",* Introductions [00:00:00] * Intro to Mosaic [00:03:20] * Training and Creating the Models [00:05:45] * Data Choices and the Importance of Repetition [00:08:45] * The Central Question: What Mix of Data Sets Should You Use? [00:10:00] * Evaluation Challenges of LLMs [0:13:00] * Flash Attention [00:16:00] * Fine-tuning for Creativity [00:19:50] * Open Source Licenses and Ethical Considerations [00:23:00] * Training Stability Enhancement [00:25:15] * Data Readiness & Training Preparation [00:30:00] * Dynamic Real-time Model Evaluation [00:34:00] * Open Science for Affordable AI Research [00:36:00] * The Open Approach [00:40:15] * The Future of Mosaic [00:44:11] * Speed and Efficiency [00:48:01] * Trends and Transformers [00:54:00] * Lightning Round and Closing [1:00:55],"In this episode of the Latent Space podcast, hosts Alessio and Swyx delved into a comprehensive discussion with Jonathan and Abhi from Mosaic ML. The conversation kicked off with Jonathan's background, covering his transition from programming languages to machine learning (ML) during his PhD at MIT, where he made a significant impact with the lottery ticket hypothesis. Jonathan's myriad interests also touched on teaching programming to lawyers, exploring law and privacy, and cryptography. Abhi's journey through his BS and MS at MIT, his research at Cerebras, and his role at Mosaic were also highlighted, while discussing the advancements in computing platforms beyond GPUs and the importance of wafer-scale computing.

The duo also introduced Mosaic ML's work, particularly their spearheading the MPT-7B effort, reflecting on how developments like meta’s LLaMA models inspired them to release a high-quality 7 billion parameter model. Despite Mosaic not initially planning to build and release a model, the combination of community challenge, research curiosity, and the need to demonstrate their tools' efficacy led to their adaptation of this plan. The MPT-7B model, along with its novel training and inference capabilities, signifies a major stride in model efficiency and accessibility, developed under the premise of ""you can do this too,"" aiming to enable customers to create their own models using Mosaic's tools.

The conversation ventured into the realm of training stability, where Mosaic ML has charted significant progress, offering an end-to-end stable model training experience which was demonstrated with their Swift intervention and recovery techniques amidst hardware failures. This achievement underscores Mosaic ML's commitment to mitigating the usual challenges associated with large-scale model training, reflecting their broader goal of democratizing access to efficient ML model training and deployment.

In essence, the podcast encapsulated Mosaic ML’s efforts towards making ML model training more accessible and efficient through open-source contributions and innovations in training methodologies. Their journey underlines the blend of ambition and practical challenges in AI research, with a strong emphasis on the open science model for fostering collective progress in the field.","1. Efficiency in AI training and inference is a major area of innovation, with efforts to reduce the cost and speed up the process being crucial for broadening accessibility and enabling experimentation.
2. The importance of open research in AI, with Mosaic ML prioritizing sharing insights and innovations to foster a vibrant open-source research community, facilitating progress in the field.
3. The potential vast improvements in hardware and mathematical precision, such as FP8 precision and H100 GPUs, could significantly reduce AI training costs and enhance model performance.
4. AI model training is not a one-time cost, with the need for continual updates and improvements to models as new data becomes available and better techniques are developed.
5. Sparsity in AI models remains a challenging area due to current hardware limitations, despite its potential benefits in making models more efficient.
6. The emergence of language models with extensive context lengths (e.g., 65K+ tokens) and the continuous exploration of how to increase model context lengths for better performance.
7. The significance of data choices in training AI models, highlighting unresolved questions regarding optimal data mixes for training impactful and efficient models.
8. The ongoing importance and utility of Transformers in AI architecture, with skepticism about their complete replacement in the near future.
9. The debate surrounding the phenomenon of emergent capabilities in AI models, including skepticism about its existence or significance.
10. AI perception and interaction, showcasing the rapid adoption and diverse usage of chatbots and AI companions by vast numbers of people, indicating a shift in how AI is integrated into daily life.
11. The balancing act between advancing AI capabilities while maintaining ethical considerations and navigating the societal impacts of AI technology.
12. The potential of approximation techniques and structured sparsity for future advancements in making AI models more efficient and cost-effective.","---

**[00:51:02] Alessio:** ""Do you think a lot of the questions that we have around, you know, what data sets we should use and things like that are just because training was so expensive before that, we just haven't run enough experiments to figure that out. And is that one of your goals is trying to make it cheaper so that we can actually get the answers?""

**[00:51:19] Jonathan:** ""Yeah, that's a big part of my personal conviction for being here. I think I'm, I'm still in my heart, the second year grad student who was jealous of all his friends who had GPUs and he didn't, and I couldn't train any models except in my laptop. And that, I mean, the lottery ticket experiments began on my laptop that I had to beg for one K 80 so that I could run amist. And I'm still that person deep down in my heart. And I'm a believer that, you know, if we wanna do science and really understand these systems and understand how to make them work well, understand how they behave, understand what makes them safe and reliable. We need to make it cheap enough that we can actually do science, and science involves running dozens of experiments.""

---

**[01:00:00] Abhinav:** ""I, I think like this, our recent inference product also makes the decision easier for us, right? So even since these open malls like we've developed so far, um, you can actually like, you know, uh, query them on our inference api, like our starter tier, and we basically charge like a, a per token fee. Very, very similar to the other API fighters. So there are pathways by which, you know, like even the open mall we provide for free still end up like helping our business out, right? You can customize them, deploy them on our, on our platform, and that way we, we still make money off of them.""

---

**[01:03:30] Abhinav:** ""Well, yes. I mean, I think some of them are sort of like, yeah, like either for emotional support or honestly just friends and stuff. Right. I mean, I think like, you know, loneliness mental health is a really a big problem everywhere. And so the most interesting I think I've found is that if you go to the subreddits, you know, for those communities and you see like how they talk about and think about their like AI friends and like these characters, it's, it's, it's like out of a science fiction book, like I would never expect this to be like reality.""

---

These quotes not only share insights into the mindset and goals driving the guest speakers' work but also touch upon broader themes in AI related to accessibility, community involvement, and the social implications of AI technologies.","1. Models
2. Open Source
3. Hardware
4. Coding Tools
5. LLM Tooling
6. FlashAttention
7. AI Engineer Career
8. Finetuning
9. Transformers Math
10. Datasets 101",Models,Open Source,Hardware,Coding Tools,LLM Tooling,FlashAttention,AI Engineer Career,Finetuning,Transformers Math,Datasets 101,0.3449489742,-0.2224744871,0.0
Guaranteed quality and structure in LLM outputs - with Shreya Rajpal of Guardrails AI,https://www.latent.space/p/guaranteed-quality-and-structure,Alessio Fanelli and swyx & Alessio,"Tue, 16 May 2023 00:14:01 GMT",3748,https://substackcdn.com/feed/podcast/1084089/post/121662773/c200847d1067d9b4de6f843a82245a5c.jpg,https://api.substack.com/feed/podcast/121662773/fc743e67d55092558f14749949b9447c.mp3,"Tomorrow, 5/16, we’re hosting Latent Space Liftoff Day in San Francisco. We have some amazing demos from founders at 5:30pm, and we’ll have an open co-working starting at 2pm. Spaces are limited, so please  RSVP here ! One of the biggest criticisms of large language models is their inability to tightly follow requirements without extensive prompt engineering. You might have seen examples of ChatGPT playing a game of chess and making many invalid moves, or adding new pieces to the board.  Guardrails AI aims to solve these issues by adding a formalized structure around inference calls, which validates both the structure and quality of the output. In this episode, Shreya Rajpal, creator of Guardrails AI, walks us through the inspiration behind the project, why it’s so important for models’ outputs to be predictable, and why she went with an XML-like syntax.  Guardrails TLDR Guardrails AI rules are created as RAILs, which have three main “atomic objects”: *  Output:  what should the output look like? *  Prompt:  template for requests that can be interpolated *  Script:  custom rules for validation and correction Each RAIL can then be used as a “guard” when calling an LLM. You can think of a guard as a wrapper for the API call. Before returning the output, it will validate it, and if it doesn’t pass it will ask the model again.  Here’s an example of a bad SQL query being returned, and what the ReAsk query looks like:  Each RAIL is also model-agnostic. This allows for output consistency across different models, even if they have slight differences in how they are prompted. Guardrails can easily be used with  LangChain  and other tools to structure your outputs! Show Notes *  Guardrails AI *  Text2SQL *  Use Guardrails and GPT to play valid chess * Shreya’s  AI Tinkerers demo *  Hazy Research Lab *  AutoPR *  Ian Goodfellow *  GANs  (Generative Adversarial Networks)","* [00:00:00] Shreya's Intro * [00:02:30] What's Guardrails AI? * [00:05:50] Why XML instead of YAML or JSON? * [00:10:00] SQL as a validation language? * [00:14:00] RAIL composability and package manager? * [00:16:00] Using Guardrails for agents * [00:23:50] Guardrails ""contracts"" and guarantees * [00:31:30] SLAs for LLMs * [00:40:00] How to prioritize as a solo founder in open source * [00:43:00] Guardrails open source community involvement * [00:46:00] Working with Ian Goodfellow * [00:50:00] Research coming out of Stanford * [00:52:00] Lightning Round","In this episode of the Latent Space Podcast, Alessio and Swyx talk with Shreya Rajpal, who has recently devoted herself full-time to developing ""Guardrails,"" an innovative AI tool she created to enforce constraints and ensure quality in AI-generated outputs. Shreya, an alum of IIT Delhi, U I U C, drive.ai, and Apple's Special Projects Group, has a rich background in AI, starting her work in the field in 2014. Guardrails, born out of Shreya's own challenges with the reliability and control of AI tools, aims to provide developers with a way to precisely specify and validate the behavior of AI models, ensuring outputs align with user expectations. The tool is model-agnostic, designed to work with any AI model that has a string input and output, and was initially prompted by the limitations Shreya encountered in managing AI's performance in practical applications. Shreya chose XML for Guardrails' specification language for its flexibility and because it allows defining properties and behaviors of the AI output in a structured way. This choice has met some skepticism due to preferences for other markup languages, but Shreya is exploring ways to make Guardrails accessible across different preferences, possibly including a code-first option. Shreya's work reflects a thoughtful blend of high-level AI research and hands-on problem solving aimed at making AI integrations more reliable and user-friendly.","1. The inception of Guardrails AI was driven by Shreya Rajpal's personal challenges in creating AI-driven projects and desiring more control over model outputs.
2. Guardrails AI offers a specification framework and code that enforces constraints on LLM outputs, facilitating both specific output structures and semantic correctness.
3. The choice of XML for Guardrails was due to its ability to effectively describe properties, handle nested fields, and elegantly manage errors through event handling.
4. There's significant interest in Guardrails from non-technical users, aiming to make it accessible for a broad audience with varying technical expertise.
5. Guardrails AI is designed to be model agnostic, enabling integration with any model that accepts string inputs and outputs.
6. The future vision includes exploring ways for models to generate their own guardrails, contributing to the development of more autonomous AI agents.
7. A notable challenge in advancing AI is ensuring determinism and reliability in model outputs, a core problem that Guardrails aims to address.
8. Guardrails integrates with LLM API calls, providing pre and post-processing to ensure outputs meet specified requirements.
9. The development and community engagement of Guardrails AI lean towards open source collaboration, highlighting the continuum of innovation and feedback.
10. Shreya Rajpal has a background in efficient machine learning and is interested in advancements that reduce the complexity and increase the accessibility of AI technologies.","Due to the format of your request, no actionable quotes can be directly extracted verbatim from the provided fictional transcript as it pertains to impacting the thinking or behavior of an AI Engineer specifically from the ""Latent Space Podcast"" transcript. However, I can generate relevant insights or summaries based on typical discussions related to AI, development practices, and project management that might occur in such a context if that would be helpful. Please let me know how you would like to proceed or if there's another way I can assist you.","1. AI Engineer Career
2. Coding Tools
3. Open Source
4. LLM Tooling
5. Models
6. Hardware
7. Multimodal
8. Agents
9. Finetuning
10. Startups",AI Engineer Career,Coding Tools,Open Source,LLM Tooling,Models,Hardware,Multimodal,Agents,Finetuning,Startups,0.1224744871,0.2224744871,0.1
"The AI Founder Gene: Being Early, Building Fast, and Believing in Greatness — with Sharif Shameem of Lexica",https://www.latent.space/p/sharif-shameem,swyx & Alessio and Alessio Fanelli,"Mon, 08 May 2023 18:05:06 GMT",3037,https://substackcdn.com/feed/podcast/1084089/post/120093157/644a41e604c67bcc0b3c1487b8b28ffd.jpg,https://api.substack.com/feed/podcast/120093157/73b072b2a42695224ca81ddd79ea46ea.mp3,"Thanks to the over 42,000 latent space explorers who checked out  our Replit episode ! We are hosting/attending  a couple more events in SF and NYC  this month. See you if in town! Lexica.art was  introduced to the world  24 hours after the release of Stable Diffusion as a search engine for prompts, gaining instant product-market fit as a world discovering generative AI also found they needed to learn prompting by example. Lexica is now 8 months old, serving 5B image searches/day, and  just shipped V3 of Lexica Aperture , their own text-to-image model! Sharif Shameem breaks his  podcast hiatus  with us for an exclusive interview covering his journey building everything with AI! The conversation is nominally about Sharif’s journey through his three startups VectorDash, Debuild, and now Lexica, but really a deeper introspection into what it takes to be a top founder in the fastest moving tech startup scene (possibly ever) of AI. We hope you enjoy this conversation as much as we did! Full transcript is below the fold. We would really appreciate if you shared our pod with friends on  Twitter , LinkedIn, Mastodon, Bluesky, or your social media poison of choice!","* [00:00] Introducing Sharif * [02:00] VectorDash * [05:00] The GPT3 Moment and Building Debuild * [09:00] Stable Diffusion and Lexica * [11:00] Lexica’s Launch & How it Works * [15:00] Being Chronically Early * [16:00] From Search to Custom Models * [17:00] AI Grant Learnings * [19:30] The Text to Image Illuminati? * [20:30] How to Learn to Train Models * [24:00] The future of Agents and Human Intervention * [29:30] GPT4 and Multimodality * [33:30] Sharif’s Startup Manual * [38:30] Lexica Aperture V1/2/3 * [40:00] Request for AI Startup - LLM Tools * [41:00] Sequencing your Genome * [42:00] Believe in Doing Great Things * [44:30] Lightning Round Show Notes * Sharif’s  website ,  Twitter ,  LinkedIn *  VectorDash  ( 5x cheaper than AWS ) * Debuild  Insider ,  Fast company ,  MIT review ,  tweet ,  tweet * Lexica *  Introducing Lexica *  Lexica Stats * Aug:  “God mode” search * Sep:  Lexica API   * Sept:  Search engine with CLIP   * Sept:  Reverse image search * Nov:  teasing Aperture * Dec:  Aperture  v1 * Dec -  Aperture v2 * Jan 2023 -  Outpainting * Apr 2023 -  Aperture v3 *  Same.energy *  AI Grant * Sharif on Agents: prescient  Airpods tweet ,  Reflection * MiniGPT4 -  Sharif on Multimodality *  Sharif Startup Manual *  Sharif Future * 23andMe Genome Sequencing Tool:  Promethease * Lightning Round *  Fave AI Product :  Cursor.so . Swyx  ChatGPT Menubar  App. *  Acceleration : Multimodality of GPT4.  Animated Drawings *  Request for Startup : Tools for LLMs, Brex for GPT Agents *  Message : Build Weird Ideas!","In this episode of the Latent Space podcast, Alessio and Wix host Sharif Amin, a notable figure in the tech world known for his innovative work on projects that utilize artificial intelligence and machine learning. The conversation starts with Sharif sharing his journey from studying at the University of Maryland to dropping out due to his involvement in a side project that demanded most of his time. This pivotal moment in his life led him to work at Mitra, where he contributed to computer vision projects, before taking on the adventurous path of entrepreneurship with VectorDash, a marketplace for renting out GPUs for machine learning research and cryptocurrency mining. 

Sharif then delves into his exciting venture into developing Debuild, inspired by his first encounter with GPT-3's capabilities, especially its application in generating code. His passion and the surprise at GPT-3's potential propelled him to further innovate, leading to his involvement in the creation of Lexica, a search engine for stable diffusion images. The podcast highlights Sharif's journey with Lexica, from its initial concept aimed at simplifying the search for image generation prompts to its evolution into a platform that vastly improves the accessibility and utility of stable diffusion technology. Through iterations of Lexica's Aperture models, Sharif discusses the nuances of developing more realistic and artistically inclined image generation capabilities, underscoring the balance between technical advancement and user satisfaction.

Throughout the conversation, Sharif emphasizes the importance of working on novel and impactful projects, sharing insights from his startup manual and reflecting on the broader implications of AI advancements for future innovations. The discussion also touches on broader themes, including the potential of multimodal models, such as GPT-4, and the exciting possibilities of combining AI with various domains to create transformative technologies and experiences.

Sharif's journey is portrayed as one of curiosity, innovation, and a strong drive to explore the boundaries of what's possible with technology. His projects, from VectorDash to Lexica, showcase a relentless pursuit of creating tools that are not only technologically advanced but also highly practical and beneficial for a wide range of users. Through his experiences, Sharif inspires others to pursue their unique ideas and contribute to the rapidly evolving landscape of AI and tech.","1. Development and deployment of VectorDash for distributed GPU usage, leveraging cryptocurrency mining rigs for more affordable and efficient machine learning and cloud gaming purposes.
2. Sharif's exploration and application of GPT-3 in creating dynamic code generation tools showed the substantial capability of AI in programming assistance.
3. Launching Lexica as a search engine inspired by stable diffusion, offering an improved way to explore and share AI-generated imagery by indexing prompts and images from various sources.
4. Lexica's early and rapid growth, driven by the appeal of accessible, high-quality AI-generated images and the use of semantic search utilizing clip embeddings to enhance the user experience.
5. The transition from simply offering a search tool to integrating image generation capabilities directly within Lexica, allowing users to not just explore but also create AI-generated images seamlessly.
6. Insights on being chronically early to technological trends and the potential advantages and challenges it presents.
7. Development from basic search functionalities in Lexica to incorporating custom models for image generation, showcasing the evolution of AI applications in creative fields.
8. Discussion on future directions for enhancing AI practicality, including the development of AI agents capable of performing complex tasks online and potentially handling financial transactions or interactions. 
9. Recognition of the need for more innovative and experimental AI applications beyond mainstream developments, encouraging unique and unexplored ideas within the AI community.","From the discussion with Sharif Amin on the Latent Space podcast, here are a few actionable quotes that might impact the thinking or behavior of an AI Engineer:

1. **On the importance of sharing and building products based on capabilities (05:00):**
   - Sharif expresses fascination with GPT-3's capabilities and how sharing his work on social media led to significant attention and opportunities: ""so mind-blowing that no one was talking about this whole GPT-3 thing. I was like, why is this not on everyone's minds? So I recorded a quick 30-second demo and I posted on Twitter... When I wake up the next morning and I had like 20,000 likes and like 100,000 people had viewed it.""

2. **On leveraging emerging AI for new ventures (11:00):** 
   - Speaking on the rapid success of Lexica post-launch with staggering numbers of queries and images served, Sharif highlights the unexpected scale of interest and engagement: ""I think we're doing like over 5 billion images served per month now... it's pretty crazy how much things have changed since then."" This underscores the potential for rapid growth when aligning products with cutting-edge AI capabilities and the zeitgeist.

3. **On the iterative nature of product development and embracing feedback (15:00):**
   - Sharif shares insights on being early to Stable Diffusion and how iteration led to better product development at Lexica: ""It was kind of that deal was like growing pretty slowly and pretty linearly. And then Lexica was just like this thing that just kept going up and up and up.""

4. **On training models and democratizing AI skills (20:30):**
   - Discussing the learning curve associated with AI and model training, Sharif remarks: ""You might be a bit slower for the first few months, but ultimately I think if you have the programming skills, you'll catch up pretty quickly... it's not like this magical dark science that only three people in the world know how to do well.""

5. **On pushing the envelope and creating innovative AI applications (44:30):**
   - Sharif speaks to the importance of building novel and ambitious AI-driven projects: ""If you have weird and interesting ideas, you should actually go out and just do them and make the thing and then share that with the world. Cause I feel like we need more people building weird ideas...""

These quotes encapsulate a mindset of exploration, rapid iteration, and the democratization of AI technology, stressing the importance of acting on unique ideas and the potential for significant impact in the AI field.","1. AI Engineer Career
2. Startups
3. Models
4. Hardware
5. Open Source
6. Finetuning
7. News
8. Coding Tools
9. LLM Tooling 
10. Multimodal",AI Engineer Career,Startups,Models,Hardware,Open Source,Finetuning,News,Coding Tools,LLM Tooling ,Multimodal,0.0,0.4449489742,0.2
No Moat: Closed AI gets its Open Source wakeup call — ft. Simon Willison,https://www.latent.space/p/no-moat,"swyx & Alessio, Alessio Fanelli, and Simon Willison","Fri, 05 May 2023 16:17:30 GMT",2629,https://substackcdn.com/feed/podcast/1084089/post/119492812/3161ccfea1d5806907af3463bc37853f.jpg,https://api.substack.com/feed/podcast/119492812/d3c7f540ad2c682c84eca545cce069c4.mp3,"It’s now almost 6 months since  Google declared Code Red , and the results — Jeff Dean’s  recap of 2022 achievements  and a mass exodus of the  top   research   talent  that contributed to it in January, Bard’s rushed launch in Feb, a slick video showing  Google Workspace AI features  and  confusing doubly linked blogposts  about PaLM API in March, and  merging Google Brain and DeepMind  in April — have not been inspiring.  Google’s internal panic is in full display now with the surfacing of  a well written memo ,  written by software engineer Luke Sernau  written in early April, revealing internal distress not seen since Steve Yegge’s infamous  Google Platforms Rant . Similar to 2011, the company’s response to an external challenge has been to mobilize the entire company to go all-in on a (from the outside) vague vision. Google’s misfortunes are well understood by now, but the last paragraph of the memo: “We have no moat, and  neither does OpenAI ”, was a banger of a mic drop. Combine this with news  this morning  that OpenAI  lost $540m last year  and will need as much as $100b more funding (after the complex  $10b Microsoft deal  in Jan), and the memo’s assertion that  both  Google and OpenAI have “no moat” against the mighty open source horde have gained some credibility in the past 24 hours. Many are criticising this memo privately: * A CEO commented to me yesterday that  Luke Sernau  does not seem to work in AI related parts of Google and “software engineers don’t understand moats”.  * Emad Mostaque, himself a perma-champion of open source and open models, has  repeatedly  stated that “Closed models will always outperform open models” because closed models can just wrap open ones. * Emad has also  commented on  the moats he does see: “ Unique usage data, Unique content, Unique talent, Unique product, Unique business model ”, most of which Google does have, and OpenAI less so (though it is winning on the talent front) * Sam Altman famously  said  that “very few to no one is Silicon Valley has a moat - not even Facebook” (implying that moats don’t actually matter, and you should spend your time thinking about more important things) * It is not actually clear what race the memo thinks Google and OpenAI are in vs Open Source. Neither are particularly concerned about running models locally on phones, and they are perfectly happy to let “ a crazy European alpha male ” run the last mile for them while they build actually monetizable cloud infrastructure. However moats are of intense interest by everybody keen on productized AI, cropping up in every  Harvey ,  Jasper , and general  AI startup vs incumbent  debate. It is  also  interesting to take the memo at face value and discuss the searing hot pace of AI progress in open source.  We hosted this discussion yesterday with Simon Willison, who apart from being an incredible communicator also wrote  a great recap of the No Moat  memo. 2,800 have  now tuned in on Twitter Spaces , but we have taken the audio and cleaned it up here. Enjoy!","* [00:00:00] Introducing the Google Memo * [00:02:48] Open Source > Closed? * [00:05:51] Running Models On Device * [00:07:52] LoRA part 1 * [00:08:42] On Moats - Size, Data * [00:11:34] Open Source Models are Comparable on Data * [00:13:04] Stackable LoRA * [00:19:44] The Need for Special Purpose Optimized Models * [00:21:12] Modular - Mojo from Chris Lattner * [00:23:33] The Promise of Language Supersets * [00:28:44] Google AI Strategy * [00:29:58] Zuck Releasing LLaMA * [00:30:42] Google Origin Confirmed * [00:30:57] Google's existential threat * [00:32:24] Non-Fiction AI Safety (""y-risk"") * [00:35:17] Prompt Injection * [00:36:00] Google vs OpenAI * [00:41:04] Personal plugs: Simon and Travis","This podcast transcript covers a discussion among Simon Willison, swyx, Travis Fischer, and Alessio Fanelli about an internal Google document, leaked and discussing Google's competitive stance and strategies in AI, particularly against OpenAI and the rising power of the open-source community in developing language models. The document, titled ""We Have No Moat and neither does Open Ai,"" argues that Google and OpenAI's focus on training larger language models might be misguided, as the open-source community, exemplified by models like Facebook's LLaMA and its derivatives (Alpaca, Vicuna), is rapidly catching up and, in some respects, surpassing them in innovation and accessibility. 

The speakers touch upon various points: 
- The document's insightful analysis, whether officially from Google or not, urging both giant corporations and the AI community to pay attention to the evolving landscape where open-source appears to have a competitive edge.
- Open source's agility in iterating and fine-tuning smaller models as opposed to the massive, costly models by corporations like Google and OpenAI.
- The possible strategic missteps by both Google and OpenAI in underestimating the open-source community's potential to lead AI innovation.
- The broader implications of open-source advancements for AI's commercial and research trajectories, specifically concerning modularity, like the LoRA (Low-Rank Adaptation) technique for improving models efficiently.
- Safety and ethics in AI, addressing both the hyperbolic 'science fiction scenarios' and the immediate, practical concerns like misuse for scams or misinformation.
- The potential shifts in AI technology development dynamics, noting the rising importance of data quality, infrastructure accessibility, and the need for new programming languages or tools (e.g., Mojo) aimed at optimizing AI model performance and deployment.

The discussion concludes with personal insights and project plugs from the participants, reflecting on the rapid pace of AI development and the need for the AI community to adapt to these changes thoughtfully and proactively.","1. Open source communities are rapidly catching up and in some instances, outpacing Google and OpenAI in developing language models, highlighting the competitiveness of open models.
2. The development and improvement of open source language models, as demonstrated by Facebook's Lama and the subsequent enhancements through Stanford's Alpaca and Vicuna, showcase the agility and innovation within the open source ecosystem.
3. The argument that smaller, more focused models that can be quickly fine-tuned may offer more practical utility and competitive advantage over larger, more comprehensive models.
4. LoRA (Low-Rank Adaptation) is identified as a promising method for making stackable improvements to models, allowing for more efficient and focused enhancements without the need to retrain entire models.
5. Running AI models on on-device hardware, including mobile phones, is advancing more rapidly than anticipated, with examples like Vicuna 13B demonstrating significant functionality with limited resources.
6. The cultivation and utilization of high-quality data, rather than sheer volume, are becoming increasingly central to model effectiveness and competitive advantage.
7. There is a growing consensus that open source development may surpass closed-source efforts, with calls for greater collaboration with the open source community to maintain competitiveness and innovation.
8. Mojo, a new programming language announced by Chris Latner, aims to combine Python simplicity with the ability to produce high-performance code, potentially revolutionizing programming efficiency and AI model implementation.
9. Concerns are raised about AI safety, both in terms of potential misuse for activities like scams and the speculative risks of highly autonomous AI systems.
10. The leaked Google Memo provides insights into Google's internal strategy discussions regarding AI development, illustrating the company's considerations on open source contributions, model sizes, and competitive positioning against entities like OpenAI.","[00:32:48] ""So his, the, the, a couple of weeks ago, his feeling was that it's just too risky for them to, to allow it to be used like that. But a couple of weeks is, is, is a couple of months in AI world. So yeah, it wouldn't be, it feels to me like strategically Facebook should be jumping right on this because this puts them at the very lead of, of open source innovation around this stuff.""

[00:34:08] ""So I feel like if you actually wanted a model that was, was a threat to human, a language model would be a tiny corner of what that thing was actually built on top of, you'd need goal setting and all sorts of other bits and pieces.""

[00:40:06] ""They are making the same mistakes that we are in their posture relative to open source. And their ability to maintain an edge is necessarily in question. Open source alternatives can and will eventually eclipse them. Unless they change their stance in this respect, at least we can make the first move.""

[00:42:48] ""So I'm I, I just founded a company. I'm working on a framework for building reliable agents that aren't toys and focused on more constrained use cases.""

These quotes can inspire AI Engineers to contemplate the pace of AI research, reassess the risks and benefits of large language models, consider the importance of open source in fostering innovation, and reflect on pragmatic approaches to AI development.","1. Open Source
2. Models
3. Hardware
4. Finetuning
5. LLM Tooling
6. AI Engineer Career
7. Coding Tools
8. Multimodal
9. News
10. Startups",Open Source,Models,Hardware,Finetuning,LLM Tooling,AI Engineer Career,Coding Tools,Multimodal,News,Startups,0.2224744871,0.4449489742,0.1
Training a SOTA Code LLM in 1 week and Quantifying the Vibes — with Reza Shabani of Replit,https://www.latent.space/p/reza-shabani,swyx & Alessio and Alessio Fanelli,"Wed, 03 May 2023 15:16:12 GMT",4171,https://substackcdn.com/feed/podcast/1084089/post/118824331/aafa41b27279e6a3c13a27e69f12762b.jpg,https://api.substack.com/feed/podcast/118824331/e08c73e511d0ba5fe3b333c14f3a3774.mp3,"Latent Space is popping off! Welcome to the over 8500 latent space explorers who have joined us. Join us this month at  various events in SF and NYC , or start your own! This post spent 22 hours at  the top of Hacker News . As  announced during their Developer Day  celebrating their  $100m fundraise  following  their Google partnership , Replit is  now open sourcing  its own state of the art code LLM: replit-code-v1-3b ( model card ,  HF Space ), which beats OpenAI’s Codex model on the industry standard HumanEval  benchmark  when finetuned on Replit data (despite being 77% smaller) and more importantly passes AmjadEval (we’ll explain!) We got an exclusive interview with  Reza Shabani , Replit’s Head of AI, to tell the story of Replit’s journey into building a data platform, building GhostWriter, and now  training their own LLM , for 22 million developers! 8 minutes of this discussion go into a live demo discussing generated code samples - which is always awkward on audio. So we’ve again gone multimodal and put up a screen recording  here  where you can follow along on the code samples! Recorded in-person at the beautiful  StudioPod  studios in San Francisco. Full transcript is below the fold. We would really appreciate if you shared our pod with friends on  Twitter , LinkedIn, Mastodon, Bluesky, or your social media poison of choice!","* [00:00:21] Introducing Reza * [00:01:49] Quantitative Finance and Data Engineering * [00:11:23] From Data to AI at Replit * [00:17:26] Replit GhostWriter * [00:20:31] Benchmarking Code LLMs * [00:23:06] AmjadEval live demo * [00:31:21] Aligning Models on Vibes * [00:33:04] Beyond Chat & Code Completion * [00:35:50] Ghostwriter Autonomous Agent * [00:38:47] Releasing Replit-code-v1-3b * [00:43:38] The YOLO training run * [00:49:49] Scaling Laws: from Kaplan to Chinchilla to LLaMA * [00:52:43] MosaicML * [00:55:36] Replit's Plans for the Future (and Hiring!) * [00:59:05] Lightning Round Show Notes * Reza Shabani on  Twitter  and  LinkedIn * also  Michele Catasta  and  Madhav Singhal *  Michele Catasta’s thread on the release of replit-code-v1-3b *  Intro to Replit Ghostwriter *  Replit Ghostwriter Chat  and  Building Ghostwriter Chat * Reza on  how to train your own LLMs  (their top blog of all time) *  Our Benchmarks 101 episode  where we discussed HumanEval *  AmjadEval live demo *  Nat.dev *  MosaicML CEO Naveen Rao on Replit’s LLM *  MosaicML Composer + FSDP code *  Replit’s AI team is hiring  in North America timezone - Fullstack engineer, Applied AI/ML, and other roles!","In the ""Latent Space"" podcast episode, Reza Shabani, the Head of AI at Replit, talks about his journey from obtaining a PhD in economics, founding a startup, working in systematic equity trading, to finally leading AI initiatives. Shabani details his surprise at people questioning his coding ability due to his economics background, emphasizing his quantitative approach in finance and how it’s a common misconception that finance doesn’t involve coding. His work has been deeply data-driven, particularly involving Natural Language Processing (NLP) and Machine Learning (ML) to analyze financial data, like his unique project analyzing CNBC close captions for financial trends.

Transitioning to his role at Replit, Shabani discusses building data infrastructure from scratch, allowing scale in data querying and processing. This foundational work was crucial for developing AI and ML projects, including the development of Ghostrider, Replit’s code autocompletion tool, and other AI-driven features that improve coding efficiency and quality. Shabani also shares insights into the complex AI and ML strategies employed in the finance industry, including a detailed example of a trading strategy that leverages emerging technology trends to predict stock performance. 

The conversation shifts to AI's potential beyond code completion, envisioning a future where AI significantly influences software development processes. Replit aims to develop an AI that can autonomously navigate and execute tasks within the IDE, highlighting how AI can transform software creation beyond mere text generation. Shabani also discusses the challenges of aligning AI models with human expectations (“vibes”) and the open-sourcing of Replit’s AI models, emphasizing the collaborative and exploratory nature of AI development in coding environments.

In a broader perspective, Shabani shares his views on AI’s rapid evolution and its implications for society, advocating for engagement with AI to harness its potential positively. Through sharing his experiences and vision, Shabani illustrates the transformative impact of AI in technology and beyond, underscoring the importance of developing and utilizing AI in reflective and innovative ways.","1. The importance of quantitative finance skills in AI development, illustrating how financial data analysis can contribute to AI advancements.
2. The innovative use of NLP and machine learning in analyzing financial news impact on stock markets, highlighting the early adoption and sophisticated data handling capabilities in the finance sector.
3. The emerging technologies trading strategy at BlackRock, showcasing how systematically analyzing trends in technology and their adoption by companies can inform investment decisions.
4. The infrastructure and technical challenges involved in handling and analyzing large-scale data, as demonstrated in the creation of data pipelines before common tools like Airflow existed.
5. The explanation of how natural language processing (NLP) and machine learning (ML) techniques can be used to gain insights from company transcripts, analyst reports, earnings calls, and other text-heavy financial documents to predict market movements.
6. The development of large language models (LLMs) and their integration into software engineering, specifically in coding assistance and the transformation of AI integration from merely providing code completions to offering sophisticated development guidance.
7. The exploration of how AI can assist in the software development process beyond code completion, potentially scaffolding entire projects, handling deployments, and more, offering a glimpse into the future of automated software development.
8. Discussion on the potential of AI to become an autonomous agent within integrated development environments (IDEs), suggesting a shift towards more intelligent tools that can assist in multifaceted development tasks.
9. Insights into the challenges of training AI models, including handling vast amounts of data, model evaluation methods, and the complexity of aligning AI outputs with human expectations or ""vibes"".
10. The release and open sourcing of a new code model by Replit, emphasizing the importance of community and collaboration in the AI research field, and the ongoing effort to democratize access to sophisticated AI tools for developers.","Given the extensive details provided in the conversation, here are the most impactful and actionable quotes for AI engineers:

**On Learning and Applying New Skills:**
[00:00:51] ""A lot of people are kind of like, do you know how to code?... Yes, I do know how to code. So I think that just kind of surprises people coming from an Econ background."" - Reza Shabani

**On the Breadth and Depth of Data in AI Applications:**
[00:02:39] ""I recorded CNBC, the Financial News Network, for 10 hours a day, every day... Extracted the close captions from the video files, and then used that to create a second by second transcript of... CNBC, merged that on with high frequency trading, quote data, and then looked at... NLP, tagging the company names..."" - Reza Shabani

**On Approaching AI Application in Finance with Innovative Data Sources:**
[00:04:33] ""...so systematic active equity at BlackRock was such an amazing group and you just end up learning so much and the possibilities there... And the really interesting stuff that I found when I went in was all related to NLP and ML... using that data on searches to predict the price of cars..."" - Reza Shabani

**On Embracing Comprehensive Data Analytics for AI Strategy Creation:**
[00:05:38] ""And so what we ended up doing is... pulled data on startups... you get that, that corpus of like when startups were getting funding. And then you can run non-negative matrix factorization on it and create these clusters of what the various emerging technologies are..."" - Reza Shabani

**On the Impact of Empowering AI with Access and Initiative within Development Environments:**
[00:37:17] ""What we really want to do is... Is merge the two in a way where we want Ghost Rider to be like an autonomous agent that can actually drive the IDE. So in these action models... you have, like, a sequence of events and then you can use transformers to kind of keep track of that sequence and predict the next event."" - Reza Shabani

**On the Power and Potential of AI beyond Conventional Applications:**
[01:01:54] ""I think a year from now, I'm really interested in seeing how a lot of this technology will be applied to domains outside of chat... it's going to become incredibly powerful... applying it to actual products, businesses, use cases, it's going to be incredibly powerful."" - Reza Shabani

These quotes shed light on the multifaceted nature of AI projects, underscore the importance of diversified skills, innovative thinking in applying AI, the value of comprehensive data analysis, and the significant potential of AI to revolutionize various domains beyond its current uses.","1. AI Engineer Career
2. Coding Tools
3. Models
4. Finetuning
5. Datasets 101
6. Open Source
7. Hardware
8. LLM Tooling
9. Multimodal
10. Agents",AI Engineer Career,Coding Tools,Models,Finetuning,Datasets 101,Open Source,Hardware,LLM Tooling,Multimodal,Agents,0.2449489742,0.0,0.0
Mapping the future of *truly* Open Models and Training Dolly for $30 — with Mike Conover of Databricks,https://www.latent.space/p/mike-conover,Mike Conover,"Sat, 29 Apr 2023 01:15:21 GMT",4559,https://substackcdn.com/feed/podcast/1084089/post/117989100/d36f9d182a9412c436d02685cad17d87.jpg,https://api.substack.com/feed/podcast/117989100/adcbc302672d7ea3b3f801ab2390696a.mp3,"The race is on for the first fully GPT3/4-equivalent, truly open source Foundation Model!  LLaMA’s release proved that a great model could be released and run on consumer-grade hardware (see  llama.cpp ), but its research license prohibits businesses from running it and all it’s variants (Alpaca, Vicuna, Koala, etc) for their own use at work. So there is great interest and desire for *truly* open source LLMs that are feasible for commercial use (with far better customization, finetuning, and privacy than the closed source LLM APIs). The previous leading contenders were Eleuther’s  GPT-J  and  Neo  on the small end (<6B parameters), and Google’s  FLAN-T5  (137B),  PaLM  (540B), and  BigScience’s BLOOM  (176B) on the high end. But Databricks is to my knowledge the first to release not just a cleanly licensed, high quality LLM that can run on affordable devices, but also a  simple Databricks notebook  that can be customized to be finetuned for your data/desired style - for $30 in 30 minutes on one machine! Mike Conover  tells the story of how a small team of Applied AI engineers got convinced Ali Ghodsi and 5,000 of their coworkers to join in the adventure of building the first open source, instruction-following LLM, fine-tuned on a human-generated instruction dataset licensed for research and commercial use. He also indulges our questions on other recent open source LLM projects, CerebasGPT and RedPajama, though we recorded this a week before Stability’s  StableLM  release.  Stick around to the end for some easter eggs featuring AI Drake! Recorded in-person at the beautiful  StudioPod  studios in San Francisco. Full transcript is below the fold. Show Notes * Mike Conover  LinkedIn  and  Twitter *  Dolly 1.0 *  Dolly 2.0 *  CICERO and Diplomacy *  Dolly and Deepspeed * LLMops:  *  https://nat.dev/ *  PromptLayer *  HumanLoop * Spreadsheets?? *  Quadratic *  Alessio’s Email GPT Drafter * Open Models *  Open Assistant *  Cerebras GPT *  RedPajama *  Reflexion ,  Recursive Criticism and Improvement * Lightning Round * AI Product: Google Maps * AI People: EleutherAI, Huggingface’s  Stas Bekman * AI Prediction: Open LLaMA reproduction, AI Twins of People ( AI Drake ), Valuing Perplexity  * Request for Startups: LLMOps/Benchmarks, Trail Mapping","* [00:00:21] Introducing Mike Conover * [00:03:10] Dolly 1.0 * [00:04:18] Making Dolly * [00:06:12] Dolly 2.0 * [00:09:28] Gamifying Instruction Tuning * [00:11:36] Summarization - Thumbnails for Language * [00:15:11] CICERO and Geopolitical AI Agents * [00:17:09] Datasets vs Intentional Design * [00:21:44] Biological Basis of AI * [00:23:27] Training Your Own LLMs * [00:28:21] You May Not Need a Large Model * [00:29:59] Good LLM Use cases * [00:31:33] Dolly Cost $30 on Databricks * [00:36:06] Databricks Open Source * [00:37:31] LLMOps and Prompt Tooling * [00:42:26] ""I'm a Sheets Maxi"" * [00:44:19] AI and Workplace Productivity * [00:47:02] OpenAssistant * [00:47:41] CerebrasGPT * [00:51:35] RedPajama * [00:54:07] Why Dolly > OpenAI GPT * [00:56:19] Open Source Licensing for AI Models * [00:57:09] Why Open Source Models? * [00:58:05] Moving Models * [01:00:34] Learning in a Simulation * [01:01:28] Why Model Reflexion and Self Criticism Works * [01:03:51] Lightning Round","The Latent Space Podcast episode features Alessio Partner and CT from Decibel Partners, Joan Bama, and swyx Brighter, who welcome guest Mike Conover, a staff software engineer at Databricks. Mike discusses his journey from obtaining his PhD in Complex Systems Analysis at Indiana University, analyzing Twitter clusters, to roles at LinkedIn and Workday, and his current position at Databricks. He shares his passion for off-trail backcountry adventures and draws parallels between navigating high-dimensional spaces in machine learning and topographic maps. The conversation covers the development of Dolly 1.0 and 2.0 at Databricks, exploring the choices behind using GPT-J over LLaMA, and the innovative process of creating a training dataset with contributions from Databricks employees. Mike also touches on broader AI challenges and opportunities, such as the potential use of AI in negotiating geopolitical conflicts and enhancing workplace productivity. He speaks on the importance of open-source models and data for AI development and shares insights into the evolving AI landscape, highlighting projects like Open Assistant and Cerberus GPT. Mike advocates for AI that enriches human experiences, from music and creativity to outdoor exploration, stressing the role of AI in managing resources and understanding our environment.","1. The development of Dolly 1.0, a 6 billion parameters model based on GPT-J.
2. Dolly 2.0 improved to 12 billion parameters, based on the model family from EleutherAI, with a brand-new training dataset created by Databricks employees.
3. Observation that large-scale language models can exhibit dramatic shifts in capabilities with relatively small additions of targeted training data, akin to phase transitions in physics.
4. Gamifying the process of instructional tuning for AI models can significantly enhance data collection and model training efficiency.
5. The challenge and importance of data summarization in AI research and application, highlighting the complexity and high demand for efficient summarization techniques.
6. Insights into how the composition of a dataset can qualitatively shape the resulting behaviors of AI models.
7. The impact of model size and training depth on AI behaviors, proposing a potential pathway towards more deterministic approaches to elicit desired capabilities from AI models.
8. Exploring the potential of AI in facilitating the negotiation of geopolitical issues, with the example of Meta's Cicero model.
9. Discussion on the distinction between discovering and intentionally designing AI capabilities, suggesting a blend of both approaches for future AI development.
10. The significance of open-source AI models and datasets for fostering collaborative advancements and addressing ethical, bias, and safety concerns in AI.","[00:11:29] ""Want it to actually be useful, not just a performance of usefulness. And I think people got that.""
[00:14:15] ""...clothes as a technology that has shaped our physiology. Right. Like, and our, our phenotypic expression, we used to be covered in hair. We evolved this technology fire would also be in this class, and our bodies changed in response to it on the very long time scale of human history.""
[00:20:22] ""Maybe we will get more intentional about, like, I want to elicit the fol, I want summarization, I want closed form, question answering. Those are the only things that matter to me. How much data do I need to produce or buy, how many parameters do I need to solve that compression problem? And maybe it will become much more deterministic, but right now it feels a lot like we're just trying things and seeing if it works, which is quite different from a lot of engineering disciplines.""
[00:28:21] ""Just, just to spell it out for people trying to, uh, go deep on stuff. Um, maybe leave breadcrumbs. Um, sure. When you say techniques, you don't just mean prompting. Oh, I mean even like named entity recognition, like Yeah, there's just like classic NLP stuff, you know, like supervised learning. I mean, multi-class classification.""
[00:35:21] ""...making sure that we have a great customer experience and that we have the resources that are required for all of our customers. You can push a button, get a computer, uh, get a Spark cluster. And I think when you look to a world where everybody is using GPUs on Databricks, making sure that we are running as efficiently as possible so that we can make Databricks a place that is extremely cost effective to train and operate these models. I think you have to solve both problems simultaneously.""

These quotes contain actionable insights and reflections that could influence the thinking and approach of AI engineers toward their work, emphasizing the importance of open source contributions, practical utility over mere performance, evolutionary technology, and efficient problem-solving techniques.","1. Models
2. Open Source
3. Finetuning
4. LLM Tooling
5. Multimodal
6. Hardware
7. AI Engineer Career
8. Datasets 101
9. Coding Tools
10. News",Models,Open Source,Finetuning,LLM Tooling,Multimodal,Hardware,AI Engineer Career,Datasets 101,Coding Tools,News,0.3449489742,0.2224744871,0.0
AI-powered Search for the Enterprise — with Deedy Das of Glean,https://www.latent.space/p/deedy-das,swyx & Alessio,"Sat, 22 Apr 2023 00:07:21 GMT",3842,https://substackcdn.com/feed/podcast/1084089/post/116379427/a752ea406ce54bf1fd5866b7d745fba9.jpg,https://api.substack.com/feed/podcast/116379427/7b13cd5cfbf87cf837ad90a02a053699.mp3,"The most recent YCombinator W23 batch graduated 59 companies building with Generative AI for everything from sales, support, engineering, data, and more: Many of these B2B startups will be seeking to establish an AI foothold in the enterprise. As they look to recent success, they will find Glean,  started in 2019  by a group of ex-Googlers to finally solve AI-enabled enterprise search. In 2022 Sequoia led  their Series C at a $1b valuation  and Glean have just refreshed their website touting new logos across Databricks, Canva, Confluent, Duolingo, Samsara, and more in the Fortune 50 and announcing  Enterprise-ready AI features  including AI answers, Expert detection, and In-context recommendations. We talked to  Deedy Das , Founding Engineer at Glean and a former Tech Lead on Google Search, on why he thinks many of these startups are solutions looking for problems, and how Glean’s holistic approach to enterprise probllem solving has brought so much success.  Deedy is also just a fascinating commentator on AI current events, being both extremely qualified and great at distilling insights, so we also went over his many viral tweets diving into Google’s competitive threats, AI Startup investing, and his exposure of Indian University Exam Fraud! Show Notes * Deedy on  LinkedIn  and  Twitter  and  Personal Site *  Glean *  Glean and Google Moma *  Golinks.io * Deedy on  Google vs ChatGPT * Deedy on  Google Ad Revenue * Deedy on  How much does it cost to train a state-of-the-art foundational LLM? * Deedy on  Google LaMDA cost * Deedy’s  Indian Exam Fraud Story *  Lightning Round * Favorite Products: (covered in segment) * Favorite AI People:  AI Pub * Predictions: Models will get faster for the same quality * Request for Products: Hybrid Email Autoresponder * Parting Takeaway: Read the research!",* [00:00:21] Introducing Deedy * [00:02:27] Introducing Glean * [00:05:41] From Syntactic to Semantic Search * [00:09:39] Why Employee Portals * [00:12:01] The Requirements of Good Enterprise Search * [00:15:26] Glean Chat? * [00:15:53] Google vs ChatGPT * [00:19:47] Search Issues: Freshness * [00:20:49] Search Issues: Ad Revenue * [00:23:17] Search Issues: Latency * [00:24:42] Search Issues: Accuracy * [00:26:24] Search Issues: Tool Use * [00:28:52] Other AI Search takes: Perplexity and Neeva * [00:30:05] Why Document QA will Struggle * [00:33:18] Investing in AI Startups * [00:35:21] Actually Interesting Ideas in AI * [00:38:13] Harry Potter IRL * [00:39:23] AI Infra Cost Math * [00:43:04] Open Source LLMs * [00:46:45] Other Modalities * [00:48:09] Exam Fraud and Generated Text Detection * [00:58:01] Lightning Round,"In this episode of the Latent Space Podcast, Alessio, a partner and CTO at Decibel Partners, and swyx, a writer and editor of Latent Space, interview a special guest, Deedy Das from Glean. Deedy, who prefers to go by Deedy, is introduced through his professional background, which includes obtaining a bachelor's and master's in Computer Science from Cornell, working at both Facebook and Google on search technologies, and his role at Glean, a company focused on building intelligent search for the workplace. 

Deedy shares that outside of his LinkedIn profile, he is a movie buff and an avid reader, preferring fiction for pleasure. He specifically mentions reading ""The Three Body Problem"" series but stopped after not enjoying the second part. At Glean, Deedy has been part of the founding team, seeing the company grow into a search unicorn. He discusses the evolution of enterprise search, driven by advancements in distributed computing, APIs, and the explosion of SaaS apps. The conversation also explores the transition from syntactic to semantic search, Glean's approach to combining traditional search techniques with modern AI advancements, and their effort to create a comprehensive employee portal that enhances workplace productivity.

The podcast touches on future enterprise environments, mentioning natural language queries becoming more common, and Glean's notable features, like an integrated employee feed and mentions system. Deedy's journey reflects a broader narrative on the transformative impact of AI and intelligent search in the modern workplace, emphasizing the importance of solving core information retrieval problems over technical novelty.

Towards the end, there's a discussion about the cost and accessibility of AI technologies, with Deedy offering insights into the financial aspects of developing large language models (LLMs) and the potential for open source models to democratize access. He concludes with thoughts on the exciting developments in AI, including multimodal models and the potential for fully interactive digital experiences reminiscent of the ""moving pictures"" from Harry Potter, representing the cutting edge of AI applications in consumer technology.","1. Enterprise search systems now benefit from distributed Elasticsearch for heavy lifting, nuanced API support from SaaS apps, and the explosion of SaaS app usage driven by remote work, making internal search increasingly critical.
2. The transition from syntax-driven to semantics-driven search in enterprise environments, potentially reducing reliance on SQL queries and dropdowns.
3. The insight that merely being a search engine is not compelling enough for product retention; enriching employee portals with features like trend document feeds, mentions, and collections that proactively present information enhances user engagement.
4. Glean's approach isn't purely based on vector search; it employs a hybrid method incorporating traditional information retrieval signals, query augmentation, vector search, and personalization to improve search quality.
5. Debates on Google vs. ChatGPT for different types of queries, noting that each has its strengths and weaknesses based on the nature of the query.
6. The technological challenge and potential ethical implications of making AI search more accurately represent fresh information.
7. Concerns about the impact of AI chat interfaces on traditional ad revenue models, with the shift from clickable ads to less interactive chat interfaces potentially disrupting current web ecosystems and financing models.
8. The importance of the intellectual rigor in tuning search algorithms, emphasizing that advancements in AI or complex algorithms are not as crucial as consistent improvement and tuning in search quality.
9. The potential dangers and ethical considerations of giving individuals the power to generate high-quality text without safeguards, raising concerns about misuse.
10. The recognition of the need for watermarking or similar techniques in generative AI to identify machine-generated content, aiding in preventing misuse or acknowledging the source of the content.","Based on the transcript provided and focusing on actionable quotes that might impact the thinking or behavior of an AI Engineer, the following insights stand out:

- **On Building AI Products Thoughtfully (00:05:41)**
  ""Often with search, it's not about how advanced or how complex the technology is, it's about the rigor and intellectual honesty that you put into tuning the ranking algorithm.""

- **The Misalignment of Enterprise Sales with Innovation (00:09:56)**
  ""The dynamics of how enterprise sales happen... doesn't always cater to employees being happy with the best tools. What it does cater to is there's different incentive structures...""

- **On the Convergence of AI and Knowledge Work (00:15:26)**
  ""I can't say anything about it, but I think that we are experimenting with many technologies that use modern AI and LLMs, and we will launch what we think users like best.""

- **The Impact of AI on Existing Technologies and Workflows (00:19:43)**
  ""if users aren't gonna click on ads and you replace the search interface with just chat, that can actually be pretty dangerous in terms of what it even means to have to create a website.""

- **On Efficiency and Latency of AI Models (00:23:17)**
  ""time solves these things. You optimize stuff, you make things a little better, and the latency will get down with time.""

- **The Transition from Solution-First to Problem-First in AI Startups (00:33:18)**
  ""The people who focus on problem first usually get much further than the people who focus solution first. And there's way too many companies that are solutions first.""

- **On the Democratization of AI Tools and Models (00:48:55)**
  ""The technology exists to put all of those pieces together... The implications for people who are older or saving people over time are huge. This could be a really cool thing to productionize.""

These quotes highlight critical thinking on current and emerging AI technologies, the importance of focusing on problems rather than just tech solutions, the ethical implications of AI accessibility, and the potential for innovation in mixing different AI modalities. These insights could guide AI engineers in making informed decisions in their work, focusing on user-centric and ethically aware development practices.","1. Models
2. Hardware
3. LLM Tooling
4. Finetuning
5. AI Engineer Career
6. Coding Tools
7. Open Source
8. Startups
9. FlashAttention
10. Monthy Recaps",Models,Hardware,LLM Tooling,Finetuning,AI Engineer Career,Coding Tools,Open Source,Startups,FlashAttention,Monthy Recaps,0.2224744871,0.0,0.1
Segment Anything Model and the Hard Problems of Computer Vision — with Joseph Nelson of Roboflow,https://www.latent.space/p/segment-anything-roboflow,swyx & Alessio,"Thu, 13 Apr 2023 16:52:39 GMT",4775,https://substackcdn.com/feed/podcast/1084089/post/114574393/0b0735ec9a982ae57458c845089b64b9.jpg,https://api.substack.com/feed/podcast/114574393/9f605c5fbd709eba14f825751f6ee50e.mp3,"2023 is the  year of Multimodal AI , and Latent Space is going multimodal too!   *  This podcast comes with a  video  demo at the 1hr mark  and it’s a good excuse to launch our YouTube - please subscribe!  *  We are also holding two  events  in San Francisco — the  first AI | UX meetup  next week (already full; we’ll send a recap here on the newsletter) and  Latent Space Liftoff Day  on May 4th ( signup here ; but get in touch if you have a high profile launch you’d like to make).  *  We also joined  the Chroma/OpenAI ChatGPT Plugins Hackathon  last week where we won  the Turing and Replit awards  and met some of you in person! This post featured on  Hacker News . Out of the five senses of the human body, I’d put sight at the very top. But weirdly when it comes to AI, Computer Vision has felt left out of the recent wave compared to image generation, text reasoning, and even audio transcription. We got our first taste of it with  the OCR capabilities demo in the GPT-4 Developer Livestream , but to date GPT-4’s vision capability has not yet been released.  Meta AI leapfrogged OpenAI and everyone else by fully open sourcing their  Segment Anything Model  (SAM) last week, complete with paper, model, weights, data ( 6x  more images and  400x  more masks than OpenImages), and a very slick demo  website . This is a marked change to their previous LLaMA release, which was not commercially licensed. The response has been ecstatic: SAM was the talk of the town at the ChatGPT Plugins Hackathon and I was fortunate enough to book  Joseph Nelson   who was frantically integrating SAM into Roboflow this past weekend. As a passionate instructor, hacker, and founder, Joseph is possibly the single best person in the world to bring the rest of us up to speed on the state of Computer Vision and the implications of SAM. I was already a fan of him from his  previous pod  with (hopefully future guest) Beyang Liu of Sourcegraph, so this served as a personal catchup as well.  Enjoy!  and let us know what other news/models/guests you’d like to have us discuss!   - swyx Recorded in-person at the beautiful  StudioPod  studios in San Francisco. Full transcript is below the fold. Show Notes * Joseph’s links:  Twitter ,  Linkedin ,  Personal *  Sourcegraph Podcast  and  Game Theory Story *  Represently *  Roboflow at Pioneer  and  YCombinator *  Udacity Self Driving Car dataset story *  Computer Vision Annotation Formats * SAM recap - top things to know for those living in a cave * https://segment-anything.com/ *  https://segment-anything.com/demo *  https://arxiv.org/pdf/2304.02643.pdf   *  https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/ *  https://blog.roboflow.com/segment-anything-breakdown/ *  https://ai.facebook.com/datasets/segment-anything/ * Ask Roboflow  https://ask.roboflow.ai/ * GPT-4 Multimodal  https://blog.roboflow.com/gpt-4-impact-speculation/ Cut for time: *  WSJ mention *  Des Moines Register story * All In Pod:  timestamped mention * In  Forbes : underrepresented investors in Series A * Roboflow greatest hits *  https://blog.roboflow.com/mountain-dew-contest-computer-vision/ *  https://blog.roboflow.com/self-driving-car-dataset-missing-pedestrians/ *  https://blog.roboflow.com/nerualhash-collision/  and Apple CSAM issue  *  https://www.rf100.org/",* [00:00:19] Introducing Joseph * [00:02:28] Why Iowa * [00:05:52] Origin of Roboflow * [00:16:12] Why Computer Vision * [00:17:50] Computer Vision Use Cases * [00:26:15] The Economics of Annotation/Segmentation * [00:32:17] Computer Vision Annotation Formats * [00:36:41] Intro to Computer Vision & Segmentation * [00:39:08] YOLO * [00:44:44] World Knowledge of Foundation Models * [00:46:21] Segment Anything Model * [00:51:29] SAM: Zero Shot Transfer * [00:51:53] SAM: Promptability * [00:53:24] SAM: Model Assisted Labeling * [00:56:03] SAM doesn't have labels * [00:59:23] Labeling on the Browser * [01:00:28] Roboflow + SAM Video Demo  * [01:07:27] Future Predictions * [01:08:04] GPT4 Multimodality * [01:09:27] Remaining Hard Problems * [01:13:57] Ask Roboflow (2019) * [01:15:26] How to keep up in AI,"In this engaging podcast episode, host Swyx talks with Joseph Nelson, the co-founder and CEO of Roboflow, a company innovating in the field of computer vision. They discuss Joseph's background in economics, his early involvement in political activism, and the creation of Represently, a Zendesk-like tool for Congress aimed at improving constituent responsiveness. 

Joseph's unique journey from natural language processing to computer vision is highlighted, including his hackathon projects like Magic Sudoku and attempts at computer vision chess. Furthermore, his participation in the Pioneer accelerator and the development of Board Boss, a gaming app that received notable attention, signal his persistent innovation in tech. 

The conversation unveils Joseph's deep Iowa roots, underscoring his pride in his home state and its impact on his identity. This personal touch segues into an exploration of Iowa's surprising contributions to tech and science, including notable figures who attended Joseph's high school.

As co-founders of Roboflow, Joseph and his team have concentrated on enhancing computer vision workflows for developers, demonstrating a commitment to making the world more programmable. By leveraging large-scale datasets and models, they aim to streamline and democratize access to advanced computer vision technologies. 

The podcast also dives deeper into the specifics of Roboflow, detailing its origin as an annotation format converter and the evolution of its offerings. Attention is given to emerging challenges and opportunities in computer vision, especially in light of Facebook’s groundbreaking ""Segment Anything"" model which significantly advances segmentation capabilities.

Joseph shares insights on keeping abreast with rapid AI advancements, underscoring the importance of curiosity, continuous learning, and the synthesis of new knowledge into accessible formats. He emphasizes the importance of producing content and engaging with communities as key strategies for staying updated and relevant in the fast-paced field of AI and tech innovation.","1. Zero-shot transfer in computer vision has significantly improved.
2. Image-to-code generation demonstrated by GPT-4 can create functional websites from drawn sketches.
3. GPT-4's capability extends to advanced OCR, allowing it to read and understand text from images directly.
4. ""Segment Anything Model"" (SAM) by Facebook allows for zero-shot segmentation, identifying and segmenting multiple objects in images without prior training specific to those objects.
5. SAM significantly reduces the time and resources required for dataset preparation and annotation in AI projects.
6. GPT-4 enhances model promptability, allowing detailed interaction through text prompts to identify or isolate specific elements within an image.
7. The combination of SAM's segmentation ability with other models like Grounding DINO can provide object identification and labeling, compensating for SAM's current limitation of not providing labels with its masks.
8. The development approach of iterative refinement used in SAM's creation, involving stages from manual annotation assistance to full automation, highlights a productive strategy for complex AI tasks.
9. RoboFlow integrates SAM for enhanced computer vision tasks, demonstrating practical applications of cutting-edge AI research.
10. Multimodality in AI, exemplified by GPT-4, is pushing the boundaries of what AI can understand and generate, moving towards more comprehensive, context-aware systems.
11. The growing ability of AI to generalize from large datasets to specific tasks suggests a future where less custom training data may be needed for specialized applications, though challenges remain in fully personalized or proprietary contexts.","[00:55:01] ""And that's one of the big things that like I'm pretty excited about with, uh, multi-modality and especially with GBT and some of these things, is that like I can take what these massive models understand...and distill them down into a different architecture that captures that portion of the world."" 

[01:11:18] ""I might have data that's truly proprietary that's like not present on the web. So like I can't rely on this model. I might have a task type that these GB T four and multi-modality models are extremely good at visual question answering. And I think they'll be able to describe images in kinda like a freeform text way.""

[01:13:02] ""But there's still gonna be a healthy set of problems to making this stuff be, be usable, but ways that we're thinking about at RoboFlow that I'm very excited about. So we already used GPT4 to do like dataset description...And then we also of course have like GPT4 powered support...""

[01:15:26] ""The way that I do is ingest sources from probably similar places that others do...The second part is production...to produce things and then...being forced to...produce things and then...be curious. Like, I dunno, like I think about someone like Jeff Dean who made MapReduce and also introduced one of the first versions of TensorFlow...everyone is learning. Some people are just further ahead on their journey and you can actually catch up pretty quickly with some strong effort.""","1. Models
2. Multimodal
3. Hardware
4. LLM Tooling
5. Open Source
6. Finetuning
7. AI Engineer Career
8. Coding Tools
9. FlashAttention
10. UX",Models,Multimodal,Hardware,LLM Tooling,Open Source,Finetuning,AI Engineer Career,Coding Tools,FlashAttention,UX,0.3449489742,0.2224744871,0.0
AI Fundamentals: Benchmarks 101,https://www.latent.space/p/benchmarks-101,Alessio Fanelli and swyx & Alessio,"Fri, 07 Apr 2023 02:18:47 GMT",3038,https://substackcdn.com/feed/podcast/1084089/post/113186559/92120159a90420fb694c09dfd0a904e8.jpg,https://api.substack.com/feed/podcast/113186559/4b5977c64f9d7fd2b9a08a6d10a9494a.mp3,"We’re trying a new format, inspired by  Acquired.fm ! No guests, no news, just highly prepared, in-depth conversation on one topic that will level up your understanding. We aren’t experts, we are learning in public. Please let us know what we got wrong and what you think of this new format! When you ask someone to break down the basic ingredients of a Large Language Model, you’ll often hear a few things: You need lots of data. You need lots of compute. You need models with billions of parameters.  Trust the  Bitter Lesson , more more more,  scale is all you need . Right? Nobody ever mentions the subtle influence of great benchmarking. LLM Benchmarks mark our progress in building artificial intelligences, progressing from  * knowing what words go with others (1985 WordNet) * recognizing names and entities (2004 Enron Emails)  * and image of numbers, letters, and clothes (1998-2017 MNIST) * language translation (2002 BLEU → 2020 XTREME) * more and more images (2009 ImageNet, CIFAR) * reasoning in sentences (2016 LAMBADA) and paragraphs (2019 AI2RC, DROP) * stringing together whole sentences (2018 GLUE and SuperGLUE) * question answering (2019 CoQA) * having common sense (2018 Swag and HellaSwag, 2019 WinoGrande) * knowledge of all human tasks and professional exams (2021 MMLU) * knowing everything (2022 BIG-Bench) People who make benchmarks are the unsung heroes of LLM research, because they dream up ever harder tests that last ever shorter periods of time. In our first  AI Fundamentals  episode, we take a trek through history to try to explain what we have learned about LLM Benchmarking, and what issues we have discovered with them.  There are way, way too many links and references to include in this email. You can follow along the work we did for our show prep in  this podcast’s accompanying repo , with all papers and selected tests pulled out. Enjoy and please let us know what other fundamentals topics you’d like us to cover!","* [00:00:21] Benchmarking Questions * [00:03:08] Why AI Benchmarks matter * [00:06:02] Introducing Benchmark Metrics * [00:08:14] Benchmarking Methodology * [00:09:45] 1985-1989: WordNet and Entailment * [00:12:44] 1998-2004 Enron Emails and MNIST * [00:14:35] 2009-14: ImageNet, CIFAR and the AlexNet Moment for Deep Learning * [00:17:42] 2018-19: GLUE and SuperGLUE - Single Sentence, Similarity and Paraphrase, Inference * [00:23:21] 2018-19: Swag and HellaSwag - Common Sense Inference * [00:26:07] Aside: How to Design Benchmarks * [00:26:51] 2021: MMLU - Human level Professional Knowledge * [00:29:39] 2021: HumanEval - Code Generation * [00:31:51] 2020: XTREME - Multilingual Benchmarks * [00:35:14] 2022: BIG-Bench - The Biggest of the Benches * [00:37:40] EDIT: Why BIG-Bench is missing from GPT4 Results * [00:38:25] Issue: GPT4 vs the mystery of the AMC10/12 * [00:40:28] Issue: Data Contamination * [00:42:13] Other Issues: Benchmark Data Quality and the Iris data set * [00:45:44] Tradeoffs of Latency, Inference Cost, Throughput * [00:49:45] Conclusion","The Latent Space Podcast episode, hosted by Alessio, a partner and CTO at Decibel Partners, and co-host swyx, a writer and editor of Latent Space, begins with a light-hearted AI benchmarking quiz to ascertain whether they are human. They tackle emoji-based movie guessing, physics questions involving acceleration due to gravity, and complex mathematical problems to demonstrate human-like reasoning abilities and humorously prove their humanity.

The podcast then delves into the primary topic: AI benchmarks. The hosts discuss the significance of benchmarks in evaluating the performance of AI models, specifically highlighting the recent release of GPT-4. They articulate the evolution of benchmarks from simple tasks to highly complex challenges that test models on a diverse array of skills, emphasizing how these benchmarks drive AI research and development by setting performance standards.

Furthermore, they explore various types of AI benchmark metrics, including accuracy, precision, and recall—detailed with relatable analogies like Spotify playlists for better understanding. They also discuss different methods of benchmarking models, namely zero-shot, few-shot, and fine-tuned approaches, elaborating on how these methods vary in terms of data and effort required for testing.

A comprehensive historical overview of benchmarking since 1985 is provided, tracing from WordNet's linguistic benchmarks to the ImageNet challenge that sparked significant advancements in deep learning and visual recognition. They cover various datasets and benchmarks that have been instrumental in progressing AI capabilities across fields like vision, language understanding, and even code generation.

In discussing the methodology and evolution of benchmarks, the hosts critique the limitations and biases inherent in benchmarks, stressing the importance of designing benchmarks that accurately reflect tasks AI models will face in real-world applications.

The episode concludes on the notion of moving beyond current benchmarks to address practical applications, suggesting that future benchmarks should consider factors like latency, cost, and scalability to ensure AI models are not only powerful in theory but also efficient and effective in real-world situations.

Overall, the podcast provides an insightful, in-depth exploration of the role of benchmarks in AI development, blending technical discussions with accessible examples and a touch of humor.","1. AI can pass sophisticated benchmarks including AMC12 math tests but struggle with simpler ones like AMC10, showing uneven capabilities.
2. GPT-4's occasional underperformance in specific areas, such as lower scores in AMC10 compared to AMC12 math tests, indicates limitations in its learning and problem-solving capabilities.
3. The emergence of BIG-bench, a comprehensive benchmark encompassing 204 tasks across diverse fields, highlights the evolving complexity of AI evaluation methods.
4. HumanEval benchmark signifies progress in code generation abilities, illustrating advancements toward automating programming tasks.
5. The introduction of the MMLU benchmark covering 57 tasks demonstrates GPT-4's capacity for extensive world knowledge and high-level problem-solving, suggesting near-expert human performance levels.
6. Evolution from benchmarks focusing on individual sentences or straightforward logic to multi-sentence, context-driven evaluations, as seen in SuperGLUE developments.
7. The creation of benchmarks addressing common sense and reasoning through scenarios or sequences, as demonstrated by SWAG and HellaSWAG.
8. Benchmarking moving beyond static datasets to include dynamic, adversary-generated questions to test AI's common sense and inference abilities.
9. Use of image and text classification benchmarks like MNIST and Enron emails demonstrate historically significant methods for assessing AI's learning and classification skills.
10. Introduction of diverse benchmarks like WordNet and Penn Treebank early in AI development showcases foundational efforts to evaluate language and semantic understanding capabilities.","**[00:44:48] ""Tradeoffs of Latency, Inference Cost, Throughput""**
- ""Production benchmarking... doesn't really exist today, but I think we'll see the rise of... And I think the main three drivers are one latency... how quickly can I infer the answer... cost... if I'm using this model, how much does each call cost me... and then throughput?""
- ""If on the left side you have model size going from smallest to biggest, and on the X axis you have latency tolerance... You start to see different type of use cases... there's always trade offs.""

**[00:48:59] ""Conclusion""** 
- ""Well that's Benchmark 1 0 1... Let us know how you think we did... This is something we're trying for the first time... We don't have to keep keeping up with the news. I think there's a lot of history that we can go back on and deepen our understanding of the context of all these evolutions in, language models.""","1. Benchmarks 101
2. Models
3. Finetuning
4. Coding Tools
5. Hardware
6. Multimodal
7. Datasets 101
8. Transformers Math
9. Open Source
10. AI Engineer Career",Benchmarks 101,Models,Finetuning,Coding Tools,Hardware,Multimodal,Datasets 101,Transformers Math,Open Source,AI Engineer Career,0.022474487099999996,0.0,0.1
Grounded Research: From Google Brain to MLOps to LLMOps — with Shreya Shankar of UC Berkeley,https://www.latent.space/p/shreya-shankar,swyx & Alessio and Alessio Fanelli,"Wed, 29 Mar 2023 00:15:09 GMT",2505,https://substackcdn.com/feed/podcast/1084089/post/111327327/63259260455dc83bd3b34d615e3e48df.jpg,https://api.substack.com/feed/podcast/111327327/38e07817447c680398649df8582e601d.mp3,"We are excited to feature our first academic on the pod! I first came across Shreya when her tweetstorm of MLOps principles went viral: Shreya’s holistic approach to production grade machine learning has taken her from Stanford to Facebook and Google Brain, being the first ML Engineer at Viaduct, and now a PhD in Databases (trust us, its relevant) at UC Berkeley with  the new EPIC Data Lab . If you know Berkeley’s history in turning cutting edge research into gamechanging startups, you should be as excited as we are! Recorded in-person at the beautiful  StudioPod  studios in San Francisco. Full transcript is below the fold. Edit from the future : Shreya obliged us with  another round of LLMOps hot takes  after the pod! Other Links * Shreya’s About:  https://www.shreya-shankar.com/about/ *  Berkeley Sky Computing Lab  - Utility Computing for the Cloud *  Berkeley Epic Data Lab  -  low-code  and  no-code  interfaces for data work, powered by next-generation predictive programming techniques *  Shreya’s ML Principles  *  Grounded Theory * Lightning Round: * Favorite AI Product: Stability  Dreamstudio * 1 Year Prediction: Data management platforms * Request for startup: Design system generator * Takeaway: It’s not a fad!",* [00:00:27] Introducing Shreya (poorly) * [00:03:38] The 3 V's of ML development * [00:05:45] Bridging Development and Production * [00:08:40] Preventing Data Leakage * [00:10:31] Berkeley's Unique Research Lab Culture * [00:11:53] From Static to Dynamically Updated Data * [00:12:55] Models as views on Data * [00:15:03] Principle: Version everything you do * [00:16:30] Principle: Always validate your data * [00:18:33] Heuristics for Model Architecture Selection * [00:20:36] The LLMOps Stack * [00:22:50] Shadow Models * [00:23:53] Keeping Up With Research * [00:26:10] Grounded Theory Research * [00:27:59] Google Brain vs Academia * [00:31:41] Advice for New Grads * [00:32:59] Helping Minorities in CS * [00:35:06] Lightning Round,"In this episode of the Latent Space podcast, the guest is Sreya, a PhD student in databases at UC Berkeley, currently on hold from her Entrepreneur in Residence position at Amplify as she focuses on her studies. Previously, she interned at Google, worked as a Machine Learning Engineer at Viaduct, and explored data management for AI and ML software development. Beyond her professional pursuits, Sreya enjoys outdoor walks, trying out coffee in the Bay Area, and cooking, particularly pastas, due to her love for carbs. She recently faced the challenge of cooking for a diverse group of 25 guests, accounting for dietary restrictions like vegan, gluten-free, and keto preferences, eventually opting to order pizza for simplicity.

Sreya's PhD research revolves around confronting the challenges of making machine learning work in the real world, emphasizing the importance of streamlined workflows for ML development, and bridging the gap between development and production environments. She delves into the ""three Vs"" of ML development — velocity, validation, and versioning — highlighting the distinct nature of ML workflows compared to traditional software engineering due to their experimental-driven approach. Despite the advancements in ML, Sreya notes a persistent gap in the development and production phase, partly due to differences in hardware requirements and execution stacks.

The podcast extensively explores Sreya's insights on data management challenges in ML, particularly data validation and version control, and the nuances of deploying and maintaining ML models in production. Sreya also touches upon the potential for academic research in data management to impact industry practices, reflecting on how the academic environment at UC Berkeley fosters innovative research in the field.

Sreya advocates for the versioning of all components in an ML experiment to facilitate reproducibility and suggests practices for effective data validation to prevent performance issues in production. She is interested in exploring how to bridge the developmental and production gap for ML models, proposing a system where models are viewed as dynamic entities that evolve with the underlying data. Her research aims to create a unified development and production experience, with a focus on developing ML pipelines that incorporate regular retraining and monitoring as integral components.

The conversation shifts towards the challenges and opportunities for new graduates entering the AI field, the importance of data management in deploying AI models, and the excitement surrounding the rapid advancements in AI technology. Sreya encourages engagement with cutting-edge research and highlights the vibrant AI community on platforms like Twitter for staying updated on the latest developments.","1. The three Vs of ML development: velocity, validation, and versioning.
2. How experimental workflow in ML differs significantly from traditional software development due to its experiment-driven nature.
3. Challenges in bridging development and production environments for ML, such as hardware and execution stack differences.
4. The concept of models as views that are frequently recomputed over data.
5. Importance of data management in AI and ML-powered software, particularly for building applications that continuously adapt and improve.
6. The need for unified development and production experiences to eliminate bugs and streamline ML deployment.
7. The problem of preventing data leakage in ML model training and the difficulty of providing developers with guardrails.
8. Discussion on dynamic data updates and moving away from static datasets to understanding evolving data in real-time.
9. Challenges in model architecture selection, especially considering operational capabilities and the balance between leveraging large models via APIs vs. hosting models in-house.
10. The emerging ML Ops stack for Large Language Models (LLMs) and the necessity for managing state, dynamic prompts, and custom filters.
11. The concept of shadow models in production for testing against a simpler baseline without affecting user experience.
12. Observations on the gap between academia and high-resource companies like Google Brain in terms of resources for training state-of-the-art models.
13. The potential for collective frustration in the industry due to underestimating data management challenges when deploying AI systems.
14. Sreya's personal interest in a service that intelligently manages and evolves color palettes for web development projects.
15. The excitement surrounding current advancements in AI and the recognition of its significant impact beyond being a mere technological fad.","From this rich and insightful conversation on the Latent Space podcast, several key quotes stand out that could significantly impact the thinking or behavior of AI Engineers:

[00:04:15] ""This paper was conducted as a loosely structured interview study... Then we keep seeing these themes of velocity wasn't the word, but it was like experiment quickly or high experimentation rate... We repeated this process three or four times... and the story kind of told itself in a way that makes sense.""

[00:05:45] ""...how do we bridge the gap between the development environments and the production environments? Right now, people separate these environments because the production environment doesn't allow people to move at the rate that they need to for experimentation.""

[00:08:40] ""How do we give developers the guardrails to not look at data that they're not supposed to? This is a really hard problem... When you have no guardrails in the systems, you do that. And of course, there's varying consequences that come from this.""

[00:12:55] ""Models are views that are frequently recomputed over your data to use database speak, and I don't see people kind of adopting that mindset when it comes to research or the data science techniques that people are learning in school.""

[00:15:03] ""Version everything you do, because in experimentation time, when you do an experiment, you need some version there because if you wanna publish those results, you need something to go back to.""

[00:16:30] ""I'm super, super big into data validation. People call it monitoring... A lot of data validation techniques for machine learning result in too many false positive alerts.""

These quotes, distilled from the discussion, underscore the importance of methodical experimentation, the seamless integration of development and production environments, the critical nature of data handling and validation, and the viewpoint of treating models as dynamic entities subject to frequent updates. Each carries actionable wisdom that can guide an AI Engineer's approach to machine learning workflows, experiment management, and the iterative development of robust, production-ready AI applications.","1. Data Management
2. ML Development Lifecycle
3. Data Science Techniques
4. Model Deployment and Production
5. Research Methodologies
6. AI Ethics and Bias
7. Performance Monitoring and Validation
8. ML Model Selection and Utilization
9. Career in AI and Machine Learning
10. Software Development Practices",Data Management,ML Development Lifecycle,Data Science Techniques,Model Deployment and Production,Research Methodologies,AI Ethics and Bias,Performance Monitoring and Validation,ML Model Selection and Utilization,Career in AI and Machine Learning,Software Development Practices,0.0,0.0,0.0
"Emergency Pod: ChatGPT's App Store Moment (w/ OpenAI's Logan Kilpatrick, LindyAI's Florent Crivello and Nader Dabit)",https://www.latent.space/p/chatgpt-plugins,Alessio Fanelli and swyx & Alessio,"Fri, 24 Mar 2023 00:29:18 GMT",5776,https://substackcdn.com/feed/podcast/1084089/post/110302604/ac7dd241f64ab3f4a156034f7a6b5a5a.jpg,https://api.substack.com/feed/podcast/110302604/fa344345d4bf2e680cd7b45ffa9ae311.mp3,"This blogpost has been updated since original release to add more links and references. The  ChatGPT Plugins announcement  today could be viewed as the launch of ChatGPT’s “App Store”, a moment as significant as when Apple opened its App Store for the iPhone in 2008 or when Facebook let developers loose on its  Open Graph  in 2010. With a dozen lines of simple JSON and a mostly-english prompt to help ChatGPT understand what the plugin does, developers will be able to add extensions to ChatGPT to get information and trigger actions in the real world.  OpenAI itself launched with some killer first party plugins for:  *  Browsing the web ,  *  writing AND executing Python code  (in an  effortlessly multimodal  way),  *  retrieving embedded documents from external datastores , * as well as 11 launch partner plugins from  Expedia  to  Milo  to  Zapier . My recap thread was well received: But the thing that broke my brain was that ChatGPT’s Python Interpreter plugin can run nontrivial code - users can upload video files and ask ChatGPT to edit it, meaning it now has gone beyond mere chat to offer a substantial compute platform with storage, memory and file upload/download.  I immediately started  my first AI Twitter Space  to process this historical moment with Alessio and friends of the pod live. OpenAI’s Logan (see  Episode 1 from *last month* …) suggested that you might be able to link ChatGPT up with Zapier triggers to do arbitrary tasks! and then Flo Crivello, who  just launched his AI Assistant startup Lindy , joined us to discuss the builder perspective. Tune in on this EMERGENCY EPISODE of Latent Space to hear developers ask and debate all the issues spilling out from the ChatGPT Plugins launch - and  let us know in the comments  if you want more/have further questions! SPECIAL NOTE:  I was caught up in the hype and was far more negative on Replit than I initially intended as I tried to figure out this new ChatGPT programming paradigm. I regret this. Replit is extremely innovative and well positioned to help you develop and host ChatGPT plugins, and of course Amjad is already on top of it: Mea culpa.",* [00:00:38] First Reactions to ChatGPT Plugins * [00:07:53] Q&A: Keeping up with AI * [00:10:39] Q&A: ChatGPT Intepreter changes Programming * [00:12:27] Q&A: ChatGPT for Education * [00:15:21] Q&A: GPT4 Sketch to Website Demo * [00:16:32] Q&A: AI Competition and Human Jobs * [00:18:44] ChatGPT Plugins as App Store * [00:34:40] Google vs ChatGPT * [00:36:04] Nader Dabit on Selling His GPT App * [00:43:16] Q&A: ChatGPT Waitlist and Voice * [00:45:26] LangChain with Human in the Loop * [00:46:58] Google vs Microsoft vs Apple * [00:51:43] ChatGPT Plugin Ideas * [00:53:49] Not an app store? * [00:55:24] LangChain and the Future of AI * [01:00:48] Q&A: ChatGPT Bots and Cronjobs * [01:04:43] Logan Joins Us! * [01:07:14] Q&A: Plugins Rollout * [01:08:26] Q&A: Plugins Discovery * [01:10:00] Q&A: OpenAI vs BingChat * [01:11:03] Q&A: App Store Monetization * [01:14:45] Q&A: ChatGPT Plugins API * [01:17:17] Q&A: Python Interpreter * [01:19:58] The History of App Stores and Marketplaces * [01:22:40] LindyAI's Flo Crivello Joins Us * [01:29:42] AI Safety * [01:31:07] Multimodal GPT4 * [01:32:10] Designing AI-safe APIs * [01:34:39] Flo's Closing Comments,"The podcast transcript discusses an emergency episode focusing on the launch of ChatGPT plugins. The hosts and guests, including Logan from OpenAI and Flo Coveo from Lin AI, share their initial reactions and thoughts on this development, which they compare to an ""app store moment for AI."" They are intrigued by the potential of ChatGPT to generate Python code, run it, and particularly its ability to process files through FFMPEG, indicating its capability as a general-purpose compute platform.

The conversation extensively covers the excitement around the new plugin ecosystem introduced by ChatGPT, highlighting the ease with which developers can now create and distribute their plugins. The framework for these plugins is notably simple, requiring just a description in English and an open API specification, which significantly lowers the barrier to entry for developers.

Discussions also delve into concerns about competition within the app store environment, pondering how smaller developers can stand out among big companies already developing for the platform. The guests speculate on the future of AI and app development, touching on the necessity of clear installation processes, the importance of a user-friendly approach to encourage developer participation, and the broader implications for industries outside of software development.

Key insights include the realization that the realm of possibilities with ChatGPT plugins extends far beyond chat functionalities, suggesting a paradigm shift in how developers and users interact with AI technologies. The participants express both excitement for the potential innovations and concern over the rapid pace of progress, questioning how to navigate this new landscape effectively.

Listeners are given an inside look at how developers are reacting in real-time to significant tech launches, providing a snapshot of the excitement, possibilities, challenges, and concerns that come with groundbreaking developments in the AI space.","1. OpenAI's ChatGPT can now run FFMPEG, allowing it to generate Python code, process video files, and provide a download link, transitioning ChatGPT into a general-purpose compute platform.
2. OpenAI demonstrated an ""App Store moment for AI"" with ChatGPT Plugins, significantly lowering the barrier to entry for developers to create and distribute their own plugins.
3. Plugins are described in English and integrated via a simple JSON file and an OpenAPI specification, making it easy for anyone to develop for ChatGPT, even without extensive technical know-how.
4. ChatGPT's Plugin system raises questions about the future of small developers in the ecosystem and how they can compete against established companies.
5. The potential influence of ChatGPT Plugins on various industries and the partnership strategy by OpenAI hint at the prioritization and possible ""winners"" in certain sectors.
6. The integration of ChatGPT with other platforms and services opens possibilities for creative development, such as programming for backend uses, despite potential limitations in the current state of AI.
7. The launch of ChatGPT Plugins marks a significant development in the AI field, likened to the biggest app store launch ever, underscoring the transformative potential of this feature.
8. OpenAI's strategy to roll out ChatGPT Plugins includes a waitlist, managing expectations around access and development potential for both users and creators.
9. Concerns and speculations arise regarding how ChatGPT and its plugins could redefine job roles, particularly in development and design, pointing towards the need for adaptation and possibly regulatory considerations.
10. The integration of ChatGPT with the web browsing and plugin capabilities extends its utility beyond traditional chat applications, potentially impacting the way users interact with the internet and digital services.","[01:34:51] ""Look, I think the market, this is going to be the model of our market, so I think it's gonna be, it's gonna be more than fine, but maybe room full. Mini here. Blue ocean. That's right. Time to build. Let's go.""

[01:35:16] ""I, I, I don't know what to think. That's, this is why I started this space because I saw that CHE BT can run f fm Peg, which means it is a compute platform, right? Like it generates Python code, it runs the Python code. It can receive files, it can store files, it has memory and then it can let you download the files. Give it some GPUs, and you can run Lama inside of chat, gbc, for whatever reason you want. It is a new compute platform now, and I want to build for it, but I don't know what I, what I can.""

[01:35:55] ""I, I agree. I think it's, it's, these large models are like the next operating system. I'm, I'm very convinced that that's the way people are gonna interact with the computers. Like, you're no longer gonna do work at your computer, you're gonna have a conversation with your computer and the computers gonna work for you.""","1. Models
2. AI Engineer Career
3. Hardware
4. Open Source
5. LLM Tooling
6. Finetuning
7. Coding Tools
8. UX
9. Datasets 101
10. Benchmarks 101",Models,AI Engineer Career,Hardware,Open Source,LLM Tooling,Finetuning,Coding Tools,UX,Datasets 101,Benchmarks 101,0.3449489742,-0.2224744871,0.0
From Astrophysics to AI: Building the future AI Data Stack — with Sarah Nagy of Seek.ai,https://www.latent.space/p/sarah-nagy,Alessio Fanelli and swyx & Alessio,"Fri, 10 Mar 2023 20:47:05 GMT",2251,https://substackcdn.com/feed/podcast/1084089/post/107646616/77fe34fd4c1985cdc616a75a26b1bb9b.jpg,https://api.substack.com/feed/podcast/107646616/ec2d10f70ff00e6b1020e5b8ec5ff83d.mp3,"If  Text is the Universal Interface , then Text to SQL is perhaps the killer B2B business usecase for Generative AI. You may have seen incredible demos from  Perplexity AI ,  OSS Insights , and  CensusGPT  where the barrier of learning SQL and schemas goes away and you can intuitively converse with your data in natural language. But in the multi-billion dollar data engineering industry,  Seek.ai  has emerged as the forerunner in building a conversational engine and knowledge base that truly democratizes data insights.  We’re proud to present our first remote interview with  Sarah Nagy  to learn how AI can help you “ seek what matters ”!",* 00:00: Intro to Sarah * 03:40: Seek.ai origin * 05:45: Data driven vs Data backfit * 09:15: How Enterprises adopt AI * 12:55: Patents and IP Law * 14:05: The Semantic Layer * 16:35: Interfaces - Dashboards vs Chat? * 21:05: LLM performance and selection * 26:05: LLMOps and LangChain * 30:55: Lightning round Show notes *  Sarah Nagy Linkedin *  Seek.ai *  Sarah on the dbt podcast Lightning Rounds * Favorite AI Product:  Stable Diffusion * Favorite AI Community:  Eleuther * One year prediction: Things will move fast! * Request for Startup: Scheduling/Emails (shoutout  Ipso.ai  from our hackathon!) * Takeaway:  Automate everything!   Get full access to Latent Space at  www.latent.space/subscrib,It seems you might have missed providing the transcript you wanted me to summarize. Could you please provide the podcast transcript or details about the podcast episode you're referring to?,,,"1. Finetuning
2. Models
3. Multimodal
4. Transformers Math
5. Datasets 101
6. AI Engineer Career
7. Coding Tools
8. LLM Tooling
9. Hardware
10. Open Source",Finetuning,Models,Multimodal,Transformers Math,Datasets 101,AI Engineer Career,Coding Tools,LLM Tooling,Hardware,Open Source,0.2449489742,0.0,0.0
"97% Cheaper, Faster, Better, Correct AI — with Varun Mohan of Codeium",https://www.latent.space/p/varun-mohan,"Alessio Fanelli, swyx & Alessio, and Varun Mohan","Thu, 02 Mar 2023 14:38:40 GMT",3052,https://substackcdn.com/feed/podcast/1084089/post/106013498/c249529d726706b2803aafb5de6e55d8.jpg,https://api.substack.com/feed/podcast/106013498/740a873827a2f0c90db85382116d1559.mp3,"OpenAI just rollicked the AI world yet again yesterday — while releasing the long awaited ChatGPT API, they  also  priced it at $2 per million tokens generated, which is 90% cheaper than the text-davinci-003 pricing of the “GPT3.5” family.  Their blogpost  on  how  they did it is vague:  Through a series of system-wide optimizations, we’ve achieved 90% cost reduction for ChatGPT since December; we’re now passing through those savings to API users. We were fortunate enough to record Episode 2 of our podcast with someone who routinely creates 90%+ improvements for their customers, and in fact have started productizing their own infra skills with Codeium, the rapidly growing  free-forever  Copilot alternative (see  What Building “Copilot for X” Really Takes ).  Varun Mohan  is CEO of Exafunction/Codeium, and he indulged us in diving deep into AI infrastructure, compute-optimal training vs inference tradeoffs, and why he loves suffering. Recorded in-person at the beautiful  StudioPod  studios in San Francisco. Full transcript is below the fold.","* 00:00: Intro to Varun and Exafunction * 03:06: GPU Efficiency, Model Flop Utilization, Dynamic Multiplexing * 05:30: Should companies own their ML infrastructure? * 07:00: The two kinds of LLM Applications * 08:30: Codeium * 14:50: “Our growth is 4-5% day over day” * 16:30: Latency, Quality, and Correctability * 20:30: Acceleration mode vs Exploration mode * 22:00: Copilot for X - Harvey AI’s deal with Allen & Overy * 25:00: Scaling Laws (Chinchilla) * 28:45: “The compute-optimal model might not be easy to serve” * 30:00: Smaller models * 32:30: Deepmind Retro can retrieve external infromation * 34:30: Implications for embedding databases * 37:10: LLMOps - Eval, Data Cleaning * 39:45: Testing/User feedback * 41:00: “Users Is All You Need” * 42:45: General Intelligence + Domain Specific Dataset * 43:15: The God Nvidia computer * 46:00: Lightning round Show notes *  Varun Mohan Linkedin *  Exafunction * Blogpost:  Are GPUs Worth it for ML *  Codeium *  Copilot statistics *  Eleuther’s The Pile  and  The Stack *  What Building “Copilot for X” Really Takes * Copilot for X *  Harvey, Copilot for Law - deal with Allen & Overy * Scaling Laws *  Training Compute-Optimal Large Language Models - arXiv  (Chinchilla paper) *  chinchilla's wild implications  (LessWrong) *  UL2 20B: An Open Source Unified Language Learner  (20B) * Paper -  Deepmind Retro * “ Does it make your beer taste better ” *  HumanEval benchmark /dataset *  Reverse Engineering Copilot internals *  Quora Poe *  Prasanna Sankar notes on FLOPs and Bandwidth *  NVIDIA H100 specs  - 3TB/s GPU memory, 900GB/s NVLink Interconnect * Optimizer state is 14x size of model - 175B params => 2.5TB to store state → needs at least 30 H100 machines with 80GB each *  Connor Leahy on The Gradient Podcast Lightning Rounds * Favorite AI Product:  Midjourney * Favorite AI Community:  Eleuther  and  GPT-J * One year prediction: Better models, more creative usecases * Request for Startup: Superathlete Fitness Assistant * Takeaway:  Continue to tinker !","In the Latent Space podcast episode, hosts Alessio Fanelli and swyx chat with Varun Mohan, co-founder of Codeium/Exafunction. Varun shares insights into his background, including his CS education at MIT, his experience as a tech lead at Nuro, and his journey into launching Exafunction with his teammates to tackle the complexities of deep learning infrastructure. He highlights the challenges companies face in managing GPUs and the innovative solutions Exafunction developed to maximize hardware utilization and efficiency.

Varun delves into the technical aspects of building deep learning models, emphasizing the need for operational efficiency and the adjustments required for managing hardware. He shares Exafunction's approach to serving deep learning workloads at scale, describing techniques like dynamic multiplexing and operator fusion to improve GPU utilization and performance. The conversation also touches on the broader implications of AI in software development, including the significance of tools like GitHub CoPilot and Codeium's ambitions in the space.

The discussion shifts to broader topics in AI and machine learning, including the importance of specialized models for specific tasks, the challenges of building AI infrastructure, and the potential future developments in AI applications. Varun expresses enthusiasm for the creativity and innovation in the AI field, especially in developing applications that leverage the capabilities of advanced models.

Overall, the episode provides a deep dive into the infrastructure challenges and innovations in deep learning, Varun's entrepreneurial journey in the AI space, and thoughtful predictions about the future of AI and machine learning applications.","1. Deep learning infrastructure is complex and challenging for companies to maintain, especially when managing GPUs due to their fundamental differences from CPUs and the issue of GPU underutilization.
2. Exafunction aims to improve GPU utilization by virtualizing GPUs and decoupling them from workloads, allowing for more efficient use of hardware without excessive costs.
3. A metric called model flop utilization (MFU) indicates the fraction of computational capacity on hardware actually utilized, highlighting inefficiencies in typical GPU use by companies.
4. Operator fusion and dynamic multiplexing techniques can significantly increase computational efficiency and reduce hardware costs.
5. For most companies, adopting pre-trained models like GPT-3 or BERT and fine-tuning for specific tasks is more practical than developing new architectures from scratch, except for specialized applications like autonomous vehicles.
6. The liberty of using off-the-shelf architectures has led to a focus on transformers due to their popularity and the challenge of serving large language models efficiently.
7. The development of Coding, an alternative to GitHub Copilot, was motivated by the expense and computational demand of running high-intensity LLM applications, showcasing Exafunction's capability to deploy cost-effective LLM-based tools.
8. A significant proportion of code generated by Copilot is retained in codebases, indicating its utility and influence on development practices.
9. The creation of an alternative to GitHub Copilot involved collecting permissively licensed public code to initially train their model, underlining the importance and challenge of gathering high-quality training data.
10. Achieving high performance and cost efficiency in serving LLM models requires intricate infrastructure and the ability to fine-tune and experiment quickly with different model architectures and operational strategies.
11. Copilot for X suggests expanding the LLM application domain to professional fields beyond coding, such as legal assistance, highlighting the potential for LLMs to impact various industries significantly.
12. The conversation about skilling laws and optimal data scaling highlights the importance of data quality and size of models relative to the training dataset for achieving high performance in LLMs.
13. The development of high-quality, task-specific datasets and evaluation metrics is critical for training effective models that address specific business needs or industry challenges.
14. Infrastructure and operational strategies for LLM applications, including data cleaning, model evaluation, and serving infrastructure, are significantly different from traditional machine learning workflows, emphasizing the need for specially tailored solutions.","Based on the provided transcript, here are some actionable quotes that might impact the thinking or behavior of an AI Engineer:

1. **[00:02:24]** Varun Mohan on Deep Learning Infrastructure Challenges:
   - ""**Realized that deep learning infrastructure is really hard to build and really hard to maintain for even the most sophisticated companies...**""

2. **[00:03:13]** On GPU Utilization and Flop Efficiency:
   - ""**People heavily underestimate what a single container can sort of do. And the GPU is left like heavily idle...the common term now with a lot of LM workloads is like the flop efficiency of these workloads.**""

3. **[00:05:45]** Varun Mohan on Optimal ML Architectures:
   - ""**Companies, for the most part, should not be trying to figure out what the optimal ML architecture is.**""

4. **[00:13:22]** On Building Products with AI Models:
   - ""**Unless you are extremely confident you need to fine-tune... probably go out and build on top of an API and spend most of your time to make it so that you can hit that quality latency trade off properly.**""

5. **[00:19:26]** Varun Mohan on Product Development Philosophy:
   - ""**Can you deliver a product that is actually something users want to use? It's exceedingly hard for something that has large scope...but it's going to take a lot of engineering effort to make it work.**""

6. **[00:34:43]** On the Future of AI and Large Language Models:
   - ""**The model in some sense will need to talk to these embedding databases...this is maybe what Bing search is sort of looking like where it's like the model itself knows what context it sort of has...**""

7. **[00:37:52]** Varun Mohan Discusses the Importance of Evaluation:
   - ""**We couldn't outsource our eval...because we wouldn't be able to ship models internally if we didn't know how to evaluate them.**""

8. **[00:43:46]** On the Importance of Synchronous Training in Deep Learning:
   - ""**There is a lot of analysis in terms of how good is asynchronous training...but everyone has converged to synchronous because the hardware is continually upleveled to handle that.**""

9. **[00:45:59]** On Building AI Applications:
   - ""**Foundation models have kind of shot way too far ahead of the apps. People need to build apps.**""

These quotes, selected across different time codes, offer insights into challenges and strategies within AI engineering, emphasizing infrastructure, evaluation, efficiency, and the future direction of AI model applications and development.","1. AI Engineer Career
2. Models
3. Hardware
4. Finetuning
5. Open Source
6. Coding Tools
7. Startups
8. Agents
9. Transformers Math
10. Monthy Recaps",AI Engineer Career,Models,Hardware,Finetuning,Open Source,Coding Tools,Startups,Agents,Transformers Math,Monthy Recaps,0.0,0.0,0.2
"ChatGPT, GPT4 hype, and Building LLM-native products — with Logan Kilpatrick of OpenAI",https://www.latent.space/p/chatgpt-gpt4-hype-and-building-llm,swyx & Alessio and Alessio Fanelli,"Thu, 23 Feb 2023 17:53:03 GMT",3097,https://substackcdn.com/feed/podcast/1084089/post/104680156/1eff75596663cb1a80d514fd5db24fea.jpg,https://api.substack.com/feed/podcast/104680156/ad23ae74f323bf3e7a11b4df0f3e3ae0.mp3,We’re so glad to launch our first podcast episode with  Logan Kilpatrick ! This also happens to be his first public interview since joining OpenAI as their first Developer Advocate. Thanks Logan! Recorded in-person at the beautiful  StudioPod  studios in San Francisco.  Full transcript is below the fold.,"* 00:29: Logan’s path to OpenAI * 07:06: On ChatGPT and GPT3 API * 16:16: On Prompt Engineering * 20:30: Usecases and LLM-Native Products * 25:38: Risks and benefits of building on OpenAI * 35:22: OpenAI Codex * 42:40: Apple's Neural Engine * 44:21: Lightning Round Show notes *  Sam Altman’s interview with Connie Loizos *  OpenAI Cookbook *  OpenAI’s new Embedding Model *  Cohere on Word and Sentence Embeddings * (referenced)  What is AGI-hard? Lightning Rounds * Favorite AI Product:  https://www.synthesia.io/ * Favorite AI Community:  MLOps   * One year prediction: Personalized AI,  https://civitai.com/ * Takeaway: AI Revolution is here!","In the latest episode of the Latent Space podcast, hosts Alessio Fanelli and swyx welcomed Logan Kilpatrick as their guest. Logan Kilpatrick is notable for his achievements as the first developer advocate at OpenAI and as the lead developer community advocate of the Julia language. His journey into these fields was influenced by his experiences at NASA, where he worked on projects related to lunar landings, and at Apple, where his focus was on computer vision. He expressed a deep commitment to supporting NumFOCUS, a nonprofit crucial to the legal and fiscal support of open-source scientific projects like Julia, Jupyter, Pandas, and NumPy.

During the conversation, Logan emphasized the importance of community contributions to open source projects and shared insights into his role at OpenAI amidst the machine learning community's explosive growth. He discussed the unique approach to developer advocacy at OpenAI, highlighting the shift away from traditional outreach efforts towards building strong documentation foundations to support developers effectively.

Logan also expressed enthusiasm for the transformative potential of ChatGPT and other AI technologies across various sectors like education, mental health, and legal services by providing highly personalized and accessible solutions. He believes that AI will soon become deeply integrated into our lives, offering tailored assistance based on individuals' specific needs and histories.

Despite the ongoing debate around AI ethics, particularly its role in content creation, Logan sees immense value in leveraging AI to streamline and enhance creative processes. He is keenly interested in the development and utilization of AI technologies like Codex for programming assistance, emphasizing the importance of understanding and adapting to the rapidly evolving landscape of AI and its implications for work, creativity, and daily life.

Overall, the podcast offered a deep dive into Logan Kilpatrick's journey through the realms of open-source advocacy and AI, underscoring the pivotal role of community support in technological advancement and the boundless potential of AI to reshape industries and personal experiences.","1. NumFOCUS's critical role in supporting over a hundred open-source scientific projects, including Julia, Jupyter, Pandas, and NumPy.
2. The future potential of ChatGPT and developers building chat-first experiences and chat products.
3. Developer relations' shift from traditional outreach to focusing on enabling developers' success through strong documentation at OpenAI.
4. The non-deterministic nature of large language models (LLMs) and the implications for developer expectations and application behavior.
5. The promise and challenge of prompt engineering in unlocking the capabilities of LLMs and the art/science balance it involves.
6. The particular excitement around personalizing AI and making AI more accessible and integrated into various aspects of daily life.
7. The rapid development and deployment of AI in education and mental health services through APIs like ChatGPT.
8. The importance of transfer learning and fine-tuning in building differentiated AI applications and services.
9. Codex and its impact on coding practices and the way it evolves with newer models.
10. The high cost and questionable sustainability of pure ""prompt engineering"" jobs as comprehensive tools, and repositories for effective prompts emerge, making nuanced prompt crafting more accessible.","Given the comprehensive nature of the conversation and focusing on actionable insights for AI engineers, here are selected quotes from Logan Kilpatrick with time codes that could significantly impact their thinking or behavior:

**On The Importance of Supporting Open Source and Communities [00:01:17]**
- ""NumFOCUS is the nonprofit that helps enable a bunch of open-source scientific projects like Julia, Jupyter, Pandas, NumPy... It's a very critical important part of the ecosystem and something that I spend a bunch of my now more limited free time helping support.""

**The Critical Nature of Developer Advocacy in AI [00:05:35]**
- ""Developer experience point of view actually. Like how can we enable developers to be successful? And that at the present moment is like building a strong foundation of documentation and things like that.""

**On Tailoring Developer Relations Work Based on Needs [00:06:24]**
- ""It's kind of like a fill in the blanks type of thing. Like you go to where you're needed the most... It is more that people are not familiar with the APIs and the best practices around programming for large language models.""

**Importance of Prompt Engineering [00:16:16]**
- ""Prompt engineering is so much more of an art than a science right now... it's gonna continue to be like a little bit of a pseudoscience, I would imagine.""

**On the Future of Embeddings and Transfer Learning [00:47:45]**
- “I think more stuff around like transfer learning for like making transfer learning easier...that's truly the way to build really cool things is transfer learning, fine-tuning, and I don't think that there's enough.”

These quotes provide insights into the importance of community support in open-source projects, the unique aspects of developer advocacy in AI, the innovative yet challenging nature of prompt engineering, the emerging significance of embeddings and transfer learning. Encouraging AI engineers to engage more deeply with these aspects could significantly influence their work and the broader AI ecosystem.","1. Open Source
2. AI Engineer Career
3. Coding Tools
4. Models
5. Finetuning
6. Multimodal
7. News
8. Event Recaps
9. Hardware
10. Education",Open Source,AI Engineer Career,Coding Tools,Models,Finetuning,Multimodal,News,Event Recaps,Hardware,Education,0.0,0.4449489742,0.0
